{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELaILu9SPaoy"
   },
   "source": [
    "# EECE 571T Project - NLP with Emotion Dataset (BERT)\n",
    "\n",
    "Focus: BERT\n",
    "<br>\n",
    "Author: Tom Sung\n",
    "\n",
    "Last updated:\n",
    "* Date: April 5, 2022\n",
    "* Time: 5:30pm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEI9wPQ8ylyo",
    "outputId": "38e9f9e2-506d-4ecb-c8a5-8cb16b808882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Check detected system hardware resources.\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifCzi282UO64",
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification\n",
    "* From same author:\n",
    "    * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)\n",
    "    * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n",
    "* Different Pre-Processing Techniques with Bag of Words w/ TF-IDF, Word Embedding, and BERT: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFTJfJkrO06u",
    "tags": []
   },
   "source": [
    "# Get data from GitHub repo\n",
    "\n",
    "**Only run this once even after if notebook environment is cleared via** `%reset -f`. The code written here imports the Kaggle data set, which I have placed on my public GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "724_Q82ZOihs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/train.txt\n",
    "!wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/test.txt\n",
    "!wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/val.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWVt__H7R9zl",
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iKV__yZmPWsQ"
   },
   "outputs": [],
   "source": [
    "# Import libraries for data import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mMWomGTaPSES"
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "data_train = pd.read_csv('train.txt',sep=';', header=None)\n",
    "data_test = pd.read_csv('test.txt',sep=';', header=None)\n",
    "data_val = pd.read_csv('val.txt',sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AtXW9_Z2Pbtq"
   },
   "outputs": [],
   "source": [
    "col_names = [\"sentence\",\"emotion\"]\n",
    "data_train.columns = col_names\n",
    "data_test.columns = col_names\n",
    "data_val.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "GC4V-55GSaVT",
    "outputId": "9c00c80e-1f34-44bd-94fd-9de8a42ba284"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the data head to make sure data is imported correctly.\n",
    "data_train.head()\n",
    "# data_test.head()\n",
    "# data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLR3Zasdel1c",
    "tags": []
   },
   "source": [
    "# Encode the emotion labels with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oFJvTfXTdrN6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode the emotion labels with unique identifiers\n",
    "data_train['emotion'].unique()\n",
    "labelencoder = LabelEncoder()\n",
    "data_train['emotion_enc'] = labelencoder.fit_transform(data_train['emotion'])\n",
    "data_test['emotion_enc'] = labelencoder.fit_transform(data_test['emotion'])\n",
    "data_val['emotion_enc'] = labelencoder.fit_transform(data_val['emotion'])\n",
    "# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bBnrKVoiBT"
   },
   "source": [
    "Sort the encoded emotion labels for some classification reports later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkm7wEN1fnMX",
    "outputId": "948262b9-e9a1-44d5-d44a-e1f2c65e53db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion        anger\n",
       "emotion_enc        0\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_label_list = data_train[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
    "# data_test[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
    "# data_val[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
    "emotion_label_list = emotion_label_list.sort_values(by='emotion_enc')\n",
    "emotion_label_list.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lLROznaxHPNU",
    "outputId": "a09cc50a-6eab-437b-e0ce-36754df76c22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  emotion  emotion_enc\n",
       "0  im feeling quite sad and sorry for myself but ...  sadness            4\n",
       "1  i feel like i am still looking at a blank canv...  sadness            4\n",
       "2                     i feel like a faithful servant     love            3\n",
       "3                  i am just feeling cranky and blue    anger            0\n",
       "4  i can have for a treat or if i am feeling festive      joy            2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()\n",
    "data_test.head()\n",
    "data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DW_8szfEo1T",
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKULabmoEqMt"
   },
   "source": [
    "We need to do some data cleaning first~, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words...~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uE7b3QZrB6O"
   },
   "source": [
    "**Data Cleaning Process:** Keep only words, convert all words to lowercase, split all words, remove stopwords, lemmization for word root.<br>\n",
    "The result of all of this work is a cleaned data vocab list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHVXM1f4iYf6"
   },
   "source": [
    "Replace stemming with lemmization, which keeps the actual form of the word better. This is necessary for using pre-existing word embedding models.\n",
    "Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJWWXLcFdzc1",
    "outputId": "46e11a77-0165-4578-fd01-43ebdaad9e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Attempting data cleaning here\n",
    "def preprocess(raw_text):\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "    \n",
    "    # stemmed words (looks like this is causing some words to be weird)\n",
    "    # ps = PorterStemmer()\n",
    "    # stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "\n",
    "    # lemmed words (trying this because this gets the root word?)\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]\n",
    "    \n",
    "    # join the cleaned words in a list\n",
    "    # cleaned_word_list = \" \".join(stemmed_words)\n",
    "    cleaned_word_list = \" \".join(lemmed_words)\n",
    "    # cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BmRPEeBpYZN"
   },
   "source": [
    "Apply data cleaning to all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "schsncvleDrh"
   },
   "outputs": [],
   "source": [
    "data_train['sentence_cleaned'] = data_train['sentence'].apply(lambda line : preprocess(line))\n",
    "data_test['sentence_cleaned'] = data_test['sentence'].apply(lambda line : preprocess(line))\n",
    "data_val['sentence_cleaned'] = data_val['sentence'].apply(lambda line : preprocess(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb9vzfUAqXoZ",
    "tags": []
   },
   "source": [
    "# Pre-Processing and Training\n",
    "\n",
    "Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>\n",
    "There are several methods available: Bag-of-words with TF-IDF, Word Embedding using ~Word2Vec~ [I used GloVe, not Word2Vec] (unknown NN), and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cEZB9u7ZOvR"
   },
   "source": [
    "## METHOD 2: BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHqmgSCPXHv9"
   },
   "source": [
    "### BERT Model (Using Google's Tensorflow Tutorial)\n",
    "\n",
    "[Link](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=_OoF9mebuSZc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzC5ycXzXLzY",
    "outputId": "626a789b-1255-44d3-f0bf-7f6305af278e"
   },
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "# !pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "# !pip install -q tf-models-official==2.7.0\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "114Q7mGjUNRI"
   },
   "source": [
    "Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pc9E55PfXo-7"
   },
   "outputs": [],
   "source": [
    "# Get BERT Model\n",
    "\n",
    "# bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "\n",
    "# \n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "# These will be used in the NN model. These will be layers in the NN model\n",
    "# bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) # This is the pre-processing model\n",
    "# bert_model = hub.KerasLayer(tfhub_handle_encoder) # This is the actual BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5ggXtbHm9tAY"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    # Input\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='txt')\n",
    "\n",
    "    # BERT Pre-Processing Layer\n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing') # This is the pre-processing model\n",
    "    encoder_inputs = bert_preprocess_model(text_input)\n",
    "\n",
    "    # BERT Model Layer\n",
    "    bert_model = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder') # This is the actual BERT model\n",
    "    outputs = bert_model(encoder_inputs)\n",
    "\n",
    "    # Fine-tuning. First, Average pooling\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(6, activation='softmax', name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "irM9EJzvTTl_"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'CaseFoldUTF8' in binary running on Tom-Ke-Jun-Sungs-MacBook-Pro-AS.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4177\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4177\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   4178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CaseFoldUTF8'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:974\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m   loader \u001b[38;5;241m=\u001b[39m \u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_graph_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_model_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mckpt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:149\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_dir \u001b[38;5;241m=\u001b[39m export_dir\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_functions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mfunction_deserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_function_def_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaved_object_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapper_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_WrapperFunction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# captures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/saved_model/function_deserialization.py:406\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 406\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_def_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_input_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_input_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructured_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:70\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001b[0m\n\u001b[1;32m     69\u001b[0m     input_shapes \u001b[38;5;241m=\u001b[39m input_shapes_attr\u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 70\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_to_graph_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     74\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:239\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m   op_def \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_op_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m op_def\u001b[38;5;241m.\u001b[39mattr:\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4181\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:\n\u001b[1;32m   4180\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 4181\u001b[0m   \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_GraphGetOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4182\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4183\u001b[0m   \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on Tom-Ke-Jun-Sungs-MacBook-Pro-AS.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_classifier_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mbuild_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m text_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# BERT Pre-Processing Layer\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m bert_preprocess_model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfhub_handle_preprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This is the pre-processing model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m bert_preprocess_model(text_input)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# BERT Model Layer\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:153\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mNoDependency(\n\u001b[1;32m    150\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;241m=\u001b[39m \u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_training_argument \u001b[38;5;241m=\u001b[39m func_has_training_argument(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hub_module_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow_hub/keras_layer.py:449\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# Expected before TF2.4.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     set_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_load_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow_hub/module_v2.py:106\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    103\u001b[0m   obj \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_v2(\n\u001b[1;32m    104\u001b[0m       module_path, tags\u001b[38;5;241m=\u001b[39mtags, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m   obj \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m obj\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m is_hub_module_v1  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:936\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(export_dir, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    847\u001b[0m   \u001b[38;5;124;03m\"\"\"Load a SavedModel from `export_dir`.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m  Signatures associated with the SavedModel are available as functions:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mload_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    937\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:977\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    974\u001b[0m   loader \u001b[38;5;241m=\u001b[39m loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    975\u001b[0m                       ckpt_options, options, filters)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 977\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    978\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    982\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loader, Loader):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on Tom-Ke-Jun-Sungs-MacBook-Pro-AS.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M27mZ7BRVIwj"
   },
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2Qj6dOXUTBe"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJeP9srOVFc0"
   },
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1hisqBBVE5D"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 160\n",
    "steps_per_epoch = len(data_train['sentence_cleaned']) // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 1e-3\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xssjE-re1atV"
   },
   "source": [
    "Record Logs in Tensorboard + Introduce training callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhHh67SV1A2J"
   },
   "outputs": [],
   "source": [
    "from  IPython import display\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "logdir = pathlib.Path('/content/tensorboard_logs')\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogIw191k0PAF"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "  return [\n",
    "    # tfdocs.modeling.EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir/name, update_freq='batch')\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIxvfOHcXdvu"
   },
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-7GPDhR98jsD",
    "outputId": "e2bcd212-163a-4486-9537-fb910e4d47d1"
   },
   "outputs": [],
   "source": [
    "# classifier_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                         loss=loss, metrics=['accuracy'])\n",
    "# classifier_model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "classifier_model.summary()\n",
    "tf.keras.utils.plot_model(classifier_model)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', \n",
    "#               optimizer='adam',\n",
    "#             #   optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtfDFAnN_Neu",
    "outputId": "629beadb-143e-44a1-fdab-afca19c364ff"
   },
   "outputs": [],
   "source": [
    "history = classifier_model.fit(x=data_train[\"sentence_cleaned\"], y=data_train['emotion_enc'],\n",
    "                               batch_size=batch_size, epochs=epochs, verbose=1, callbacks=get_callbacks('BERT'),\n",
    "                               validation_data=[data_val[\"sentence_cleaned\"], data_val['emotion_enc']])\n",
    "\n",
    "# history = classifier_model.fit(x=data_train[\"sentence_cleaned\"], y=data_train['emotion_enc'],\n",
    "#                                epochs=epochs, verbose=1, callbacks=get_callbacks('BERT'),\n",
    "#                                validation_data=[data_val[\"sentence_cleaned\"], data_val['emotion_enc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGARaEzCn-00"
   },
   "source": [
    "#### Predict & See Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC05jqs4JjS-"
   },
   "source": [
    "First, let's do the prediction using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDG7t6Mf4NRp"
   },
   "outputs": [],
   "source": [
    "y_test_predict = classifier_model.predict(data_test[\"sentence_cleaned\"])\n",
    "y_test_predict_encoded = [np.argmax(item) for item in y_test_predict]\n",
    "y_test_actual = [item for item in data_test[\"emotion_enc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S3OX9agJnCC"
   },
   "source": [
    "Obtain Classification Report and Accuracy Score. Save the data in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKGS8e2oAUhK"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test_actual, y_test_predict_encoded)\n",
    "print(\"Accuracy:\",  round(accuracy,3))\n",
    "print(\"Detail:\")\n",
    "classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,\n",
    "      target_names=emotion_label_list[\"emotion\"])\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTUgkkyiFQt2"
   },
   "outputs": [],
   "source": [
    "# Attempting to save this as a pandas dataframe, which is then saved as a CSV\n",
    "classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,\n",
    "      target_names=emotion_label_list[\"emotion\"], output_dict=True)\n",
    "df = pd.DataFrame(classification_report).transpose()\n",
    "df.head()\n",
    "df.to_csv('/content/BERT_ClassificationReport.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhPQhlLKJwgT"
   },
   "source": [
    "Obtain Confusion Matrix and save the resulting figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h4gWQG7Bmm7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cm = sklearn.metrics.confusion_matrix(y_test_actual, y_test_predict_encoded)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Prediction\", ylabel=\"True\", xticklabels=emotion_label_list[\"emotion\"], \n",
    "       yticklabels=emotion_label_list[\"emotion\"], title=\"Confusion Matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "# Save Confusion Matrix\n",
    "fig.savefig('bert_confusionMatrix.png',format='png',bbox_inches=\"tight\",dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCMSX6iAJ4R6"
   },
   "source": [
    "Visualize the NN Training Performance in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLy1ej5fKaDb"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "# Open an embedded TensorBoard viewer\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNpWEEP49g9G",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ~Test~\n",
    "\n",
    "This is test for the imported BERT model. Just for show. It won't be used for the NN itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA_zasFLYo3G"
   },
   "outputs": [],
   "source": [
    "# # text_test = list([['this is such an amazing movie!'],['that was a disaster.']])\n",
    "# text_test = ['this is such an amazing movie']\n",
    "# text_preprocessed = bert_preprocess_model(text_test)\n",
    "# # text_preprocessed = bert_preprocess_model(data_train[\"sentence_cleaned\"])\n",
    "\n",
    "# bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "# print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "# print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n",
    "\n",
    "\n",
    "# print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "# print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "# print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "# print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "# print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj_ZNEvTknNt",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT Model (Using HuggingFace transformers library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNgE8FNJZUm9"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki2YxvB6aLFm"
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # BERT tokenizer\n",
    "# # tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', num_labels=6, do_lower_case=True)\n",
    "# # tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=6, do_lower_case=True)\n",
    "# # test_nlp_recognizer = pipeline(\"sentiment-analysis\", model=\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# # BERT Model\n",
    "# config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "# # config = transformers.AutoConfig(dropout=0.2, attention_dropout=0.2)\n",
    "# # config = transformers.AutoConfig.from_pretrained(\"prajjwal1/bert-tiny\", hidden_dropout_prob=0.2)\n",
    "# config.output_hidden_states = False\n",
    "\n",
    "# nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "# # nlp = transformers.AutoModel.from_pretrained(\"prajjwal1/bert-tiny\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV9CPtK3lgLh"
   },
   "source": [
    "From the website (https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794): <br>\n",
    "*First of all, we need to select the sequence max length. This time Im gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like zzdata is given, BERT would split it into [z, ##z, ##data]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIuzBeJQmpT9"
   },
   "source": [
    "So a summary of the large paragraph above:\n",
    "In BERT, we have three things to keep track of\n",
    "1. Token ID: The \"regular\" tokenization. \n",
    "    *  This includes text start token (101), unknown word (100) token, text end token (102), and padding token (0)\n",
    "2. Mask: Distinguishes between text and padding. 0 indicates padding\n",
    "3. Segment: Keeps track of the text end ([SEP]) token\n",
    "\n",
    "Now, we are not using code from the link above. I have directly used the tokenizer object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tWIPHkdAxZi"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del idx, masks, segments\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# histo_plot_data = np.zeros((3,87))\n",
    "# # Set max length of the sentence\n",
    "# maxlen=72\n",
    "\n",
    "# # I really should be using data cleaning that I used for Word Embedding, but let's have something working first.\n",
    "# def bert_tokenize(corpus, dataset_str):\n",
    "#     histo_plot_data = np.zeros((87))\n",
    "#     # corpus = data_train[\"sentence_cleaned\"]\n",
    "#     idx, masks, segments = [], [], []\n",
    "#     longest = 0\n",
    "#     longest_index = 0;\n",
    "#     feedback_sum = 0\n",
    "\n",
    "#     for i, element in enumerate(corpus):\n",
    "\n",
    "#         tmp = tokenizer(element)\n",
    "#         idx.append(tmp[\"input_ids\"])\n",
    "#         masks.append(tmp[\"attention_mask\"])\n",
    "#         # segments.append(tmp[\"token_type_ids\"])\n",
    "        \n",
    "#         if len(tmp[\"input_ids\"]) > longest:\n",
    "#             longest = len(tmp[\"input_ids\"])\n",
    "#             longest_index = i\n",
    "\n",
    "#         histo_plot_data[len(tmp[\"input_ids\"])-1] += 1\n",
    "#         feedback_sum += len(tmp[\"input_ids\"])\n",
    "\n",
    "#         # corpus_tokenized.append(tmp)\n",
    "#     print(f\"{dataset_str}: Longest sentence using BERT Tokenization is {longest} at index {longest_index}.\")\n",
    "#     print(f\"Average ID length: {feedback_sum/len(corpus)}\")\n",
    "\n",
    "#     for i, element in enumerate(idx):\n",
    "#         tmp = maxlen - len(element)\n",
    "#         if tmp > 0:\n",
    "#             idx[i] += tmp*[0]\n",
    "#             masks[i] += tmp*[0]\n",
    "#             # segments[i] += tmp*[1]\n",
    "#         else:\n",
    "#             idx[i] = idx[i][0:maxlen]\n",
    "#             idx[i][maxlen-1] = 102\n",
    "#             masks[i] = masks[i][0:maxlen]\n",
    "#             # segments[i] = segments[i][0:maxlen]\n",
    "#     return [np.asarray(idx, dtype='int32'), np.asarray(masks, dtype='int32')], histo_plot_data\n",
    "\n",
    "# X_train, histo_plot_data[0,:]  = bert_tokenize(data_train[\"sentence_cleaned\"], 'X_train')\n",
    "# X_val, histo_plot_data[1,:] = bert_tokenize(data_val[\"sentence_cleaned\"], 'X_val')\n",
    "# X_test, histo_plot_data[2,:] = bert_tokenize(data_test[\"sentence_cleaned\"], 'X_test')\n",
    "\n",
    "# # X_train=[np.asarray(idx, dtype='int32'), \n",
    "# #          np.asarray(masks, dtype='int32')]\n",
    "\n",
    "# # Plotting the ID histogram to see the distribution\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.barh(range(1,maxlen+1), histo_plot_data[0,:])\n",
    "# plt.barh(range(1,maxlen+1), histo_plot_data[1,:])\n",
    "# plt.barh(range(1,maxlen+1), histo_plot_data[2,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIjDY0WsNHmv"
   },
   "source": [
    "BERT NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwwmQlfFNG4g"
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras import layers, models, optimizers\n",
    "# import keras\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Inputs\n",
    "# idx = layers.Input((maxlen), dtype='int32',name='input_idx')\n",
    "# masks = layers.Input((maxlen), dtype='int32',name='input_masks')\n",
    "# # segments = layers.Input((maxlen), dtype='int32',name='input_segments')\n",
    "\n",
    "# # Pre-trained BERT\n",
    "# # we already import nlp above, we are using that.\n",
    "\n",
    "# # nlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
    "# bert_out = nlp([idx, masks])\n",
    "\n",
    "# # fine-tuning\n",
    "# x = layers.GlobalAveragePooling1D()(bert_out[0])\n",
    "# # x = layers.Dense(64, activation='relu')(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# # x = layers.Dense(64, activation='sigmoid')(x)\n",
    "# y_out = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# model = models.Model([idx, masks], y_out)\n",
    "\n",
    "# # The BERT model is pre-trained; we don't need to train that layer. Only train the last two Dense layers basically\n",
    "# for layer in model.layers[:4]:\n",
    "#     layer.trainable=False\n",
    "\n",
    "# # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "#             #   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "#             #   metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', \n",
    "#               optimizer='adam',\n",
    "#             #   optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlljgP7fzCZB"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/tensorflow/docs\n",
    "# import tensorflow_docs as tfdocs\n",
    "# import tensorflow_docs.modeling\n",
    "# import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjW1a7SQwK57",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Doing some other testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjxt-OnIlIh8"
   },
   "outputs": [],
   "source": [
    "# Get feature matrix\n",
    "# import re\n",
    "\n",
    "# corpus = data_train[\"sentence\"]\n",
    "\n",
    "# # Choose large maximum length as BERT splits made-up word into subtokens until it recognizes a word\n",
    "# # In our case, the longest sentence is 300 characters, so I'm just going to set maxlen to 300 for now\n",
    "# maxlen = 50\n",
    "\n",
    "# # add special tokens\n",
    "# maxqnans = int((maxlen-20)/2)\n",
    "# corpus_tokenized = [\"[CLS] \"+\n",
    "#              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "#              str(txt).lower().strip()))[:maxqnans])+\n",
    "#              \" [SEP] \" for txt in corpus]\n",
    "\n",
    "# # generate masks\n",
    "# masks = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized]\n",
    "\n",
    "# # Padding\n",
    "# txt2seq = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in corpus_tokenized]\n",
    "\n",
    "# # Generate idx\n",
    "# idx = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq]\n",
    "\n",
    "# ## generate segments\n",
    "# segments = [] \n",
    "# for seq in txt2seq:\n",
    "#     temp, i = [], 0\n",
    "#     for token in seq.split(\" \"):\n",
    "#         temp.append(i)\n",
    "#         if token == \"[SEP]\":\n",
    "#              i += 1\n",
    "#     segments.append(temp)\n",
    "\n",
    "# ## feature matrix\n",
    "# X_train = [np.asarray(idx, dtype='int32'), \n",
    "#            np.asarray(masks, dtype='int32'), \n",
    "#            np.asarray(segments, dtype='int32')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHy-Bp8urB69"
   },
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# print(\"txt: \", data_train[\"sentence\"].iloc[0])\n",
    "# print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "# print(\"idx: \", X_train[0][i])\n",
    "# print(\"mask: \", X_train[1][i])\n",
    "# print(\"segment: \", X_train[2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiEieHRndTzF"
   },
   "outputs": [],
   "source": [
    "# # Testing embedding with the BERT model\n",
    "# # Hidden layer with embeddings\n",
    "# txt = \"bank river\"\n",
    "# input_ids = np.array(tokenizer.encode(txt))[None, :]\n",
    "# embedding = nlp(input_ids)\n",
    "# embedding[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM3f-vhqwJxd"
   },
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow-text==2.8.*\"\n",
    "# !pip install tf-models-official==2.7.0\n",
    "# !pip install official.nlp\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "# from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lUUCZ6tWxyZ6"
   },
   "outputs": [],
   "source": [
    "#@title ~Commented Out: Choose a BERT model to fine-tune~\n",
    "\n",
    "# bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "# map_name_to_handle = {\n",
    "#     'bert_en_uncased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "#     'bert_en_cased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "#     'bert_multi_cased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "#     'albert_en_base':\n",
    "#         'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "#     'electra_small':\n",
    "#         'https://tfhub.dev/google/electra_small/2',\n",
    "#     'electra_base':\n",
    "#         'https://tfhub.dev/google/electra_base/2',\n",
    "#     'experts_pubmed':\n",
    "#         'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "#     'experts_wiki_books':\n",
    "#         'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "#     'talking-heads_base':\n",
    "#         'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "# }\n",
    "\n",
    "# map_model_to_preprocess = {\n",
    "#     'bert_en_uncased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'bert_en_cased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'bert_multi_cased_L-12_H-768_A-12':\n",
    "#         'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "#     'albert_en_base':\n",
    "#         'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "#     'electra_small':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'electra_base':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'experts_pubmed':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'experts_wiki_books':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "#     'talking-heads_base':\n",
    "#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "# }\n",
    "\n",
    "# tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "# tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "# print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "# print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM-23rpTxKiF"
   },
   "outputs": [],
   "source": [
    "# bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "# text_test = [\"this is an amazing movie\"]\n",
    "# text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "# print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "# print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzhHn8vezOfu"
   },
   "outputs": [],
   "source": [
    "# Testing the tokenizer model...\n",
    "\n",
    "# text_test = [\"this is an amazing movie\", \"this is a crappy move on your part\"]\n",
    "# # text_preprocessed = bert_preprocess_model(text_test)\n",
    "# text_preprocessed = tokenizer(text_test)\n",
    "# print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMPQ0676j_BB",
    "tags": []
   },
   "source": [
    "### Histogram: Uncleaned Sentences\n",
    "\n",
    "Let's find out how long the uncleaned sentence is, since the example link does not do data cleaning on the sentences in the sense that common stop words are still provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ylfhpf-EjyGA"
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# import seaborn as sns\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data_train[\"sentence\"])\n",
    "# dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "# X_train = tokenizer.texts_to_sequences(data_train[\"sentence\"])\n",
    "# X_test = tokenizer.texts_to_sequences(data_test[\"sentence\"])\n",
    "# X_val = tokenizer.texts_to_sequences(data_val[\"sentence\"])\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT27TvOxgWWi"
   },
   "outputs": [],
   "source": [
    "# # string_name = ['X_train', 'X_test', 'X_val']\n",
    "# dict_data = {'X_train': X_train,\n",
    "#              'X_test': X_test,\n",
    "#              'X_val': X_val}\n",
    "# histo_plot_data = np.zeros((3,66))\n",
    "\n",
    "# tmp_counter = 0;\n",
    "# for key, value in dict_data.items():\n",
    "#     feedback = 0;\n",
    "#     feedback_sum = 0;\n",
    "#     for i in value:\n",
    "#         histo_plot_data[tmp_counter, len(i)-1] += 1\n",
    "#         feedback_sum += len(i)\n",
    "#         if len(i) > feedback:\n",
    "#             feedback = len(i)\n",
    "#     print(f\"{key}, Longest ID: {feedback}, Average ID length: {feedback_sum/len(value)}\")\n",
    "#     tmp_counter += 1\n",
    "# del tmp_counter\n",
    "\n",
    "# # Longest sentence has 35 elements. Average is around 10.\n",
    "# # TODO: This value, which influences padding, should be adjusted I think...\n",
    "# # maxlen = 20\n",
    "\n",
    "# # Delete unneeded variables\n",
    "# del X_train, X_test, X_val, vocab_size, dic_vocabulary, tokenizer, feedback\n",
    "# del feedback_sum, dict_data, tmp_counter, i, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6fuYFdZkTdJ"
   },
   "outputs": [],
   "source": [
    "# # Plotting the ID histogram to see the distribution\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.barh(range(1,66+1), histo_plot_data[0,:])\n",
    "# plt.barh(range(1,66+1), histo_plot_data[1,:])\n",
    "# plt.barh(range(1,66+1), histo_plot_data[2,:])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ifCzi282UO64",
    "UFTJfJkrO06u",
    "LWVt__H7R9zl",
    "0DW_8szfEo1T",
    "xNpWEEP49g9G",
    "rj_ZNEvTknNt",
    "fjW1a7SQwK57",
    "nMPQ0676j_BB"
   ],
   "name": "EECE_571T_Project_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eece571T",
   "language": "python",
   "name": "eece571t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
