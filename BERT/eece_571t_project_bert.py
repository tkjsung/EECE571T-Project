# -*- coding: utf-8 -*-
"""EECE_571T_Project_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11BZAEYf3P4PV-IyMOiB877yPV3rEw8jm

# EECE 571T Project - NLP with Emotion Dataset (BERT)

Focus: BERT
<br>
Author: Tom Sung

Last updated:
* Date: April 5, 2022
* Time: 5:30pm
"""

# Check detected system hardware resources.
# import os
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print("Num CPUs Available: ", len(tf.config.experimental.list_physical_devices('CPU')))

"""## References

* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification
* From same author:
    * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)
    * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d
* Different Pre-Processing Techniques with Bag of Words w/ TF-IDF, Word Embedding, and BERT: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794

# Get data from GitHub repo

**Only run this once even after if notebook environment is cleared via** `%reset -f`. The code written here imports the Kaggle data set, which I have placed on my public GitHub repo.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/train.txt
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/test.txt
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T_Dataset/master/Project/val.txt

"""# Import Data"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries for data import
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline

# Read CSV
data_train = pd.read_csv('train.txt',sep=';', header=None)
data_test = pd.read_csv('test.txt',sep=';', header=None)
data_val = pd.read_csv('val.txt',sep=';', header=None)

col_names = ["sentence","emotion"]
data_train.columns = col_names
data_test.columns = col_names
data_val.columns = col_names

# See the data head to make sure data is imported correctly.
data_train.head()
# data_test.head()
# data_val.head()

"""# Encode the emotion labels with unique identifiers"""

from sklearn.preprocessing import LabelEncoder
# Encode the emotion labels with unique identifiers
data_train['emotion'].unique()
labelencoder = LabelEncoder()
data_train['emotion_enc'] = labelencoder.fit_transform(data_train['emotion'])
data_test['emotion_enc'] = labelencoder.fit_transform(data_test['emotion'])
data_val['emotion_enc'] = labelencoder.fit_transform(data_val['emotion'])
# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below.

"""Sort the encoded emotion labels for some classification reports later"""

emotion_label_list = data_train[['emotion','emotion_enc']].drop_duplicates(keep='first')
# data_test[['emotion','emotion_enc']].drop_duplicates(keep='first')
# data_val[['emotion','emotion_enc']].drop_duplicates(keep='first')
emotion_label_list = emotion_label_list.sort_values(by='emotion_enc')
emotion_label_list.iloc[0]

data_train.head()
data_test.head()
data_val.head()

"""# Data Cleaning

We need to do some data cleaning first~, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words...~

**Data Cleaning Process:** Keep only words, convert all words to lowercase, split all words, remove stopwords, lemmization for word root.<br>
The result of all of this work is a cleaned data vocab list.

Replace stemming with lemmization, which keeps the actual form of the word better. This is necessary for using pre-existing word embedding models.
Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/
"""

import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Attempting data cleaning here
def preprocess(raw_text):
    # keep only words
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    # convert to lower case and split 
    words = letters_only_text.lower().split()

    # remove stopwords
    stopword_set = set(stopwords.words("english"))
    meaningful_words = [w for w in words if w not in stopword_set]
    
    # stemmed words (looks like this is causing some words to be weird)
    # ps = PorterStemmer()
    # stemmed_words = [ps.stem(word) for word in meaningful_words]

    # lemmed words (trying this because this gets the root word?)
    lem = WordNetLemmatizer()
    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]
    
    # join the cleaned words in a list
    # cleaned_word_list = " ".join(stemmed_words)
    cleaned_word_list = " ".join(lemmed_words)
    # cleaned_word_list = " ".join(meaningful_words)

    return cleaned_word_list

"""Apply data cleaning to all data sets."""

data_train['sentence_cleaned'] = data_train['sentence'].apply(lambda line : preprocess(line))
data_test['sentence_cleaned'] = data_test['sentence'].apply(lambda line : preprocess(line))
data_val['sentence_cleaned'] = data_val['sentence'].apply(lambda line : preprocess(line))

"""# Pre-Processing and Training

Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>
There are several methods available: Bag-of-words with TF-IDF, Word Embedding using ~Word2Vec~ [I used GloVe, not Word2Vec] (unknown NN), and BERT.

## METHOD 2: BERT

### BERT Model (Using Google's Tensorflow Tutorial)

[Link](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=_OoF9mebuSZc)
"""

# A dependency of the preprocessing for BERT inputs
!pip install -q -U "tensorflow-text==2.8.*"
!pip install -q tf-models-official==2.7.0

import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')

"""Defining Model"""

# Get BERT Model

# bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'

# 
tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'

# These will be used in the NN model. These will be layers in the NN model
# bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) # This is the pre-processing model
# bert_model = hub.KerasLayer(tfhub_handle_encoder) # This is the actual BERT model

def build_classifier_model():
    # Input
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='txt')

    # BERT Pre-Processing Layer
    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing') # This is the pre-processing model
    encoder_inputs = bert_preprocess_model(text_input)

    # BERT Model Layer
    bert_model = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder') # This is the actual BERT model
    outputs = bert_model(encoder_inputs)

    # Fine-tuning. First, Average pooling
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.2)(net)
    net = tf.keras.layers.Dense(6, activation='softmax', name='classifier')(net)
    model = tf.keras.Model(text_input, net)
    return model

classifier_model = build_classifier_model()

"""Loss Function"""

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
metrics = tf.metrics.SparseCategoricalAccuracy()

"""Optimizer"""

epochs = 10
batch_size = 160
steps_per_epoch = len(data_train['sentence_cleaned']) // batch_size
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 1e-3
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

"""Record Logs in Tensorboard + Introduce training callback"""

from  IPython import display
import pathlib
import shutil
import tempfile

# logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
logdir = pathlib.Path('/content/tensorboard_logs')
shutil.rmtree(logdir, ignore_errors=True)

def get_callbacks(name):
  return [
    # tfdocs.modeling.EpochDots(),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),
    tf.keras.callbacks.TensorBoard(log_dir=logdir/name, update_freq='batch')
  ]

"""Compile Model"""

classifier_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
# classifier_model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])


classifier_model.summary()
tf.keras.utils.plot_model(classifier_model)

# model.compile(loss='sparse_categorical_crossentropy', 
#               optimizer='adam',
#             #   optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
#               metrics=['accuracy'])

history = classifier_model.fit(x=data_train["sentence_cleaned"], y=data_train['emotion_enc'],
                               batch_size=batch_size, epochs=epochs, verbose=1, callbacks=get_callbacks('BERT'),
                               validation_data=[data_val["sentence_cleaned"], data_val['emotion_enc']])

# history = classifier_model.fit(x=data_train["sentence_cleaned"], y=data_train['emotion_enc'],
#                                epochs=epochs, verbose=1, callbacks=get_callbacks('BERT'),
#                                validation_data=[data_val["sentence_cleaned"], data_val['emotion_enc']])

"""#### Predict & See Results

First, let's do the prediction using the test dataset.
"""

y_test_predict = classifier_model.predict(data_test["sentence_cleaned"])
y_test_predict_encoded = [np.argmax(item) for item in y_test_predict]
y_test_actual = [item for item in data_test["emotion_enc"]]

"""Obtain Classification Report and Accuracy Score. Save the data in a CSV file."""

import sklearn
accuracy = sklearn.metrics.accuracy_score(y_test_actual, y_test_predict_encoded)
print("Accuracy:",  round(accuracy,3))
print("Detail:")
classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,
      target_names=emotion_label_list["emotion"])
print(classification_report)

# Attempting to save this as a pandas dataframe, which is then saved as a CSV
classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,
      target_names=emotion_label_list["emotion"], output_dict=True)
df = pd.DataFrame(classification_report).transpose()
df.head()
df.to_csv('/content/BERT_ClassificationReport.csv')

"""Obtain Confusion Matrix and save the resulting figure."""

import seaborn as sns
cm = sklearn.metrics.confusion_matrix(y_test_actual, y_test_predict_encoded)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
            cbar=False)
ax.set(xlabel="Prediction", ylabel="True", xticklabels=emotion_label_list["emotion"], 
       yticklabels=emotion_label_list["emotion"], title="Confusion Matrix")
plt.yticks(rotation=0)
# Save Confusion Matrix
fig.savefig('bert_confusionMatrix.png',format='png',bbox_inches="tight",dpi=400)

"""Visualize the NN Training Performance in Tensorboard."""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
# %reload_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}

"""#### ~Test~

This is test for the imported BERT model. Just for show. It won't be used for the NN itself.
"""

# # text_test = list([['this is such an amazing movie!'],['that was a disaster.']])
# text_test = ['this is such an amazing movie']
# text_preprocessed = bert_preprocess_model(text_test)
# # text_preprocessed = bert_preprocess_model(data_train["sentence_cleaned"])

# bert_results = bert_model(text_preprocessed)

# print(f'Keys       : {list(text_preprocessed.keys())}')
# print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
# print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
# print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
# print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')


# print(f'Loaded BERT: {tfhub_handle_encoder}')
# print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
# print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
# print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
# print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')

"""### BERT Model (Using HuggingFace transformers library)"""

# !pip install transformers
# import transformers

# from transformers import pipeline

# # BERT tokenizer
# # tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', num_labels=6, do_lower_case=True)
# # tokenizer = transformers.AutoTokenizer.from_pretrained("prajjwal1/bert-tiny", num_labels=6, do_lower_case=True)
# # test_nlp_recognizer = pipeline("sentiment-analysis", model="prajjwal1/bert-tiny")

# # BERT Model
# config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)
# # config = transformers.AutoConfig(dropout=0.2, attention_dropout=0.2)
# # config = transformers.AutoConfig.from_pretrained("prajjwal1/bert-tiny", hidden_dropout_prob=0.2)
# config.output_hidden_states = False

# nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)
# # nlp = transformers.AutoModel.from_pretrained("prajjwal1/bert-tiny", config=config)

"""From the website (https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794): <br>
*First of all, we need to select the sequence max length. This time I’m gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like “zzdata” is given, BERT would split it into [“z”, “##z”, “##data”]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length*

So a summary of the large paragraph above:
In BERT, we have three things to keep track of
1. Token ID: The "regular" tokenization. 
    *  This includes text start token (101), unknown word (100) token, text end token (102), and padding token (0)
2. Mask: Distinguishes between text and padding. 0 indicates padding
3. Segment: Keeps track of the text end ([SEP]) token

Now, we are not using code from the link above. I have directly used the tokenizer object itself.
"""

# try:
#     del idx, masks, segments
# except:
#     pass

# histo_plot_data = np.zeros((3,87))
# # Set max length of the sentence
# maxlen=72

# # I really should be using data cleaning that I used for Word Embedding, but let's have something working first.
# def bert_tokenize(corpus, dataset_str):
#     histo_plot_data = np.zeros((87))
#     # corpus = data_train["sentence_cleaned"]
#     idx, masks, segments = [], [], []
#     longest = 0
#     longest_index = 0;
#     feedback_sum = 0

#     for i, element in enumerate(corpus):

#         tmp = tokenizer(element)
#         idx.append(tmp["input_ids"])
#         masks.append(tmp["attention_mask"])
#         # segments.append(tmp["token_type_ids"])
        
#         if len(tmp["input_ids"]) > longest:
#             longest = len(tmp["input_ids"])
#             longest_index = i

#         histo_plot_data[len(tmp["input_ids"])-1] += 1
#         feedback_sum += len(tmp["input_ids"])

#         # corpus_tokenized.append(tmp)
#     print(f"{dataset_str}: Longest sentence using BERT Tokenization is {longest} at index {longest_index}.")
#     print(f"Average ID length: {feedback_sum/len(corpus)}")

#     for i, element in enumerate(idx):
#         tmp = maxlen - len(element)
#         if tmp > 0:
#             idx[i] += tmp*[0]
#             masks[i] += tmp*[0]
#             # segments[i] += tmp*[1]
#         else:
#             idx[i] = idx[i][0:maxlen]
#             idx[i][maxlen-1] = 102
#             masks[i] = masks[i][0:maxlen]
#             # segments[i] = segments[i][0:maxlen]
#     return [np.asarray(idx, dtype='int32'), np.asarray(masks, dtype='int32')], histo_plot_data

# X_train, histo_plot_data[0,:]  = bert_tokenize(data_train["sentence_cleaned"], 'X_train')
# X_val, histo_plot_data[1,:] = bert_tokenize(data_val["sentence_cleaned"], 'X_val')
# X_test, histo_plot_data[2,:] = bert_tokenize(data_test["sentence_cleaned"], 'X_test')

# # X_train=[np.asarray(idx, dtype='int32'), 
# #          np.asarray(masks, dtype='int32')]

# # Plotting the ID histogram to see the distribution
# import matplotlib.pyplot as plt
# plt.barh(range(1,maxlen+1), histo_plot_data[0,:])
# plt.barh(range(1,maxlen+1), histo_plot_data[1,:])
# plt.barh(range(1,maxlen+1), histo_plot_data[2,:])

"""BERT NN"""

# from keras.models import Sequential
# from keras import layers, models, optimizers
# import keras
# import tensorflow as tf

# # Inputs
# idx = layers.Input((maxlen), dtype='int32',name='input_idx')
# masks = layers.Input((maxlen), dtype='int32',name='input_masks')
# # segments = layers.Input((maxlen), dtype='int32',name='input_segments')

# # Pre-trained BERT
# # we already import nlp above, we are using that.

# # nlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')
# bert_out = nlp([idx, masks])

# # fine-tuning
# x = layers.GlobalAveragePooling1D()(bert_out[0])
# # x = layers.Dense(64, activation='relu')(x)
# x = layers.Dense(64, activation='relu')(x)
# # x = layers.Dense(64, activation='sigmoid')(x)
# y_out = layers.Dense(6, activation='softmax')(x)

# model = models.Model([idx, masks], y_out)

# # The BERT model is pre-trained; we don't need to train that layer. Only train the last two Dense layers basically
# for layer in model.layers[:4]:
#     layer.trainable=False

# # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
#             #   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
#             #   metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

# model.compile(loss='sparse_categorical_crossentropy', 
#               optimizer='adam',
#             #   optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
#               metrics=['accuracy'])
# model.summary()

# !pip install git+https://github.com/tensorflow/docs
# import tensorflow_docs as tfdocs
# import tensorflow_docs.modeling
# import tensorflow_docs.plots

"""### Doing some other testing here"""

# Get feature matrix
# import re

# corpus = data_train["sentence"]

# # Choose large maximum length as BERT splits made-up word into subtokens until it recognizes a word
# # In our case, the longest sentence is 300 characters, so I'm just going to set maxlen to 300 for now
# maxlen = 50

# # add special tokens
# maxqnans = int((maxlen-20)/2)
# corpus_tokenized = ["[CLS] "+
#              " ".join(tokenizer.tokenize(re.sub(r'[^\w\s]+|\n', '', 
#              str(txt).lower().strip()))[:maxqnans])+
#              " [SEP] " for txt in corpus]

# # generate masks
# masks = [[1]*len(txt.split(" ")) + [0]*(maxlen - len(txt.split(" "))) for txt in corpus_tokenized]

# # Padding
# txt2seq = [txt + " [PAD]"*(maxlen-len(txt.split(" "))) if len(txt.split(" ")) != maxlen else txt for txt in corpus_tokenized]

# # Generate idx
# idx = [tokenizer.encode(seq.split(" ")) for seq in txt2seq]

# ## generate segments
# segments = [] 
# for seq in txt2seq:
#     temp, i = [], 0
#     for token in seq.split(" "):
#         temp.append(i)
#         if token == "[SEP]":
#              i += 1
#     segments.append(temp)

# ## feature matrix
# X_train = [np.asarray(idx, dtype='int32'), 
#            np.asarray(masks, dtype='int32'), 
#            np.asarray(segments, dtype='int32')]

# i = 3
# print("txt: ", data_train["sentence"].iloc[0])
# print("tokenized:", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])
# print("idx: ", X_train[0][i])
# print("mask: ", X_train[1][i])
# print("segment: ", X_train[2][i])

# # Testing embedding with the BERT model
# # Hidden layer with embeddings
# txt = "bank river"
# input_ids = np.array(tokenizer.encode(txt))[None, :]
# embedding = nlp(input_ids)
# embedding[0][0]

# !pip install "tensorflow-text==2.8.*"
# !pip install tf-models-official==2.7.0
# !pip install official.nlp

# import tensorflow as tf
# import tensorflow_hub as hub
# import tensorflow_text as text
# from official.nlp import optimization

#@title ~Commented Out: Choose a BERT model to fine-tune~

# bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param ["bert_en_uncased_L-12_H-768_A-12", "bert_en_cased_L-12_H-768_A-12", "bert_multi_cased_L-12_H-768_A-12", "small_bert/bert_en_uncased_L-2_H-128_A-2", "small_bert/bert_en_uncased_L-2_H-256_A-4", "small_bert/bert_en_uncased_L-2_H-512_A-8", "small_bert/bert_en_uncased_L-2_H-768_A-12", "small_bert/bert_en_uncased_L-4_H-128_A-2", "small_bert/bert_en_uncased_L-4_H-256_A-4", "small_bert/bert_en_uncased_L-4_H-512_A-8", "small_bert/bert_en_uncased_L-4_H-768_A-12", "small_bert/bert_en_uncased_L-6_H-128_A-2", "small_bert/bert_en_uncased_L-6_H-256_A-4", "small_bert/bert_en_uncased_L-6_H-512_A-8", "small_bert/bert_en_uncased_L-6_H-768_A-12", "small_bert/bert_en_uncased_L-8_H-128_A-2", "small_bert/bert_en_uncased_L-8_H-256_A-4", "small_bert/bert_en_uncased_L-8_H-512_A-8", "small_bert/bert_en_uncased_L-8_H-768_A-12", "small_bert/bert_en_uncased_L-10_H-128_A-2", "small_bert/bert_en_uncased_L-10_H-256_A-4", "small_bert/bert_en_uncased_L-10_H-512_A-8", "small_bert/bert_en_uncased_L-10_H-768_A-12", "small_bert/bert_en_uncased_L-12_H-128_A-2", "small_bert/bert_en_uncased_L-12_H-256_A-4", "small_bert/bert_en_uncased_L-12_H-512_A-8", "small_bert/bert_en_uncased_L-12_H-768_A-12", "albert_en_base", "electra_small", "electra_base", "experts_pubmed", "experts_wiki_books", "talking-heads_base"]

# map_name_to_handle = {
#     'bert_en_uncased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
#     'bert_en_cased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
#     'bert_multi_cased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
#     'small_bert/bert_en_uncased_L-2_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-2_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-2_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-2_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
#     'small_bert/bert_en_uncased_L-4_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-4_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-4_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-4_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
#     'small_bert/bert_en_uncased_L-6_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-6_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-6_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-6_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
#     'small_bert/bert_en_uncased_L-8_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-8_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-8_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-8_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
#     'small_bert/bert_en_uncased_L-10_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-10_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-10_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-10_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
#     'small_bert/bert_en_uncased_L-12_H-128_A-2':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
#     'small_bert/bert_en_uncased_L-12_H-256_A-4':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
#     'small_bert/bert_en_uncased_L-12_H-512_A-8':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
#     'small_bert/bert_en_uncased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
#     'albert_en_base':
#         'https://tfhub.dev/tensorflow/albert_en_base/2',
#     'electra_small':
#         'https://tfhub.dev/google/electra_small/2',
#     'electra_base':
#         'https://tfhub.dev/google/electra_base/2',
#     'experts_pubmed':
#         'https://tfhub.dev/google/experts/bert/pubmed/2',
#     'experts_wiki_books':
#         'https://tfhub.dev/google/experts/bert/wiki_books/2',
#     'talking-heads_base':
#         'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
# }

# map_model_to_preprocess = {
#     'bert_en_uncased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'bert_en_cased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',
#     'small_bert/bert_en_uncased_L-2_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-2_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-2_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-2_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-4_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-4_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-4_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-4_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-6_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-6_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-6_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-6_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-8_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-8_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-8_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-8_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-10_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-10_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-10_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-10_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-12_H-128_A-2':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-12_H-256_A-4':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-12_H-512_A-8':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'small_bert/bert_en_uncased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'bert_multi_cased_L-12_H-768_A-12':
#         'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',
#     'albert_en_base':
#         'https://tfhub.dev/tensorflow/albert_en_preprocess/3',
#     'electra_small':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'electra_base':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'experts_pubmed':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'experts_wiki_books':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
#     'talking-heads_base':
#         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
# }

# tfhub_handle_encoder = map_name_to_handle[bert_model_name]
# tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

# print(f'BERT model selected           : {tfhub_handle_encoder}')
# print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

# bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
# text_test = ["this is an amazing movie"]
# text_preprocessed = bert_preprocess_model(text_test)

# print(f'Keys       : {list(text_preprocessed.keys())}')
# print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
# print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
# print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
# print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

# Testing the tokenizer model...

# text_test = ["this is an amazing movie", "this is a crappy move on your part"]
# # text_preprocessed = bert_preprocess_model(text_test)
# text_preprocessed = tokenizer(text_test)
# print(text_preprocessed)

"""### Histogram: Uncleaned Sentences

Let's find out how long the uncleaned sentence is, since the example link does not do data cleaning on the sentences in the sense that common stop words are still provided.
"""

# from keras.preprocessing.text import Tokenizer
# from keras.preprocessing.sequence import pad_sequences
# import seaborn as sns

# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(data_train["sentence"])
# dic_vocabulary = tokenizer.word_index

# X_train = tokenizer.texts_to_sequences(data_train["sentence"])
# X_test = tokenizer.texts_to_sequences(data_test["sentence"])
# X_val = tokenizer.texts_to_sequences(data_val["sentence"])

# vocab_size = len(tokenizer.word_index) + 1

# # string_name = ['X_train', 'X_test', 'X_val']
# dict_data = {'X_train': X_train,
#              'X_test': X_test,
#              'X_val': X_val}
# histo_plot_data = np.zeros((3,66))

# tmp_counter = 0;
# for key, value in dict_data.items():
#     feedback = 0;
#     feedback_sum = 0;
#     for i in value:
#         histo_plot_data[tmp_counter, len(i)-1] += 1
#         feedback_sum += len(i)
#         if len(i) > feedback:
#             feedback = len(i)
#     print(f"{key}, Longest ID: {feedback}, Average ID length: {feedback_sum/len(value)}")
#     tmp_counter += 1
# del tmp_counter

# # Longest sentence has 35 elements. Average is around 10.
# # TODO: This value, which influences padding, should be adjusted I think...
# # maxlen = 20

# # Delete unneeded variables
# del X_train, X_test, X_val, vocab_size, dic_vocabulary, tokenizer, feedback
# del feedback_sum, dict_data, tmp_counter, i, key, value

# # Plotting the ID histogram to see the distribution
# import matplotlib.pyplot as plt

# plt.barh(range(1,66+1), histo_plot_data[0,:])
# plt.barh(range(1,66+1), histo_plot_data[1,:])
# plt.barh(range(1,66+1), histo_plot_data[2,:])