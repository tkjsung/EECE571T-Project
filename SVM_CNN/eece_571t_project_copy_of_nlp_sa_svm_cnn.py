# -*- coding: utf-8 -*-
"""EECE 571T Project: Copy of NLP_SA_SVM_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cc1SlmBhHXzGi-hF99fl6M-MuF2w7w6Y

## General Introduction

This is for EECE571 Advanced Machine Learning Tools project which focuses on sentiment analysis. The dataset for this project can be found here: https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp

In this colab file, we perform text vectorization which is the first step of the NLP.

The text vectorization method used in the file are based on BoW model and TF-IDF model.

Both models can be found in sklearn: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words

For specific BoW guide: https://www.einfochips.com/blog/nlp-text-vectorization/

For TF-IDF guide: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
https://towardsdatascience.com/text-vectorization-term-frequency-inverse-document-frequency-tfidf-5a3f9604da6d

However, before doing text vectorization we first need to do data cleasing i.e., lemmatization, stopwords removal, etc. 

For the CNN model, we use word embedding to vectorize the text input and then use CNN for the modlel.
word embedding: https://machinelearningmastery.com/what-are-word-embeddings/

CNN on SA: https://d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html

##Import the necessary libraries
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
import re
import pprint
import numpy as np

"""##Load the dataset from Github

the guide: https://www.youtube.com/watch?v=rAw-G3VEs6Q

Note: the Github project should be public.
"""

url_test = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/raw/main/test.txt'
url_train = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/blob/main/train.txt?raw=true'
url_val = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/raw/main/val.txt'

# read in the data from github link above
test_data = pd.read_csv(url_test, sep=';',header = None,names=['content','label'])
train_data = pd.read_csv(url_train, sep=';',header = None,names=['content','label'])
val_data = pd.read_csv(url_val, sep=';',header = None,names=['content','label'])

# split the dataFrame into input and output dataset
X_test = test_data.content
y_test = test_data.label
X_train = train_data.content
y_train = train_data.label
X_val = val_data.content
y_val = val_data.label

print(f'shape of x_test_data {X_test.shape} and y_test_data {y_test.shape}')
print(f'shape of x_train_data {X_train.shape} and y_train_data {y_train.shape}')
print(f'shape of x_val_data {X_val.shape} and y_val_data {y_val.shape}')
X_test[9]

from pandas import option_context

with option_context('display.max_colwidth', 400):
    display(train_data.head())

import matplotlib.pyplot as plt

train_data.label.value_counts().plot.bar()
plt.title('Training Dataset Classes')
plt.show()
test_data.label.value_counts().plot.bar()
plt.title('Testing Dataset Classes')
plt.show()
val_data.label.value_counts().plot.bar()
plt.title('Validation Dataset Classes')
plt.show()

"""###y Label to one hot encoding

y Label to numerical value: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html

y label to one hot encoding: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html

Not necessary for SVM as SVM support text into and do the classification automatically.
"""

'''
from sklearn.preprocessing import LabelBinarizer
lb = LabelBinarizer()
y_train = lb.fit_transform(y_train)
y_test = lb.transform(y_test)
y_val = lb.transform(y_val)
print(y_train.shape, y_test.shape, y_val.shape)
'''

"""##Data preprocessing for test_data, train_data, and val_data

Case lowerdown, Stopwords removal, lemmatization
"""

# download the nltk stopwords list
nltk.download('stopwords')
stops = set(stopwords.words('english'))
stops.add('im')
print(f'the length of the stopwords list is {len(stops)}')

# import the lemmitizer from nltk

# download wordnet as WordNetLemmatizer uses wordnet's morphy function
nltk.download('wordnet') 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

"""Text Cleansing function for stopwords removal and lemmatization"""

def text_cleansing(ds, stops):

  corp = []

  for row in ds:

    # remove any sign that is not a letter
    new_row = re.sub('[^a-zA-Z]',' ',str(row))
    new_row = new_row.lower()
    # remove stopwords and lemmatize 
    new_row = [lemmatizer.lemmatize(word) for word in new_row.split() if word not in stops]

    corp.append(' '.join(str(x) for x in new_row))

  return corp

"""One possible issue for stopwors removal is that word $\color{red}{\text{im}}$ is not in the stopword list. Better to visualize the word frequency.

"""

X_test = text_cleansing(X_test,stops)
X_train = text_cleansing(X_train,stops)
X_val = text_cleansing(X_val,stops)

"""## Text Visulaization

###$\color{red}{\text{Optional Word frequency visulization function}}$
"""

# Commented out IPython magic to ensure Python compatibility.
# download the ntlk punkt dataset for textblob
nltk.download('punkt')
from textblob import TextBlob
from operator import itemgetter
# %matplotlib inline
import matplotlib.pyplot as plt

def word_visual(data_clean):
  # create a string of cleaned data
  data_corpus = ' '.join(data_clean)
  # create TextBlob
  data_blob = TextBlob(data_corpus)
  # get the word-frequency pair
  data_items = data_blob.word_counts.items() 
  # sort the dict_item
  data_items_sorted = sorted(data_items,key=itemgetter(1),reverse = True)
  # plot the top 200 words
  data_items_df = pd.DataFrame(data_items_sorted[0:50],columns = ['word','counts'])
  data_items_axes = data_items_df.plot.bar(x='word',y='counts',legend =False)
  plt.gcf().tight_layout()

word_visual(X_train)
word_visual(X_test)
word_visual(X_val)

"""###$\color{red}{\text{Optional Word Cloud Visualization}}$"""

# Commented out IPython magic to ensure Python compatibility.
# import the related libraries in order to open pic from github 
from PIL import Image
import requests
from io import BytesIO
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt

# import the library for word cloud
from wordcloud import WordCloud

def word_cloud_visual(data_clean):

  # Pic link from Github
  url = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/raw/main/mask_heart.png'
  
  # read the pic from github as ndarray
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  img = np.asanyarray(img)
  
  # word cloud config
  wordcloud = WordCloud(width = 1600, height = 1200, colormap='prism',mask = img,background_color='white')
  wordcloud = wordcloud.generate(' '.join(data_clean))
  plt.imshow(wordcloud)

word_cloud_visual(X_train)

word_cloud_visual(X_test)

word_cloud_visual(X_val)

"""## Vectorize preprocessed data with BoW model and TF-IDF

For BoW model: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer

For TF-IDF model: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#

General Guidance for Text Feature Extraction: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

https://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-reff
"""

# BoW Vectorizer
from sklearn.feature_extraction.text import CountVectorizer
from numpy import save   # save numpy data to .npy file so that, it can be load quickly

vec_BoW = CountVectorizer()
X_train_BoW = vec_BoW.fit_transform(X_train)
X_test_BoW = vec_BoW.transform(X_test)
X_val_BoW = vec_BoW.transform(X_val)
print(vec_BoW.get_feature_names_out().shape,X_train_BoW.shape,X_test_BoW.shape,X_val_BoW.shape)
print(type(X_train_BoW),type(X_test_BoW),type(X_val_BoW))

# TF-IDF Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

vec_Tfidf = TfidfVectorizer()
X_train_Tfidf = vec_Tfidf.fit_transform(X_train)
X_test_Tfidf = vec_Tfidf.transform(X_test)
X_val_Tfidf = vec_Tfidf.transform(X_val)
print(vec_Tfidf.get_feature_names_out().shape,X_train_Tfidf.shape,X_test_Tfidf.shape,X_val_Tfidf.shape)
print(type(X_train_Tfidf),type(X_test_Tfidf),type(X_val_Tfidf))

"""The result from BoW and TFIDF models are sparse matrix:

https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/

# <font color='red'>SVM on the Preprocessed data </font>

**SVM result is quite consisent, for both BOW and TFIDF the accuracy on test dataset is 89.6%**

Guide on SVM: https://scikit-learn.org/stable/modules/svm.html#svm-classification

## function to display classification result
"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def clf_result(model_name, expected, predicted,classes):
  print(f'{model_name} performance:\n')

  print('Classification report:\n')
  print(classification_report(expected,predicted))

  print('Confusion matrix:\n')
  clf_matrix = confusion_matrix(expected, predicted,labels=classes)
  print(clf_matrix)

  print('\nConfusion matrix heat map:\n')
  disp = ConfusionMatrixDisplay(clf_matrix,display_labels=classes)
  disp.plot(cmap=plt.cm.Blues,colorbar=False)
  plt.show()

"""## SVM svc on BoW model"""

# Commented out IPython magic to ensure Python compatibility.
# import the svc model and do the training

from sklearn.svm import LinearSVC

svc_linear_BoW = LinearSVC()
# %timeit svc_linear_BoW.fit(X_train_BoW,y_train)

# report the accuracy result
y_predict = svc_linear_BoW.predict(X_test_BoW)
svc_linear_BoW.score(X_test_BoW,y_test)

clf_result('svc_linear_BoW on Tesing dataset',y_test,y_predict,svc_linear_BoW.classes_)

"""## SVM svc on TFIDF Model"""

# Commented out IPython magic to ensure Python compatibility.
svc_linear_Tfidf = LinearSVC()

# %timeit svc_linear_Tfidf.fit(X_train_Tfidf,y_train)

y_predict = svc_linear_Tfidf.predict(X_test_Tfidf)
svc_linear_Tfidf.score(X_test_Tfidf,y_test)

clf_result('svc_linear_Tfidf on Testing dataset',y_test,y_predict,svc_linear_Tfidf.classes_)

"""## Try Dimensinal Reduction

### TruncatedSVD (similar to PCA) on BoW Model and see the result.

more info: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html

The result shows that diemsion reduction takes longer to finish training and the result is not better than original one even with 2000 features. 
The reason is the orignal input is a sparse matrix, however, after dimension reduction, the matrix changes into a dense matrix, thus it takes longer for the algorithm to run.

**Conlcusion: Dimension Reduction Not suggested.**
"""

from sklearn.decomposition import TruncatedSVD

N = [500, 1000, 1500, 2000]

trunc = TruncatedSVD(n_components=2000)

X_train_BoW_Trunc = trunc.fit(X_train_BoW)
X_test_BoW_Trunc = trunc.transform(X_test_BoW)

X_train_BoW_Trunc = trunc.transform(X_train_BoW)
X_train_BoW_Trunc.shape

"""### train the dimensional reduced data with svc"""

# Commented out IPython magic to ensure Python compatibility.
svc_linear_BoW_pca = LinearSVC()
# %timeit svc_linear_BoW_pca.fit(X_train_BoW_Trunc,y_train)

y_predict = svc_linear_BoW_pca.predict(X_test_BoW_Trunc)
svc_linear_BoW_pca.score(X_test_BoW_Trunc,y_test)

"""**The result after dimensional reduction is a little bit worse than the orignal one, and takes much longer to work.**"""

clf_result('SVC for BoW model with dimension reduction on Tesing dataset',y_test,y_predict,svc_linear_BoW_pca.classes_)

"""## SVM parameter Tuning

Here we try differnt regularization parameter C for the training problem.

C = [0.01, 0.1, 10, 100] and check the result on the validation dataset; select the C setting that gives the best performance;

With this C setting, report the classification on the tesing dataset.
"""

# this function takes input of BoW and TFIDF word vectorizing models
# then construct different svcs with different regularization parameter
# find the best parameters setting and test that parameter on validaion dataset and report the result.

def svc_tuning(input_type_name,Xtrain, ytrain, Xtest, ytest, Xval, yval):
  regus = [0.01, 0.1, 1, 10, 100]
  scores= []

  for regu in regus:
    # Construct the svc
    svc = LinearSVC(C=regu)
    svc.fit(Xtrain,ytrain)
    scores.append(svc.score(Xval,yval))
    print(f'Reguarization = {regu:<4}, Accuracy on Validation dataset is: {svc.score(Xval,yval)}')

  # find the index of the regu setting that gives the highest score on test dataset and calculate the generalization accuracy
  idx = [i for i in range(len(scores)) if scores[i]==max(scores)]
  print('\n*****************************************************************************************************************************\n')

  print(f'The regularization parameter that gives the highest accuracy on the Validation datset is C={regus[idx[0]]}, and the accuracy is {scores[idx[0]]}\n')

  #Try the best regularization parameter on the validation dataset
  svc = LinearSVC(C=regus[idx[0]])
  svc.fit(Xtrain,ytrain)

  y_predict = svc.predict(Xtest)
  print(f'The classification accuracy for C={regus[idx[0]]} on Tesing dataset is {svc.score(Xtest,ytest)}.\n')

  clf_result(f'svc_linear_{input_type_name} on Testing dataset with C={regus[idx[0]]}', ytest, y_predict,svc.classes_)

svc_tuning('BoW',X_train_BoW,y_train,X_test_BoW,y_test,X_val_BoW,y_val)

svc_tuning('Tfidf',X_train_Tfidf,y_train,X_test_Tfidf,y_test,X_val_Tfidf,y_val)

"""#<font color='red'>CNN on the Preprocessed data</font>

**After Parameter Tuning, the best result for CNN on test dataset is around 90.5%.**

## Import the data from Github
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
import re
import pprint
import numpy as np

url_test = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/raw/main/test.txt'
url_train = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/blob/main/train.txt?raw=true'
url_val = 'https://github.com/AndonisWang/EECE571_ADV_ML_TL_Project/raw/main/val.txt'

# read in the data from github link above
test_data = pd.read_csv(url_test, sep=';',header = None,names=['content','label'])
train_data = pd.read_csv(url_train, sep=';',header = None,names=['content','label'])
val_data = pd.read_csv(url_val, sep=';',header = None,names=['content','label'])

# split the dataFrame into input and output dataset
X_test = test_data.content
y_test = test_data.label
X_train = train_data.content
y_train = train_data.label
X_val = val_data.content
y_val = val_data.label

"""## Data Preprocessing

Codes are copied from previous part
"""

# download the nltk stopwords list
nltk.download('stopwords')
stops = set(stopwords.words('english'))

# add in im to the stopwords list
stops.add('im')

print(f'the length of the stopwords list is {len(stops)}')

# import the lemmitizer from nltk

# download wordnet as WordNetLemmatizer uses wordnet's morphy function
nltk.download('wordnet') 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()


def text_cleansing(ds, stops):

  corp = []

  for row in ds:

    # remove any sign that is not a letter
    new_row = re.sub('[^a-zA-Z]',' ',str(row))
    new_row = new_row.lower()
    # remove stopwords and lemmatize 
    new_row = [lemmatizer.lemmatize(word) for word in new_row.split() if word not in stops]

    corp.append(' '.join(str(x) for x in new_row))

  return corp


X_test = text_cleansing(X_test,stops)
X_train = text_cleansing(X_train,stops)
X_val = text_cleansing(X_val,stops)

"""## One-hot encoding for y

For Neuron netwroks the label has to be one-hot encoded, thus do the one-hot encoding for labe y

###First use label encoder to encode the text label to 0-5
"""

import sklearn
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit(y_train)
print(le.classes_)
y_train = le.transform(y_train)
y_test =  le.transform(y_test)
y_val =   le.transform(y_val)
y_train[0:5]

"""###Use one_hot encoder from tensorflow to do the one hod encoding"""

import tensorflow
from tensorflow import one_hot

y_train = one_hot(y_train,6)
y_test  =one_hot(y_test,depth = 6)
y_val  = one_hot(y_val,depth =6)

print(f'The shape of y_train after one hot encoding is {y_train.shape}')
print(f'The shape of y_test after one hot encoding is {y_test.shape}')
print(f'The shape of y_val after one hot encoding is {y_val.shape}')

"""## Tokenize the text by creating tensorflow's Tokenizer

https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer
"""

import tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer

# Create a token object and set the the oov_token (out of vocabulary word to 'UNK' character)
token = Tokenizer(oov_token='UNK')

# Create the vocabulary based on the X_train dataset
token.fit_on_texts(X_train)

"""Get the index to word dictionary"""

# get the index to word dictionary
import json
token.get_config()['index_word']
index_word_dict=json.loads(token.get_config()['index_word'])

# check the dictinary length, i.e., the totoal number of words in the vocabulary
# This result is different from sklearn's CountVectorizer
VOCAB_SIZE = len(index_word_dict)
print(VOCAB_SIZE,len(token.word_index))

#Tokenize the training, testing, and validation dataset based on the vocabulary generated by the training dataset

X_train = token.texts_to_sequences(X_train)
X_test = token.texts_to_sequences(X_test)
X_val = token.texts_to_sequences(X_val)

X_train[0:5]

"""###Check the length of each tokenized sample in training, testing, valdiation dataset; plot the dirstribution accordingly."""

import matplotlib.pyplot as plt

len_X_train = [len(i) for i in X_train]
len_X_test  = [len(i) for i in X_test]
len_X_val   = [len(i) for i in X_val]

fig,axes = plt.subplots(nrows=3,ncols=1,figsize=(6,9))
axes[0].hist(len_X_train)
axes[0].set_title('Training Dataset Length Dirstibution after Preprocessing')
axes[1].hist(len_X_test)
axes[1].set_title('Testing Dataset Length Dirstibution after Preprocessing')
axes[2].hist(len_X_val)
axes[2].set_title('Validation Dataset Length Dirstibution after Preprocessing')

plt.tight_layout()

"""## Make each sample the same length

This is beacuse for CNN to work appropriately, we usually have a dense layer at the end of the Conv layer, without a fixed length, we could not do the dense layer.

Thus we use pad_sequences function provided by tensorflow to do this.

https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences


"""

# MAX_LEN set is based on the previous histogram distribution.

MAX_LEN = 35

# Pad each sequence to the same length

import tensorflow

from tensorflow.keras.preprocessing.sequence import pad_sequences

X_train = pad_sequences(X_train, maxlen = MAX_LEN, padding='post')
X_test  = pad_sequences(X_test, maxlen = MAX_LEN, padding = 'post')
X_val   = pad_sequences(X_val, maxlen = MAX_LEN,padding = 'post')

print(f'The shape of the traing dataset after padding is {X_train.shape}')
print(f'The shape of the testing dataset after padding is {X_test.shape}')
print(f'The shape of the validation dataset after padding is {X_val.shape}')

X_train[0]

"""## Build the CNN model - <font color='red'>Model finished, but needs fine tune hyper-paramters to improve the accuracy</font>

###After the preprocessing, we are ready to build the CNN model. 

The idea of using CNN for text classification comes from below two examples.

https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07B%20-%20Text%20Classification%20Deep%20Learning%20CNN%20Models.ipynb#scrollTo=LR3mdd8kjgW1

https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb#scrollTo=CO2yJtgVkxlC
"""

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Dropout

"""###Notes for CNN layers

The frst layer for NLP model is the embedding model which is ued to learn the embedding vectors.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding

Instead of using 2D convolution, we use 1D convolution (A.K.A. <font color='red'>Temporal Convolutoon</font>) based on the word embedding input.
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D
"""

model = Sequential()

EMBED_SIZE = 50

model.add(Embedding(VOCAB_SIZE+1, EMBED_SIZE, input_length=MAX_LEN))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0)))
model.add(Dropout(0.2))
model.add(Dense(6, activation='softmax',kernel_regularizer=tf.keras.regularizers.L2(l2=0)))
model.summary()
plot_model(model,show_shapes=True)

plot_model(model,show_shapes=True, rankdir='LR')

# Complie the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

def get_callbacks():
  return [
    # Early stop
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10),
    # log the result with Tensorboard
    tf.keras.callbacks.TensorBoard()
    # Save the best result in folder
    #tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/checkpoint',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)
  ]

# fit the model
model.fit(X_train, y_train, epochs=100,validation_data=(X_val,y_val),callbacks=get_callbacks())

"""### Evaluate CNN model output result

####Report the accuracy for test dataset
"""

# Evaluate the Model on Test dataset
loss, acc = model.evaluate(X_test, y_test)
print('Test Accuracy: %f' % (acc*100))

# y_predict is a ndarray with shape of 2000X6, the largest value indicated the class index
y_predict = model.predict(X_test)

# find the index (the index is the encoded class) of the largest value in each y_predict row
predicted = [le.classes_[i] for i in np.argmax(y_predict,axis=1)]

# inverse from one-hot to label class
expected = [le.classes_[i] for i in np.argmax(y_test,axis=1)]

"""####Display the confusion matrix; Code copied from previous SVM part"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def clf_result(model_name, expected, predicted,classes):
  print(f'{model_name} performance:\n')

  print('Classification report:\n')
  print(classification_report(expected,predicted))

  print('Confusion matrix:\n')
  clf_matrix = confusion_matrix(expected, predicted,labels=classes)
  print(clf_matrix)

  print('\nConfusion matrix heat map:\n')
  disp = ConfusionMatrixDisplay(clf_matrix,display_labels=classes)
  disp.plot(cmap=plt.cm.Blues,colorbar=False)
  plt.show()

clf_result('CNN model',expected,predicted,le.classes_)

"""#### Display Tensor Board"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.python.ops import logging_ops
# %load_ext tensorboard

# %tensorboard --logdir logs

"""### Display the test samples that are wrongly classified"""

miscla_indx = [i for i in range(X_test.shape[0]) if predicted[i]!=expected[i]]

import csv

cnn_miscla_File = open('cnn_miscla.csv','w',newline ='')
cnn_miscla_Dictwriter = csv.DictWriter(cnn_miscla_File,['Original Sentence','Expected Lable','Predicted Lable'])
cnn_miscla_Dictwriter.writeheader()

for indx in miscla_indx:
   cnn_miscla_Dictwriter.writerow({'Original Sentence':test_data.content[indx],
                           'Expected Lable':expected[indx],
                           'Predicted Lable':predicted[indx]}
                          )
cnn_miscla_File.close()

cnn_miscla_df = pd.read_csv('./cnn_miscla.csv')
cnn_miscla_df.head()