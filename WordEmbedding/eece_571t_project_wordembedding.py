# -*- coding: utf-8 -*-
"""EECE_571T_Project_WordEmbedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reJYHe9gR8GCauA2yaUQpKvioLyL28BP

# EECE 571T Project - NLP with Emotion Dataset (Word Embedding w/ LSTM & GRU)

Focus: Word Embedding
<br>
Author: Tom Sung

Last updated:
* Date: April 5, 2022
* Time: 5:42pm

If there is time, TO DO [March 24]:
* Get histogram (bar graph) of how much each emotion label takes up
* Remove columns in data frame via this code snippet: `data_val = data_val.drop(data_val[data_val.emotion == 'love'].index)`
"""

# Check detected system hardware resources.
# import os
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print("Num CPUs Available: ", len(tf.config.experimental.list_physical_devices('CPU')))

"""## References
(There are more references throughout the document, I just haven't consolidated them all here yet)

* Making our own word2vec model: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/
* https://medium.com/@adriensieg/text-similarities-da019229c894
* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification
* From same author:
    * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)
    * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d
* Different Pre-Processing Techniques with Bag of Words w/ TF-IDF, Word Embedding, and BERT: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794

# Get data from GitHub repo

**Only run this once even after if notebook environment is cleared via** `%reset -f`. The code written here imports the Kaggle data set, which I have placed on my public GitHub repo.
"""

# Commented out IPython magic to ensure Python compatibility.
# # !wget https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip
# # !unzip /content/master.zip
# # For local computer use:
# # !unzip master.zip
# %%capture
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T-Dataset/master/Project/train.txt
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T-Dataset/master/Project/test.txt
# !wget https://raw.githubusercontent.com/tkjsung/EECE571T-Dataset/master/Project/val.txt

"""# Import Data"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries for data import
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline

# Read CSV
data_train = pd.read_csv('train.txt',sep=';', header=None)
data_test = pd.read_csv('test.txt',sep=';', header=None)
data_val = pd.read_csv('val.txt',sep=';', header=None)
# data_train = pd.read_csv('/content/EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)
# data_test = pd.read_csv('/content/EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)
# data_val = pd.read_csv('/content/EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)

# Read CSV on local computer
# data_train = pd.read_csv('EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)
# data_test = pd.read_csv('EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)
# data_val = pd.read_csv('EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)

col_names = ["sentence","emotion"]
data_train.columns = col_names
data_test.columns = col_names
data_val.columns = col_names

# See the data head to make sure data is imported correctly.
data_train.head()
# data_test.head()
# data_val.head()

"""# Encode the emotion labels with unique identifiers"""

from sklearn.preprocessing import LabelEncoder
# Encode the emotion labels with unique identifiers
data_train['emotion'].unique()
labelencoder = LabelEncoder()
data_train['emotion_enc'] = labelencoder.fit_transform(data_train['emotion'])
data_test['emotion_enc'] = labelencoder.fit_transform(data_test['emotion'])
data_val['emotion_enc'] = labelencoder.fit_transform(data_val['emotion'])
# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below.

"""Sort the encoded emotion labels for some classification reports later"""

emotion_label_list = data_train[['emotion','emotion_enc']].drop_duplicates(keep='first')
# data_test[['emotion','emotion_enc']].drop_duplicates(keep='first')
# data_val[['emotion','emotion_enc']].drop_duplicates(keep='first')
emotion_label_list = emotion_label_list.sort_values(by='emotion_enc')
emotion_label_list.iloc[0]

data_train.head()
data_test.head()
data_val.head()

"""# Data Cleaning

We need to do some data cleaning first~, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words...~

**Data Cleaning Process:** Keep only words, convert all words to lowercase, split all words, remove stopwords, lemmization for word root.<br>
The result of all of this work is a cleaned data vocab list.

Replace stemming with lemmization, which keeps the actual form of the word better. This is necessary for using pre-existing word embedding models.
Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/
"""

import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Attempting data cleaning here
def preprocess(raw_text):
    # keep only words
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    # convert to lower case and split 
    words = letters_only_text.lower().split()

    # remove stopwords
    stopword_set = set(stopwords.words("english"))
    meaningful_words = [w for w in words if w not in stopword_set]
    
    # stemmed words (looks like this is causing some words to be weird)
    # ps = PorterStemmer()
    # stemmed_words = [ps.stem(word) for word in meaningful_words]

    # lemmed words (trying this because this gets the root word?)
    lem = WordNetLemmatizer()
    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]
    
    # join the cleaned words in a list
    # cleaned_word_list = " ".join(stemmed_words)
    cleaned_word_list = " ".join(lemmed_words)
    # cleaned_word_list = " ".join(meaningful_words)

    return cleaned_word_list

"""Apply data cleaning to all data sets."""

data_train['sentence_cleaned'] = data_train['sentence'].apply(lambda line : preprocess(line))
data_test['sentence_cleaned'] = data_test['sentence'].apply(lambda line : preprocess(line))
data_val['sentence_cleaned'] = data_val['sentence'].apply(lambda line : preprocess(line))

"""# Pre-Processing and Training

Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>
There are several methods available: Bag-of-words with TF-IDF, Word Embedding using ~Word2Vec~ [I used GloVe, not Word2Vec] (unknown NN), and BERT.

## METHOD 1: Word Embedding

I did pre-processing, word stemming, and stuff like that in Data Cleaning. The simplest way avoid words not being found in a database is if word stemming is not performed on the dataset (or as I just found out, use lemmization instead. More computationally complex but better for actually working with word embedding techniques (I think)).

Partial reference: Find words in the Word2VecKeyedVector (using 2.3 in source https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb) by using `Word2VecKeyedVector.index2word`. This returns a list of the word2vec array.

Instructions used for pre-processing (this part): https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (as posted on Feb.17)

For CNN (not attempted): https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c
"""

# DO NOT RUN THIS BLOCK MORE THAN ONCE IN ONE SESSION
# Import gensim data
# import gensim.downloader as api
import gensim
# Load a pre-trained word embedding model
# Gensim data obtained from https://github.com/RaRe-Technologies/gensim-data (official source)
# word_embed = api.load('glove-twitter-25')
# word_embed = api.load('word2vec-google-news-300') # This is 1.6GB... good luck doing this on Google Colab...

# print(api.load("glove-twitter-25", return_path=True))
# print(api.load('word2vec-google-news-300', return_path=True))

# Check dimension of word vectors
# word_embed.vector_size

"""### Pre-Processing
Using Keras for Preprocessing. Steps taken:
1. Called the Tokenizer object
2. Added Training Set Vocabulary to the Tokenizer object (`fit_on_texts`)
    * Viewed the added vocabulary using `tokenizer.word_index` command.
3. Convert all text to numeric values using `text_to_sequences` method function
4. Padded the length of every sample so that the input matrix would be equal in size
"""

data_train.head()

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import seaborn as sns

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data_train["sentence_cleaned"])
dic_vocabulary = tokenizer.word_index

X_train = tokenizer.texts_to_sequences(data_train["sentence_cleaned"])
X_test = tokenizer.texts_to_sequences(data_test["sentence_cleaned"])
X_val = tokenizer.texts_to_sequences(data_val["sentence_cleaned"])

vocab_size = len(tokenizer.word_index) + 1

"""Finding out which data set has the longest "sentence" a.k.a. useful words that we did not eliminate via lemmization."""

# string_name = ['X_train', 'X_test', 'X_val']
dict_data = {'X_train': X_train,
             'X_test': X_test,
             'X_val': X_val}
histo_plot_data = np.zeros((3,35))

tmp_counter = 0;
for key, value in dict_data.items():
    feedback = 0;
    feedback_sum = 0;
    for i in value:
        histo_plot_data[tmp_counter, len(i)-1] += 1
        feedback_sum += len(i)
        if len(i) > feedback:
            feedback = len(i)
    print(f"{key}, Longest ID: {feedback}, Average ID length: {feedback_sum/len(value)}")
    tmp_counter += 1
del tmp_counter, feedback, feedback_sum

# Longest sentence has 35 elements. Average is around 10.
# TODO: This value, which influences padding, should be adjusted I think...
maxlen = 35

# Plotting the ID histogram to see the distribution
import matplotlib.pyplot as plt

plt.barh(range(1,35+1), histo_plot_data[0,:])
plt.barh(range(1,35+1), histo_plot_data[1,:])
plt.barh(range(1,35+1), histo_plot_data[2,:])

"""Pad each sample to the same length."""

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)
X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)

# Honestly don't know what this is doing, I just followed the website's instructions
# Looks like this shows the padding heat map or something similar to that
sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)
plt.show()

"""Obtain the Embedding Matrix, which is necessary for the machine learning algorithm.

Get my own embedding matrix from the Gensim library
"""

vocab_list = []

for item in data_train["sentence_cleaned"]:
    lst_words = item.split()
    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]
    vocab_list.append(lst_grams)
del lst_grams, lst_words

embeddings = np.zeros((len(dic_vocabulary)+1, 25))
counter=0
## fit word2vec model
# nlp = gensim.models.word2vec.Word2Vec(dic_vocabulary.keys(), size=25,   
#             window=8, min_count=1, sg=1, iter=30)
nlp = gensim.models.word2vec.Word2Vec(vocab_list, size=25,   
            window=8, min_count=1, sg=1, iter=30)

for word, idx in dic_vocabulary.items():
    try:
        embeddings[idx] = nlp.wv[word]
    except:
        counter += 1
        pass

print(f"Number of words in dictionary that have been assigned a matrix of 0's: {counter}")

# Was using pre-trained word embedding matrix here.
# embeddings = np.zeros((len(dic_vocabulary)+1, 25))
# counter=0
# for word, idx in dic_vocabulary.items():
#     # embeddings[idx] = word_embed[word]
#     try:
#         # Reminder: word_embed is the pre-trained word embedding model...
#         embeddings[idx] = word_embed[word]
#         # embeddings[idx] = nlp[word]
#     except:
#         counter += 1
#         pass

# print(f"Number of words in dictionary that have been assigned a matrix of 0's: {counter}")
# del counter

# IGNORE THESE
# word = "data"
# print("dic[word]:", dic_vocabulary[word], "|idx")
# print("embeddings[idx]:", embeddings[dic_vocabulary[word]].shape, 
#       "|vector")

"""### Neural Network

Referencing source: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794

Record Logs in Tensorboard
"""

from  IPython import display
import pathlib
import shutil
import tempfile

# logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
logdir = pathlib.Path('/content/tensorboard_logs')
shutil.rmtree(logdir, ignore_errors=True)

def get_callbacks(name):
  return [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),
    tf.keras.callbacks.TensorBoard(log_dir=logdir/name, update_freq='batch')
  ]

"""Build Neural Network Model"""

import tensorflow as tf

def build_classifier_model():
    def attention_layer(inputs, neurons):
        x = tf.keras.layers.Permute((2,1))(inputs)
        x = tf.keras.layers.Dense(neurons, activation="softmax")(x)
        x = tf.keras.layers.Permute((2,1), name='attention')(x)
        x = tf.keras.layers.multiply([inputs, x])
        return x

    # input
    x_in = tf.keras.layers.Input(shape=(maxlen,))

    # embedding
    # trainable=False means that these embedding weights will not change
    x = tf.keras.layers.Embedding(input_dim=embeddings.shape[0],
                        output_dim=embeddings.shape[1],
                        weights=[embeddings],
                        input_length=maxlen, trainable=True)(x_in)

    # apply attention
    # x = attention_layer(x, neurons=maxlen)

    # 2 layers of bidirectional lstm
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=maxlen, dropout=0.2, return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=maxlen, dropout=0.2))(x)

    # Testing: 2 layers of GRU
    # x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=maxlen, dropout=0.2, return_sequences=True))(x)
    # x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=maxlen, dropout=0.2))(x)

    # final dense layers
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    y_out = tf.keras.layers.Dense(6, activation='softmax')(x)
    classifier_model = tf.keras.Model(x_in, y_out)

    return classifier_model

classifier_model = build_classifier_model()

"""Optimizer"""

epochs = 40
batch_size = 32
steps_per_epoch = len(data_train['sentence_cleaned']) // batch_size
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)
init_lr = 1e-3

# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
#     initial_learning_rate=0.1,
#     decay_steps=num_train_steps,
#     decay_rate=0.98)

# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999,
#     epsilon=1e-07, amsgrad=False)


# optimizer = optimization.create_optimizer(init_lr=init_lr,
#                                           num_train_steps=num_train_steps,
#                                           num_warmup_steps=num_warmup_steps,
#                                           optimizer_type='adamw')

"""Compile model"""

classifier_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
classifier_model.summary()
tf.keras.utils.plot_model(classifier_model)

"""Train model"""

# The fitting method should be placed in a variable so that results can be easily extracted later...
# For now, I would like to see training happening in real time, so making it verbose I guess.
# Still need to adjust hyper-parameters for better results... if I can get better results.
# batch_size=256 (default given on the website)
history = classifier_model.fit(x=X_train, y=data_train['emotion_enc'], batch_size=batch_size, epochs=epochs,
                     shuffle=True, verbose=1, validation_data=[X_val, data_val['emotion_enc']],
                     callbacks=get_callbacks('WordEmbedding_LSTM'))

"""Get the probabilities of each sentence via `model.predict()` using the testing data set"""

# classifier_model.evaluate(X_test, data_test["emotion_enc"], batch_size=1)
y_test_predict = classifier_model.predict(X_test)
y_test_predict_encoded = [np.argmax(item) for item in y_test_predict]
y_test_actual = [item for item in data_test["emotion_enc"]]

"""Obtain Classification Report and Accuracy Score. Save the data in a CSV file."""

import sklearn
accuracy = sklearn.metrics.accuracy_score(y_test_actual, y_test_predict_encoded)
print("Accuracy:",  round(accuracy,3))
print("Detail:")
classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,
      target_names=emotion_label_list["emotion"])
print(classification_report)

# Attempting to save this as a pandas dataframe, which is then saved as a CSV
classification_report = sklearn.metrics.classification_report(y_test_actual, y_test_predict_encoded,
      target_names=emotion_label_list["emotion"], output_dict=True)
df = pd.DataFrame(classification_report).transpose()
df.head()
df.to_csv('/content/LSTM_ClassificationReport.csv')

"""Obtain Confusion Matrix and save the resulting figure."""

import seaborn as sns
cm = sklearn.metrics.confusion_matrix(y_test_actual, y_test_predict_encoded)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
            cbar=False)
ax.set(xlabel="Prediction", ylabel="True", xticklabels=emotion_label_list["emotion"], 
       yticklabels=emotion_label_list["emotion"], title="Confusion Matrix")
plt.yticks(rotation=0)
# Save Confusion Matrix
fig.savefig('LSTM_confusionMatrix.png',format='png',bbox_inches="tight",dpi=400)

"""Visualize the NN Training Performance in Tensorboard."""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
# %reload_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}

"""### **[IGNORE]** Training Neural Network

I think I was trying to use CNN here? Anyways I'm not doing that anymore so ignore this.
"""

# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
# def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):
#     # fit the training dataset on the classifier
#     classifier.fit(feature_vector_train, label)
    
#     # predict the labels on validation dataset
#     predictions = classifier.predict(feature_vector_valid)
    
#     if is_neural_net:
#         predictions = predictions.argmax(axis=-1)
    
#     return metrics.accuracy_score(predictions, data_val['label_enc'])

# # Test CNN (This doesn't work of course lol...)
# from keras.models import Sequential
# from keras import layers, models, optimizers

# # Add an Input Layer
# input_layer = layers.Input((50, ))

# # Add the word embedding Layer
# embedding_layer = layers.Embedding(vocab_size, 25, weights=[embedding_matrix], trainable=False)(input_layer)
# embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)

# # Add the convolutional Layer
# conv_layer = layers.Convolution1D(100, 3, activation="relu")(embedding_layer)

# # Add the pooling Layer
# pooling_layer = layers.GlobalMaxPool1D()(conv_layer)

# # Add the output Layers
# output_layer1 = layers.Dense(50, activation="relu")(pooling_layer)
# output_layer1 = layers.Dropout(0.25)(output_layer1)
# output_layer2 = layers.Dense(1, activation="sigmoid")(output_layer1)

# # Compile the model
# model = models.Model(inputs=input_layer, outputs=output_layer2)
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# accuracy = train_model(model, X_train, data_train['label_enc'], X_val, is_neural_net=True)
# print("CNN, Word Embeddings", accuracy)

# history = model.fit(X_train, data_train['label_enc'],
#                     epochs=10,
#                     validation_data=(X_test, data_test['label_enc']),
#                     batch_size=10)



# embedding_dim = 50
# model = Sequential()
# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))
# model.add(layers.Conv1D(128, 5, activation='relu'))
# model.add(layers.GlobalMaxPooling1D())
# model.add(layers.Dense(10, activation='relu'))
# model.add(layers.Dense(1, activation='sigmoid'))
# model.compile(optimizer='adam',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])
# history = model.fit(X_train, data_train['label_enc'],
#                     epochs=10,
#                     validation_data=(X_test, data_test['label_enc']),
#                     batch_size=10)

"""### **[IGNORE]** Ignore commented code (below) for the rest of this section.

There was an attempt to build my own Word Embedding matrix, but the code would not run properly...
<br>
Maybe I should revisit this now and test my own Word Embedding algorithm.
"""

# Testing n-grams stuff here.
# import nltk
# from nltk.util import ngrams
# from collections import Counter

# corpus = data_train["sentence"]
# for string in corpus:
#     token = nltk.word_tokenize(string)
#     bigrams = ngrams(token,2)
#     trigrams = ngrams(token,3)
# print(Counter(trigrams))

# Train our own word embedding model based on our own data set.
# corpus = data_train["sentence"]

# Create list of lists of unigrams
# lst_corpus = []
# for string in corpus:
#     lst_words = string.split()
#     lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]
#     lst_corpus.append(lst_grams)


# Detect bigrams and trigrams
# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=" ".encode(), min_count=5, threshold=10)
# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)

# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=" ".encode(), min_count=5, threshold=10)
# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)

# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=25, window=8, min_count=1, sg=1, epochs=30)
# word = "data"
# nlp[word].shape

# Testing code from StackOverflow
# https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim
# documents = ["the mayor of new york was there", "machine learning can be useful sometimes","new york mayor was present"]

# sentence_stream = [doc.split(" ") for doc in documents]
# print(sentence_stream)

# bigram = gensim.models.phrases.Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')

# bigram_phraser = gensim.models.phrases.Phraser(bigram)


# print(bigram_phraser)

# for sent in sentence_stream:
#     tokens_ = bigram_phraser[sent]

#     print(tokens_)

# Testing: Gets the index of where the embedded model
# model.vocab["whatever"].index
# Now use the source above, section 2.3 and follow instructions there.
# (And write it in the section below)

"""# IGNORE THINGS IN THIS SECTION.

**Ignore code blocks below this one please.**
"""

# import nltk
# from nltk.corpus import stopwords
# nltk.download('punkt')
# nltk.download('stopwords')

# # Filter out stopwords
# stop_words = set(stopwords.words('english'))

# words = [word for word in words if not word in stop_words]

# from keras_preprocessing.text import Tokenizer
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(pd.concat(data_train, axis=0))

# vocabSize = 15000

"""Padding will require the text to be already in numbers... so I can't run this yet."""

# from nltk.stem.porter import PorterStemmer
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# import re

# def text_cleaning(df, column):
#   stemmer = PorterStemmer()
#   corpus = []

#   for text in df[column]:
#     text = re.sub("[^a-zA-Z]", " ", text)
#     text = text.lower()
#     text = text.split()
#     text = [stemmer.stem(word) for word in text if word not in stop_words]
#     text = " ".join(text)
#     corpus.append(text)
  
#   # pad = pad_sequences(sequences=corpus, maxlen=max_len, padding='pre')
#   # return pad
#   return corpus

# data_train_clean = text_cleaning(data_train, 'sentence')
# data_test_clean = text_cleaning(data_test, 'sentence')
# data_val_clean = text_cleaning(data_val, 'sentence')

"""## Pre-Processing: Method 1

Source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751
"""

# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# from sklearn import decomposition, ensemble

# import pandas, xgboost, numpy, textblob, string
# from keras.preprocessing import text, sequence
# from keras import layers, models, optimizers

# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_train['sentence']), maxlen=300)
# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_test['sentence']), maxlen=300)
# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_val['sentence']), maxlen=300)

# Create list of strings into a single long string for processing
# title_list = [title for title in data_train['sentence']]

# We definitely are not doing this.
# Collapse the list of strings into a single long string for processing
# big_title_string = ' '.join(title_list)

# from nltk.tokenize import word_tokenize
# Tokenize the string into words
# tokens = word_tokenize(big_title_string)

# Filter out stopwords
# from nltk.corpus import stopwords
# stop_words = set(stopwords.words('english'))

# words = [word for word in words if not word in stop_words]

"""## Pre-Processing: Method 2

Sources:

* https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb
* https://www.tensorflow.org/text/guide/word_embeddings
* Only BOW and TF-IDF: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/
"""

# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# from sklearn import decomposition, ensemble

# import pandas, xgboost, numpy, textblob, string
# from keras.preprocessing import text, sequence
# from keras import layers, models, optimizers

# data_train['length'] = [len(x) for x in token]
# data_train.head()
# max_len = data_train['length'].max()
# print(max_len)