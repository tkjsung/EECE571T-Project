{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELaILu9SPaoy"
   },
   "source": [
    "# EECE 571T Project - NLP with Emotion Dataset\n",
    "\n",
    "Author: Tom Sung\n",
    "\n",
    "Last updated:\n",
    "* Date: March 1, 2022\n",
    "* Time: 6:13pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifCzi282UO64",
    "tags": []
   },
   "source": [
    "## References\n",
    "(There are more references throughout the document, I just haven't consolidated them all here yet)\n",
    "\n",
    "* Making our own word2vec model: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "* https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification\n",
    "* From same author:\n",
    "    * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)\n",
    "    * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n",
    "* Different Pre-Processing Techniques with Bag of Words w/ TF-IDF, Word Embedding, and BERT: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFTJfJkrO06u",
    "tags": []
   },
   "source": [
    "## Get data from GitHub repo\n",
    "\n",
    "**Only run this once even after if notebook environment is cleared via** `%reset -f`. The code written here imports the Kaggle data set, which I have placed on my public GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "724_Q82ZOihs",
    "outputId": "8b5ddd35-e5cc-431b-aba4-10ef08de57eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-03 16:19:47--  https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/tkjsung/EECE571T_Dataset/zip/refs/heads/master [following]\n",
      "--2022-03-03 16:19:48--  https://codeload.github.com/tkjsung/EECE571T_Dataset/zip/refs/heads/master\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘master.zip’\n",
      "\n",
      "master.zip              [   <=>              ] 798.87K  1.63MB/s    in 0.5s    \n",
      "\n",
      "2022-03-03 16:19:49 (1.63 MB/s) - ‘master.zip’ saved [818042]\n",
      "\n",
      "Archive:  master.zip\n",
      "f84fef58c648047c03c671498e0375bf224f000e\n",
      "   creating: EECE571T_Dataset-master/\n",
      "  inflating: EECE571T_Dataset-master/.gitignore  \n",
      "   creating: EECE571T_Dataset-master/Assignment 1/\n",
      "  inflating: EECE571T_Dataset-master/Assignment 1/.DS_Store  \n",
      "   creating: EECE571T_Dataset-master/Assignment 1/Section 2/\n",
      "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/README.md  \n",
      "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/winequality-red.csv  \n",
      "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/winequality-white.csv  \n",
      "   creating: EECE571T_Dataset-master/Project/\n",
      "  inflating: EECE571T_Dataset-master/Project/test.txt  \n",
      "  inflating: EECE571T_Dataset-master/Project/train.txt  \n",
      "  inflating: EECE571T_Dataset-master/Project/val.txt  \n",
      "  inflating: EECE571T_Dataset-master/README.md  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
    "# !unzip /content/master.zip\n",
    "# For local computer use:\n",
    "!unzip master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWVt__H7R9zl",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iKV__yZmPWsQ"
   },
   "outputs": [],
   "source": [
    "# Import libraries for data import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mMWomGTaPSES",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "# data_train = pd.read_csv('/content/EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
    "# data_test = pd.read_csv('/content/EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
    "# data_val = pd.read_csv('/content/EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)\n",
    "\n",
    "# Read CSV on local computer\n",
    "data_train = pd.read_csv('EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
    "data_test = pd.read_csv('EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
    "data_val = pd.read_csv('EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AtXW9_Z2Pbtq"
   },
   "outputs": [],
   "source": [
    "col_names = [\"sentence\",\"emotion\"]\n",
    "data_train.columns = col_names\n",
    "data_test.columns = col_names\n",
    "data_val.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "GC4V-55GSaVT",
    "outputId": "2639daad-075b-4698-a754-2d9eca6872e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the data head to make sure data is imported correctly.\n",
    "data_train.head()\n",
    "# data_test.head()\n",
    "# data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLR3Zasdel1c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encode the emotion labels with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oFJvTfXTdrN6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode the emotion labels with unique identifiers\n",
    "data_train['emotion'].unique()\n",
    "labelencoder = LabelEncoder()\n",
    "data_train['emotion_enc'] = labelencoder.fit_transform(data_train['emotion'])\n",
    "data_test['emotion_enc'] = labelencoder.fit_transform(data_test['emotion'])\n",
    "data_val['emotion_enc'] = labelencoder.fit_transform(data_val['emotion'])\n",
    "# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bBnrKVoiBT"
   },
   "source": [
    "Display the encoded emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "kkm7wEN1fnMX",
    "outputId": "4d96e5c9-849f-4ffb-a473-d4b8b9e8cece"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surprise</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fear</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    emotion  emotion_enc\n",
       "0   sadness            4\n",
       "2     anger            0\n",
       "3      love            3\n",
       "6  surprise            5\n",
       "7      fear            1\n",
       "8       joy            2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
    "# data_test[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
    "# data_val[['emotion','emotion_enc']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhyCE_IromWn"
   },
   "source": [
    "[OLD-Don't need this] ~Add sentence length to each sentence. It should calculate number of characters, including spaces and punctuation.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L0bFupawluI7"
   },
   "outputs": [],
   "source": [
    "# data_train['length'] = [len(x) for x in data_train['sentence']]\n",
    "# data_test['length'] = [len(x) for x in data_test['sentence']]\n",
    "# data_val['length'] = [len(x) for x in data_val['sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "lLROznaxHPNU",
    "outputId": "091c9172-952c-4227-e79b-600e5b31a1d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  emotion  emotion_enc\n",
       "0  im feeling quite sad and sorry for myself but ...  sadness            4\n",
       "1  i feel like i am still looking at a blank canv...  sadness            4\n",
       "2                     i feel like a faithful servant     love            3\n",
       "3                  i am just feeling cranky and blue    anger            0\n",
       "4  i can have for a treat or if i am feeling festive      joy            2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()\n",
    "data_test.head()\n",
    "data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IU3wyOOocsQ"
   },
   "source": [
    "[OLD-Unncessary] ~Finding the maximum sentence length. It seems to be 300. From the testing and validation set, they are 296 and 295, respectively.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hhBSqPnQoL-l"
   },
   "outputs": [],
   "source": [
    "# max_len = data_train['length'].max()\n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DW_8szfEo1T",
    "tags": []
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKULabmoEqMt"
   },
   "source": [
    "We need to do some data cleaning first~, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words...~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uE7b3QZrB6O"
   },
   "source": [
    "**Data Cleaning Process:** Keep only words, convert all words to lowercase, split all words, remove stopwords, lemmization for word root.<br>\n",
    "The result of all of this work is a cleaned data vocab list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHVXM1f4iYf6"
   },
   "source": [
    "Replace stemming with lemmization, which keeps the actual form of the word better. This is necessary for using pre-existing word embedding models.\n",
    "Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJWWXLcFdzc1",
    "outputId": "d32a2fed-7225-4083-f254-570529692883"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/tomsung/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Attempting data cleaning here\n",
    "def preprocess(raw_text):\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "    \n",
    "    # stemmed words (looks like this is causing some words to be weird)\n",
    "    # ps = PorterStemmer()\n",
    "    # stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "\n",
    "    # lemmed words (trying this because this gets the root word?)\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]\n",
    "    \n",
    "    # join the cleaned words in a list\n",
    "    # cleaned_word_list = \" \".join(stemmed_words)\n",
    "    cleaned_word_list = \" \".join(lemmed_words)\n",
    "    # cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BmRPEeBpYZN"
   },
   "source": [
    "Apply data cleaning to all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "schsncvleDrh"
   },
   "outputs": [],
   "source": [
    "data_train['sentence_cleaned'] = data_train['sentence'].apply(lambda line : preprocess(line))\n",
    "data_test['sentence_cleaned'] = data_test['sentence'].apply(lambda line : preprocess(line))\n",
    "data_val['sentence_cleaned'] = data_val['sentence'].apply(lambda line : preprocess(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb9vzfUAqXoZ",
    "tags": []
   },
   "source": [
    "## Pre-Processing and Training\n",
    "\n",
    "Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>\n",
    "There are several methods available: Bag-of-words with TF-IDF, Word Embedding using ~Word2Vec~ [I used GloVe, not Word2Vec] (unknown NN), and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDtYhj0SaH7y",
    "tags": []
   },
   "source": [
    "### METHOD 1: Word Embedding\n",
    "\n",
    "I did pre-processing, word stemming, and stuff like that in Data Cleaning. The simplest way avoid words not being found in a database is if word stemming is not performed on the dataset (or as I just found out, use lemmization instead. More computationally complex but better for actually working with word embedding techniques (I think))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyQ0gXpbrWuW"
   },
   "source": [
    "Partial reference: Find words in the Word2VecKeyedVector (using 2.3 in source https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb) by using `Word2VecKeyedVector.index2word`. This returns a list of the word2vec array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7otfrHqvs_4S"
   },
   "source": [
    "Instructions used for pre-processing (this part): https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (as posted on Feb.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rrmiLXAPbty"
   },
   "source": [
    "For CNN (not attempted): https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4X05qMsQmcl",
    "outputId": "6fba3100-a00f-4acb-ebe9-8ce4d377524b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tomsung/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN THIS BLOCK MORE THAN ONCE IN ONE SESSION\n",
    "# Import gensim data\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "# Load a pre-trained word embedding model\n",
    "# Gensim data obtained from https://github.com/RaRe-Technologies/gensim-data (official source)\n",
    "word_embed = api.load('glove-twitter-25')\n",
    "# word_embed = api.load('word2vec-google-news-300') # This is 1.6GB... good luck doing this on Google Colab...\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(api.load(\"glove-twitter-25\", return_path=True))\n",
    "# print(api.load('word2vec-google-news-300', return_path=True))\n",
    "\n",
    "# Check dimension of word vectors\n",
    "# model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkZ0LrphPbtz"
   },
   "source": [
    "#### Pre-Processing\n",
    "Using Keras for Preprocessing. Steps taken:\n",
    "1. Called the Tokenizer object\n",
    "2. Added Training Set Vocabulary to the Tokenizer object (`fit_on_texts`)\n",
    "    * Viewed the added vocabulary using `tokenizer.word_index` command.\n",
    "3. Convert all text to numeric values using `text_to_sequences` method function\n",
    "4. Padded the length of every sample so that the input matrix would be equal in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_enc</th>\n",
       "      <th>sentence_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  emotion  emotion_enc  \\\n",
       "0                            i didnt feel humiliated  sadness            4   \n",
       "1  i can go from feeling so hopeless to so damned...  sadness            4   \n",
       "2   im grabbing a minute to post i feel greedy wrong    anger            0   \n",
       "3  i am ever feeling nostalgic about the fireplac...     love            3   \n",
       "4                               i am feeling grouchy    anger            0   \n",
       "\n",
       "                                    sentence_cleaned  \n",
       "0                              didnt feel humiliated  \n",
       "1  go feeling hopeless damned hopeful around some...  \n",
       "2          im grabbing minute post feel greedy wrong  \n",
       "3  ever feeling nostalgic fireplace know still pr...  \n",
       "4                                    feeling grouchy  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EUzjRawYPbtz"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_train[\"sentence_cleaned\"])\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_train[\"sentence_cleaned\"])\n",
    "X_test = tokenizer.texts_to_sequences(data_test[\"sentence_cleaned\"])\n",
    "X_val = tokenizer.texts_to_sequences(data_val[\"sentence_cleaned\"])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffDRXRISyhsg"
   },
   "source": [
    "Finding out which data set has the longest \"sentence\" a.k.a. useful words that we did not eliminate via lemmization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfsSVjnqwRRM",
    "outputId": "f92d412b-bc71-4ef7-eb78-f3660272fad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, Longest ID: 35, Average ID length: 9.3530625\n",
      "X_test, Longest ID: 30, Average ID length: 8.85\n",
      "X_val, Longest ID: 28, Average ID length: 8.764\n"
     ]
    }
   ],
   "source": [
    "# string_name = ['X_train', 'X_test', 'X_val']\n",
    "dict_data = {'X_train': X_train,\n",
    "             'X_test': X_test,\n",
    "             'X_val': X_val}\n",
    "histo_plot_data = np.zeros((3,35))\n",
    "\n",
    "tmp_counter = 0;\n",
    "for key, value in dict_data.items():\n",
    "    feedback = 0;\n",
    "    feedback_sum = 0;\n",
    "    for i in value:\n",
    "        histo_plot_data[tmp_counter, len(i)-1] += 1\n",
    "        feedback_sum += len(i)\n",
    "        if len(i) > feedback:\n",
    "            feedback = len(i)\n",
    "    print(f\"{key}, Longest ID: {feedback}, Average ID length: {feedback_sum/len(value)}\")\n",
    "    tmp_counter += 1\n",
    "del tmp_counter\n",
    "\n",
    "# Longest sentence has 35 elements. Average is around 10.\n",
    "# TODO: This value, which influences padding, should be adjusted I think...\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "uJKPzj9zObv0",
    "outputId": "e58ca170-92de-42d1-f8f4-34f559b8b462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 35 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the ID histogram to see the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(range(1,35+1), histo_plot_data[0,:])\n",
    "plt.barh(range(1,35+1), histo_plot_data[1,:])\n",
    "plt.barh(range(1,35+1), histo_plot_data[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3pBemRHwEN9"
   },
   "source": [
    "Pad each sample to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "WhQBTFT6v54X",
    "outputId": "9ace4714-3937-4ed8-9bd4-3c9e669bf92d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Honestly don't know what this is doing, I just followed the website's instructions\n",
    "# Looks like this shows the padding heat map or something similar to that\n",
    "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh1SpY8hvmmN"
   },
   "source": [
    "Obtain the Embedding Matrix, which is necessary for the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjN7DmxxUp3b",
    "outputId": "454ada94-0f4b-493a-abe0-5208c8b10a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary that have been assigned a matrix of 0's: 872\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(dic_vocabulary)+1, 25))\n",
    "counter=0\n",
    "for word, idx in dic_vocabulary.items():\n",
    "    # embeddings[idx] = word_embed[word]\n",
    "    try:\n",
    "        # Reminder: word_embed is the pre-trained word embedding model...\n",
    "        embeddings[idx] = word_embed[word]\n",
    "    except:\n",
    "        counter += 1\n",
    "        pass\n",
    "\n",
    "print(f\"Number of words in dictionary that have been assigned a matrix of 0's: {counter}\")\n",
    "\n",
    "# word = \"data\"\n",
    "# print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "# print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
    "#       \"|vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_0cPtAOdcek"
   },
   "source": [
    "#### Neural Network\n",
    "\n",
    "Referencing source: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be-tKnSqdjLK",
    "outputId": "f60007dd-9a22-40ba-aaa2-20ce65554eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 20, 25)       336975      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 25, 20)       0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 25, 20)       420         ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " attention (Permute)            (None, 20, 25)       0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 20, 25)       0           ['embedding[0][0]',              \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 20, 40)       7360        ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 40)          9760        ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           2624        ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357,529\n",
      "Trainable params: 20,554\n",
      "Non-trainable params: 336,975\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name='attention')(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "# input\n",
    "x_in = layers.Input(shape=(maxlen,))\n",
    "\n",
    "# embedding\n",
    "# trainable=False means that these embedding weights will not change. What if they did though?\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],\n",
    "                     output_dim=embeddings.shape[1],\n",
    "                     weights=[embeddings],\n",
    "                     input_length=maxlen, trainable=False)(x_in)\n",
    "\n",
    "# apply attention\n",
    "x = attention_layer(x, neurons=maxlen)\n",
    "\n",
    "# 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2))(x)\n",
    "\n",
    "# final dense layers\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGE2_s1_gB7B",
    "outputId": "cb014e07-7c48-4316-f0a4-5240f1324809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "16000/16000 [==============================] - 101s 6ms/step - loss: 0.8655 - accuracy: 0.6681 - val_loss: 0.7022 - val_accuracy: 0.7425\n",
      "Epoch 2/40\n",
      "16000/16000 [==============================] - 102s 6ms/step - loss: 0.8539 - accuracy: 0.6737 - val_loss: 0.6839 - val_accuracy: 0.7315\n",
      "Epoch 3/40\n",
      "16000/16000 [==============================] - 100s 6ms/step - loss: 0.8315 - accuracy: 0.6785 - val_loss: 0.6518 - val_accuracy: 0.7470\n",
      "Epoch 4/40\n",
      "16000/16000 [==============================] - 102s 6ms/step - loss: 0.8209 - accuracy: 0.6860 - val_loss: 0.6584 - val_accuracy: 0.7445\n",
      "Epoch 5/40\n",
      "16000/16000 [==============================] - 102s 6ms/step - loss: 0.8024 - accuracy: 0.6949 - val_loss: 0.6019 - val_accuracy: 0.7665\n",
      "Epoch 6/40\n",
      "16000/16000 [==============================] - 102s 6ms/step - loss: 0.7856 - accuracy: 0.7006 - val_loss: 0.6016 - val_accuracy: 0.7665\n",
      "Epoch 7/40\n",
      " 2123/16000 [==>...........................] - ETA: 1:25 - loss: 0.7857 - accuracy: 0.6962"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The fitting method should be placed in a variable so that results can be easily extracted later...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# For now, I would like to see training happening in real time, so making it verbose I guess.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Still need to adjust hyper-parameters for better results... if I can get better results.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# batch_size=256 (default given on the website)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion_enc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion_enc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/eece571T/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The fitting method should be placed in a variable so that results can be easily extracted later...\n",
    "# For now, I would like to see training happening in real time, so making it verbose I guess.\n",
    "# Still need to adjust hyper-parameters for better results... if I can get better results.\n",
    "# batch_size=256 (default given on the website)\n",
    "model.fit(x=X_train, y=data_train['emotion_enc'], batch_size=1, epochs=40,\n",
    "                     shuffle=True, verbose=1, validation_data=[X_val, data_val['emotion_enc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 3ms/step - loss: 0.7393 - accuracy: 0.7275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7392883896827698, 0.7275000214576721]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, data_test[\"emotion_enc\"], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.05300365e-03, 4.03906545e-03, 9.51157272e-01, 1.45240808e-02,\n",
       "        2.19184756e-02, 1.30812370e-03],\n",
       "       [7.76913315e-02, 1.95582933e-03, 1.19362362e-02, 4.50924417e-04,\n",
       "        9.07127917e-01, 8.37749278e-04],\n",
       "       [1.76157206e-01, 8.05409029e-02, 2.62332186e-02, 1.02013838e-03,\n",
       "        7.03148782e-01, 1.28997555e-02],\n",
       "       ...,\n",
       "       [8.01671180e-04, 2.16602487e-03, 9.66567695e-01, 6.98006805e-03,\n",
       "        1.12308040e-02, 1.22537604e-02],\n",
       "       [2.96376832e-02, 5.71211101e-03, 8.61365855e-01, 9.56519693e-02,\n",
       "        7.53415702e-03, 9.81168705e-05],\n",
       "       [7.51884803e-02, 2.93652564e-01, 5.51766641e-02, 1.67350080e-02,\n",
       "        2.61369050e-01, 2.97878206e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qshai5NXPbt0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### [IGNORE] ~Using GloVe (Pre-trained Word Embeddings)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9-xZ59BPbt0"
   },
   "outputs": [],
   "source": [
    "# IGNORE this code block for now.\n",
    "# import numpy as np\n",
    "\n",
    "# def create_embedding_matrix(model, word_index, embedding_dim):\n",
    "#     counter=0\n",
    "#     vocab_size = len(word_index) + 1\n",
    "#     embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    \n",
    "#     for word in word_index:\n",
    "#         idx = word_index[word]\n",
    "#         try:\n",
    "#             tmp_vec = model[word]\n",
    "#         except:\n",
    "#             tmp_vec = np.zeros(embedding_dim)\n",
    "#             counter += 1\n",
    "\n",
    "#         embedding_matrix[idx] = np.array(tmp_vec, dtype=np.float32)[:embedding_dim]\n",
    "    \n",
    "#     # with open(filepath) as f:\n",
    "#     #     for line in f:\n",
    "#     #         word, *vector = line.split()\n",
    "#     #         if word in word_index:\n",
    "#     #             idx = word_index[word]\n",
    "#     #             embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "#     print(f\"Word Index length: {len(word_index)}. Total number of 0's: {counter}\")\n",
    "#     return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDo_14cNPbt0"
   },
   "outputs": [],
   "source": [
    "# word_index['test']\n",
    "# word_embed['test']\n",
    "# t1, *test = model['test']\n",
    "# print(t1)\n",
    "# print(test)\n",
    "# print(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReAo3-zuPbt1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IGNORE this code block for now.\n",
    "# embedding_dim = 25\n",
    "# embedding_matrix = create_embedding_matrix(word_embed, tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqF_6VpyPbt1",
    "tags": []
   },
   "source": [
    "#### **[IGNORE]** Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTquW7oWPbt2"
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "#     # fit the training dataset on the classifier\n",
    "#     classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "#     # predict the labels on validation dataset\n",
    "#     predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "#     if is_neural_net:\n",
    "#         predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "#     return metrics.accuracy_score(predictions, data_val['label_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42r9ks6uPbt2"
   },
   "outputs": [],
   "source": [
    "# # Test CNN (This doesn't work of course lol...)\n",
    "# from keras.models import Sequential\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# # Add an Input Layer\n",
    "# input_layer = layers.Input((50, ))\n",
    "\n",
    "# # Add the word embedding Layer\n",
    "# embedding_layer = layers.Embedding(vocab_size, 25, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "# embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "# # Add the convolutional Layer\n",
    "# conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "# # Add the pooling Layer\n",
    "# pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "# # Add the output Layers\n",
    "# output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "# output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "# output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "# # Compile the model\n",
    "# model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# accuracy = train_model(model, X_train, data_train['label_enc'], X_val, is_neural_net=True)\n",
    "# print(\"CNN, Word Embeddings\", accuracy)\n",
    "\n",
    "# history = model.fit(X_train, data_train['label_enc'],\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_test, data_test['label_enc']),\n",
    "#                     batch_size=10)\n",
    "\n",
    "\n",
    "\n",
    "# embedding_dim = 50\n",
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history = model.fit(X_train, data_train['label_enc'],\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_test, data_test['label_enc']),\n",
    "#                     batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVrMKBCmPbt2",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **[IGNORE]** Ignore commented code (below) for the rest of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQb6WF0yPbt2"
   },
   "outputs": [],
   "source": [
    "# Testing n-grams stuff here.\n",
    "# import nltk\n",
    "# from nltk.util import ngrams\n",
    "# from collections import Counter\n",
    "\n",
    "# corpus = data_train[\"sentence\"]\n",
    "# for string in corpus:\n",
    "#     token = nltk.word_tokenize(string)\n",
    "#     bigrams = ngrams(token,2)\n",
    "#     trigrams = ngrams(token,3)\n",
    "# print(Counter(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpeHaMt8Pbt2"
   },
   "outputs": [],
   "source": [
    "# Train our own word embedding model based on our own data set.\n",
    "# corpus = data_train[\"sentence\"]\n",
    "\n",
    "# Create list of lists of unigrams\n",
    "# lst_corpus = []\n",
    "# for string in corpus:\n",
    "#     lst_words = string.split()\n",
    "#     lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "#     lst_corpus.append(lst_grams)\n",
    "\n",
    "\n",
    "# Detect bigrams and trigrams\n",
    "# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "\n",
    "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "\n",
    "# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=25, window=8, min_count=1, sg=1, epochs=30)\n",
    "# word = \"data\"\n",
    "# nlp[word].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIpmKoilPbt2"
   },
   "outputs": [],
   "source": [
    "# Testing code from StackOverflow\n",
    "# https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim\n",
    "# documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
    "\n",
    "# sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "# print(sentence_stream)\n",
    "\n",
    "# bigram = gensim.models.phrases.Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
    "\n",
    "# bigram_phraser = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "\n",
    "# print(bigram_phraser)\n",
    "\n",
    "# for sent in sentence_stream:\n",
    "#     tokens_ = bigram_phraser[sent]\n",
    "\n",
    "#     print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZRSSdaxsMy3"
   },
   "outputs": [],
   "source": [
    "# Testing: Gets the index of where the embedded model\n",
    "# model.vocab[\"whatever\"].index\n",
    "# Now use the source above, section 2.3 and follow instructions there.\n",
    "# (And write it in the section below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjaWwtbek649",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## IGNORE THINGS IN THIS SECTION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcywnEKFVxd9"
   },
   "source": [
    "**Ignore code blocks below this one please.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WdV1IG0JCWf"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Filter out stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words = [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC31INsoHmNE"
   },
   "outputs": [],
   "source": [
    "# from keras_preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(pd.concat(data_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZjIwxrqH487"
   },
   "outputs": [],
   "source": [
    "# vocabSize = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OGxJ0j5O__d"
   },
   "source": [
    "Padding will require the text to be already in numbers... so I can't run this yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl0_r2neH690"
   },
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import re\n",
    "\n",
    "# def text_cleaning(df, column):\n",
    "#   stemmer = PorterStemmer()\n",
    "#   corpus = []\n",
    "\n",
    "#   for text in df[column]:\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = text.lower()\n",
    "#     text = text.split()\n",
    "#     text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
    "#     text = \" \".join(text)\n",
    "#     corpus.append(text)\n",
    "  \n",
    "#   # pad = pad_sequences(sequences=corpus, maxlen=max_len, padding='pre')\n",
    "#   # return pad\n",
    "#   return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVLWdKpaKfex"
   },
   "outputs": [],
   "source": [
    "# data_train_clean = text_cleaning(data_train, 'sentence')\n",
    "# data_test_clean = text_cleaning(data_test, 'sentence')\n",
    "# data_val_clean = text_cleaning(data_val, 'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1av-NH6N034E"
   },
   "source": [
    "### Pre-Processing: Method 1\n",
    "\n",
    "Source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0TCQh74V32I"
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_train['sentence']), maxlen=300)\n",
    "# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_test['sentence']), maxlen=300)\n",
    "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_val['sentence']), maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yLxfHPCx4fD"
   },
   "outputs": [],
   "source": [
    "# Create list of strings into a single long string for processing\n",
    "# title_list = [title for title in data_train['sentence']]\n",
    "\n",
    "# We definitely are not doing this.\n",
    "# Collapse the list of strings into a single long string for processing\n",
    "# big_title_string = ' '.join(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2mvRI_nzT4j"
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# Tokenize the string into words\n",
    "# tokens = word_tokenize(big_title_string)\n",
    "\n",
    "# Filter out stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words = [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLKapia-1GDk"
   },
   "source": [
    "### Pre-Processing: Method 2\n",
    "\n",
    "Sources:\n",
    "\n",
    "* https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb\n",
    "* https://www.tensorflow.org/text/guide/word_embeddings\n",
    "* Only BOW and TF-IDF: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D4g0Y622mgc"
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQtB1K1x3UU1"
   },
   "outputs": [],
   "source": [
    "# data_train['length'] = [len(x) for x in token]\n",
    "# data_train.head()\n",
    "# max_len = data_train['length'].max()\n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHP0--sCkC-z"
   },
   "source": [
    "### Word Vectorization\n",
    "\n",
    "We can use the `gensim` library to train our own word2vec model on a custom corpus either with CBOW or Skip Gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b9z05AQyy4P"
   },
   "source": [
    "word2vec cannot create a vector from a word that is not in its vocabulary. So we need to specify \"if word in model.vocab\" when creating the full list of word vectors (source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfwxCBpUmCBc"
   },
   "outputs": [],
   "source": [
    "# Relevant Libraries for Word Vectorization\n",
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn import decomposition, ensemble\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# import gensim\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from gensim.models import Word2Vec"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ifCzi282UO64",
    "UFTJfJkrO06u",
    "LWVt__H7R9zl",
    "PLR3Zasdel1c",
    "qshai5NXPbt0",
    "nqF_6VpyPbt1",
    "lVrMKBCmPbt2",
    "xjaWwtbek649",
    "1av-NH6N034E",
    "wLKapia-1GDk",
    "DHP0--sCkC-z"
   ],
   "history_visible": true,
   "name": "EECE_571T_Project_Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eece571T",
   "language": "python",
   "name": "eece571t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
