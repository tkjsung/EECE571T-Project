{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELaILu9SPaoy"
   },
   "source": [
    "# EECE 571T Project - NLP with Emotion Dataset\n",
    "\n",
    "Last updated:\n",
    "* Date: February 24, 2022\n",
    "* Time: 8:55pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifCzi282UO64",
    "tags": []
   },
   "source": [
    "## References:\n",
    "\n",
    "* Making our own word2vec model: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "* https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification\n",
    "* From same author:\n",
    "  * [**Feb.17**] https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)\n",
    "  * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFTJfJkrO06u",
    "tags": []
   },
   "source": [
    "## Get data from GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "724_Q82ZOihs",
    "outputId": "f769a5cb-a735-4665-e56e-fd20f7bb7c88"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
    "!unzip /content/master.zip\n",
    "# For local computer use:\n",
    "# !unzip master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWVt__H7R9zl",
    "tags": []
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKV__yZmPWsQ"
   },
   "outputs": [],
   "source": [
    "# Import libraries for data import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMWomGTaPSES"
   },
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "data_train = pd.read_csv('/content/EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
    "data_test = pd.read_csv('/content/EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
    "data_val = pd.read_csv('/content/EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)\n",
    "\n",
    "# Read CSV on local computer\n",
    "data_train = pd.read_csv('EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
    "data_test = pd.read_csv('EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
    "data_val = pd.read_csv('EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"sentence\",\"emotion\"]\n",
    "data_train.columns = col_names\n",
    "data_test.columns = col_names\n",
    "data_val.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC4V-55GSaVT"
   },
   "outputs": [],
   "source": [
    "# See the data head to make sure data is imported correctly.\n",
    "data_train.head()\n",
    "# data_test.head()\n",
    "# data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLR3Zasdel1c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encode the emotion labels with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFJvTfXTdrN6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode the emotion labels with unique identifiers\n",
    "data_train['emotion'].unique()\n",
    "labelencoder = LabelEncoder()\n",
    "data_train['label_enc'] = labelencoder.fit_transform(data_train['emotion'])\n",
    "data_test['label_enc'] = labelencoder.fit_transform(data_test['emotion'])\n",
    "data_val['label_enc'] = labelencoder.fit_transform(data_val['emotion'])\n",
    "# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-bBnrKVoiBT"
   },
   "source": [
    "Display the encoded emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "kkm7wEN1fnMX",
    "outputId": "ecac29ac-27f8-4ce0-8811-f7159f11f356"
   },
   "outputs": [],
   "source": [
    "data_train[['emotion','label_enc']].drop_duplicates(keep='first')\n",
    "# data_test[['emotion','label_enc']].drop_duplicates(keep='first')\n",
    "# data_val[['emotion','label_enc']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhyCE_IromWn"
   },
   "source": [
    "Add sentence length to each sentence. It should calculate number of characters, including spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0bFupawluI7"
   },
   "outputs": [],
   "source": [
    "data_train['length'] = [len(x) for x in data_train['sentence']]\n",
    "data_test['length'] = [len(x) for x in data_test['sentence']]\n",
    "data_val['length'] = [len(x) for x in data_val['sentence']]\n",
    "# data_train.head()\n",
    "# data_test.head()\n",
    "# data_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "lLROznaxHPNU",
    "outputId": "be97a0ea-b1c6-42cd-f120-6411ffa67105"
   },
   "outputs": [],
   "source": [
    "data_train.head()\n",
    "data_test.head()\n",
    "data_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IU3wyOOocsQ"
   },
   "source": [
    "Finding the maximum sentence length. It seems to be 300. From the testing and validation set, they are 296 and 295, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhBSqPnQoL-l",
    "outputId": "bc71d8c4-7d7c-4704-be31-f298d475328e"
   },
   "outputs": [],
   "source": [
    "max_len = data_train['length'].max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DW_8szfEo1T",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKULabmoEqMt"
   },
   "source": [
    "We need to do some data cleaning first, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP_zCSh79y5Z"
   },
   "source": [
    "<!-- Tokenize the words. This uses `keras.preprocessing` library. We get a tokenizer that fits onto our training set's sentences. Then a dictionary of words is created from the tokenizer. -->\n",
    "\n",
    "~*Feb.16: Instructions in this code block is commented out*~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHVXM1f4iYf6"
   },
   "source": [
    "**Feb.17:** For stemming, I think we should replace it with lemmization, which looks to be better and would probably work better for word2vec.\n",
    "Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJWWXLcFdzc1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Attempting data cleaning here\n",
    "def preprocess(raw_text):\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "    \n",
    "    #stemmed words (looks like this is causing some words to be weird)\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "\n",
    "    #lemmed words (trying this because this gets the root word?)\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]\n",
    "    \n",
    "    #join the cleaned words in a list\n",
    "    # cleaned_word_list = \" \".join(stemmed_words)\n",
    "    cleaned_word_list = \" \".join(lemmed_words)\n",
    "    # cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BmRPEeBpYZN"
   },
   "source": [
    "Apply data cleaning to all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "schsncvleDrh"
   },
   "outputs": [],
   "source": [
    "data_train['sentence'] = data_train['sentence'].apply(lambda line : preprocess(line))\n",
    "data_test['sentence'] = data_test['sentence'].apply(lambda line : preprocess(line))\n",
    "data_val['sentence'] = data_val['sentence'].apply(lambda line : preprocess(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb9vzfUAqXoZ",
    "tags": []
   },
   "source": [
    "## Pre-Processing and Training\n",
    "\n",
    "Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>\n",
    "There are several methods available: Bag-of-owrds with TF-IDF, Word Embedding using Word2Vec (unknown NN), and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBznUlBupqqF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [IGNORE] METHOD 1: TF-IDF\n",
    "\n",
    "I think I misunderstood what this actuually is doing... In terms of `texts_to_sequences` function in `keras.preprocessing.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlYMLkDjiqVA"
   },
   "source": [
    "Tokenize text and vectorize. (This is literally TF-IDF, as per Tensorflow's documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buLUD-rfdvS2"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from keras.preprocessing import text\n",
    "token = text.Tokenizer() # uses keras.preprocessing I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "dxwFfYZW1KmA"
   },
   "outputs": [],
   "source": [
    "token.fit_on_texts(data_train['sentence'])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaakCiM3c0pw"
   },
   "outputs": [],
   "source": [
    "# Text to sequence\n",
    "x_train_token = token.texts_to_sequences(data_train['sentence'])\n",
    "x_test_token = token.texts_to_sequences(data_test['sentence'])\n",
    "x_val_token = token.texts_to_sequences(data_val['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwG1MSVLi5Pr"
   },
   "source": [
    "Pad the data sets to be of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDNYmSwRi49k",
    "outputId": "3149ef9d-dcf9-4191-8430-27b5317d2604"
   },
   "outputs": [],
   "source": [
    "def checkLength(listArr):\n",
    "  max = 0\n",
    "  for i in range(0,len(listArr)):\n",
    "    if(max < len(listArr[i])):\n",
    "      max = len(listArr[i])\n",
    "  return max\n",
    "print(checkLength(x_train_token))\n",
    "print(checkLength(x_test_token))\n",
    "print(checkLength(x_val_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aylls2V6kAhL"
   },
   "source": [
    "Max length is 35. Pad all arrays to be of size 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEs7s4vSkENi"
   },
   "outputs": [],
   "source": [
    "# Need to add padding code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDtYhj0SaH7y",
    "tags": []
   },
   "source": [
    "### METHOD 2: Word Embedding\n",
    "\n",
    "I did pre-processing, word stemming, and stuff like that in Data Cleaning. The simplest way avoid words not being found in a database is if word stemming is not performed on the dataset (or as I just found out, use lemmization instead. More computationally complex but better for actually working with word embedding techniques (I think))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyQ0gXpbrWuW"
   },
   "source": [
    "~~**February 16:**~~ Find words in the Word2VecKeyedVector (using 2.3 in source https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb) by using `Word2VecKeyedVector.index2word`. This returns a list of the word2vec array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feb.24** Try this link: https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4X05qMsQmcl",
    "outputId": "4e6547c4-97e8-488a-c62e-5b3846f74cd8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 50.2% 834.0/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN THIS BLOCK MORE THAN ONCE IN ONE SESSION\n",
    "# Import gensim data\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "# Load a pre-trained word embedding model\n",
    "# Gensim data obtained from https://github.com/RaRe-Technologies/gensim-data (official source)\n",
    "word_embed = api.load('glove-twitter-25')\n",
    "# word_embed = api.load('word2vec-google-news-300')\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "print(api.load(\"glove-twitter-25\", return_path=True))\n",
    "# print(api.load('word2vec-google-news-300', return_path=True))\n",
    "\n",
    "# Check dimension of word vectors\n",
    "# model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_train[\"sentence\"])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_train[\"sentence\"])\n",
    "X_test = tokenizer.texts_to_sequences(data_test[\"sentence\"])\n",
    "X_val = tokenizer.texts_to_sequences(data_val[\"sentence\"])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 50\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GloVe (Pre-trained Word Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(model, word_index, embedding_dim):\n",
    "    counter=0\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((word_index+1, embedding_dim))\n",
    "    \n",
    "    for word in word_index:\n",
    "        idx = word_index[word]\n",
    "        try:\n",
    "            tmp_vec = model[word]\n",
    "        except:\n",
    "            tmp_vec = np.zeros(embedding_dim)\n",
    "            counter += 1\n",
    "\n",
    "        embedding_matrix[idx] = np.array(tmp_vec, dtype=np.float32)[:embedding_dim]\n",
    "    \n",
    "    # with open(filepath) as f:\n",
    "    #     for line in f:\n",
    "    #         word, *vector = line.split()\n",
    "    #         if word in word_index:\n",
    "    #             idx = word_index[word]\n",
    "    #             embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    print(f\"Word Index length: {len(word_index)}. Total number of 0's: {counter}\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index['test']\n",
    "# word_embed['test']\n",
    "# t1, *test = model['test']\n",
    "# print(t1)\n",
    "# print(test)\n",
    "# print(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index length: 13478. Total number of 0's: 872\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 25\n",
    "embedding_matrix = create_embedding_matrix(word_embed, tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (In Progress) Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, data_val['label_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 7s 13ms/step - loss: -27057.1328 - accuracy: 0.1212\n",
      "CNN, Word Embeddings 0.1375\n"
     ]
    }
   ],
   "source": [
    "# Test CNN (This doesn't work of course lol...)\n",
    "from keras.models import Sequential\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "# Add an Input Layer\n",
    "input_layer = layers.Input((50, ))\n",
    "\n",
    "# Add the word embedding Layer\n",
    "embedding_layer = layers.Embedding(vocab_size, 25, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "# Add the convolutional Layer\n",
    "conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "# Add the pooling Layer\n",
    "pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "# Add the output Layers\n",
    "output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "# Compile the model\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "accuracy = train_model(model, X_train, data_train['label_enc'], X_val, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\", accuracy)\n",
    "\n",
    "# history = model.fit(X_train, data_train['label_enc'],\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_test, data_test['label_enc']),\n",
    "#                     batch_size=10)\n",
    "\n",
    "\n",
    "\n",
    "# embedding_dim = 50\n",
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history = model.fit(X_train, data_train['label_enc'],\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_test, data_test['label_enc']),\n",
    "#                     batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Ignore commented code (below) for the rest of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing n-grams stuff here.\n",
    "# import nltk\n",
    "# from nltk.util import ngrams\n",
    "# from collections import Counter\n",
    "\n",
    "# corpus = data_train[\"sentence\"]\n",
    "# for string in corpus:\n",
    "#     token = nltk.word_tokenize(string)\n",
    "#     bigrams = ngrams(token,2)\n",
    "#     trigrams = ngrams(token,3)\n",
    "# print(Counter(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our own word embedding model based on our own data set.\n",
    "# corpus = data_train[\"sentence\"]\n",
    "\n",
    "# Create list of lists of unigrams\n",
    "# lst_corpus = []\n",
    "# for string in corpus:\n",
    "#     lst_words = string.split()\n",
    "#     lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
    "#     lst_corpus.append(lst_grams)\n",
    "\n",
    "\n",
    "# Detect bigrams and trigrams\n",
    "# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "\n",
    "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=\" \".encode(), min_count=5, threshold=10)\n",
    "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "\n",
    "# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=25, window=8, min_count=1, sg=1, epochs=30)\n",
    "# word = \"data\"\n",
    "# nlp[word].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code from StackOverflow\n",
    "# https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim\n",
    "# documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
    "\n",
    "# sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "# print(sentence_stream)\n",
    "\n",
    "# bigram = gensim.models.phrases.Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
    "\n",
    "# bigram_phraser = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "\n",
    "# print(bigram_phraser)\n",
    "\n",
    "# for sent in sentence_stream:\n",
    "#     tokens_ = bigram_phraser[sent]\n",
    "\n",
    "#     print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZRSSdaxsMy3"
   },
   "outputs": [],
   "source": [
    "# Testing: Gets the index of where the embedded model\n",
    "# model.vocab[\"whatever\"].index\n",
    "# Now use the source above, section 2.3 and follow instructions there.\n",
    "# (And write it in the section below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjaWwtbek649",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## IGNORE THINGS IN THIS SECTION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcywnEKFVxd9"
   },
   "source": [
    "**Ignore code blocks below this one please.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WdV1IG0JCWf",
    "outputId": "db7f03f3-7b2b-4325-bff1-716924a40fbc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words = [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC31INsoHmNE"
   },
   "outputs": [],
   "source": [
    "# from keras_preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(pd.concat(data_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZjIwxrqH487"
   },
   "outputs": [],
   "source": [
    "vocabSize = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OGxJ0j5O__d"
   },
   "source": [
    "Padding will require the text to be already in numbers... so I can't run this yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl0_r2neH690"
   },
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import re\n",
    "\n",
    "# def text_cleaning(df, column):\n",
    "#   stemmer = PorterStemmer()\n",
    "#   corpus = []\n",
    "\n",
    "#   for text in df[column]:\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = text.lower()\n",
    "#     text = text.split()\n",
    "#     text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
    "#     text = \" \".join(text)\n",
    "#     corpus.append(text)\n",
    "  \n",
    "#   # pad = pad_sequences(sequences=corpus, maxlen=max_len, padding='pre')\n",
    "#   # return pad\n",
    "#   return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVLWdKpaKfex"
   },
   "outputs": [],
   "source": [
    "# data_train_clean = text_cleaning(data_train, 'sentence')\n",
    "# data_test_clean = text_cleaning(data_test, 'sentence')\n",
    "# data_val_clean = text_cleaning(data_val, 'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1av-NH6N034E"
   },
   "source": [
    "### Pre-Processing: Method 1\n",
    "\n",
    "Source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0TCQh74V32I"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_train['sentence']), maxlen=300)\n",
    "# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_test['sentence']), maxlen=300)\n",
    "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_val['sentence']), maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yLxfHPCx4fD"
   },
   "outputs": [],
   "source": [
    "# Create list of strings into a single long string for processing\n",
    "# title_list = [title for title in data_train['sentence']]\n",
    "\n",
    "# We definitely are not doing this.\n",
    "# Collapse the list of strings into a single long string for processing\n",
    "# big_title_string = ' '.join(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lxp_sIod0NjU"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2mvRI_nzT4j"
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# Tokenize the string into words\n",
    "# tokens = word_tokenize(big_title_string)\n",
    "\n",
    "# Filter out stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words = [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLKapia-1GDk"
   },
   "source": [
    "### Pre-Processing: Method 2\n",
    "\n",
    "Sources:\n",
    "\n",
    "* https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb\n",
    "* https://www.tensorflow.org/text/guide/word_embeddings\n",
    "* Only BOW and TF-IDF: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D4g0Y622mgc"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQtB1K1x3UU1"
   },
   "outputs": [],
   "source": [
    "# data_train['length'] = [len(x) for x in token]\n",
    "# data_train.head()\n",
    "# max_len = data_train['length'].max()\n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHP0--sCkC-z"
   },
   "source": [
    "### Word Vectorization\n",
    "\n",
    "We can use the `gensim` library to train our own word2vec model on a custom corpus either with CBOW or Skip Gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b9z05AQyy4P"
   },
   "source": [
    "word2vec cannot create a vector from a word that is not in its vocabulary. So we need to specify \"if word in model.vocab\" when creating the full list of word vectors (source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfwxCBpUmCBc"
   },
   "outputs": [],
   "source": [
    "# Relevant Libraries for Word Vectorization\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rcz8xO0fUK02"
   },
   "outputs": [],
   "source": [
    "# CBOW model\n",
    "# model1 = gensim.models.Word2Vec(data_train, min_count=1, size=100, window=5)\n",
    "# Skip Gram Model\n",
    "# model2 = gensim.models.Word2Vec(data_train, min_count=1, size=100, window=5,sg=1)\n",
    "# model1.build_vocab()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ifCzi282UO64",
    "UFTJfJkrO06u",
    "xjaWwtbek649",
    "1av-NH6N034E",
    "wLKapia-1GDk",
    "DHP0--sCkC-z"
   ],
   "name": "EECE 571T Project - Testing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eece571T",
   "language": "python",
   "name": "eece571t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
