{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELaILu9SPaoy"
      },
      "source": [
        "# EECE 571T Project - NLP with Emotion Dataset\n",
        "\n",
        "Author: Tom Sung\n",
        "\n",
        "Last updated:\n",
        "* Date: March 1, 2022\n",
        "* Time: 6:13pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifCzi282UO64",
        "tags": []
      },
      "source": [
        "## References\n",
        "(There are more references throughout the document, I just haven't consolidated them all here yet)\n",
        "\n",
        "* Making our own word2vec model: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
        "* https://medium.com/@adriensieg/text-similarities-da019229c894\n",
        "* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification\n",
        "* From same author:\n",
        "    * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)\n",
        "    * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n",
        "* Different Pre-Processing Techniques with Bag of Words w/ TF-IDF, Word Embedding, and BERT: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTJfJkrO06u",
        "tags": []
      },
      "source": [
        "## Get data from GitHub repo\n",
        "\n",
        "**Only run this once** to import the Kaggle data set, which I have placed on my public GitHub repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "724_Q82ZOihs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae009bf-2889-4ad3-92c3-50ae2cd77e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-02 01:47:12--  https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/tkjsung/EECE571T_Dataset/zip/refs/heads/master [following]\n",
            "--2022-03-02 01:47:12--  https://codeload.github.com/tkjsung/EECE571T_Dataset/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.121.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.121.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ] 798.87K  4.15MB/s    in 0.2s    \n",
            "\n",
            "2022-03-02 01:47:12 (4.15 MB/s) - ‘master.zip’ saved [818042]\n",
            "\n",
            "Archive:  /content/master.zip\n",
            "f84fef58c648047c03c671498e0375bf224f000e\n",
            "   creating: EECE571T_Dataset-master/\n",
            "  inflating: EECE571T_Dataset-master/.gitignore  \n",
            "   creating: EECE571T_Dataset-master/Assignment 1/\n",
            "  inflating: EECE571T_Dataset-master/Assignment 1/.DS_Store  \n",
            "   creating: EECE571T_Dataset-master/Assignment 1/Section 2/\n",
            "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/README.md  \n",
            "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/winequality-red.csv  \n",
            "  inflating: EECE571T_Dataset-master/Assignment 1/Section 2/winequality-white.csv  \n",
            "   creating: EECE571T_Dataset-master/Project/\n",
            "  inflating: EECE571T_Dataset-master/Project/test.txt  \n",
            "  inflating: EECE571T_Dataset-master/Project/train.txt  \n",
            "  inflating: EECE571T_Dataset-master/Project/val.txt  \n",
            "  inflating: EECE571T_Dataset-master/README.md  \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
        "!unzip /content/master.zip\n",
        "# For local computer use:\n",
        "# !unzip master.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWVt__H7R9zl",
        "tags": []
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKV__yZmPWsQ"
      },
      "outputs": [],
      "source": [
        "# Import libraries for data import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMWomGTaPSES"
      },
      "outputs": [],
      "source": [
        "# Read CSV\n",
        "# data_train = pd.read_csv('/content/EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
        "# data_test = pd.read_csv('/content/EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
        "# data_val = pd.read_csv('/content/EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)\n",
        "\n",
        "# Read CSV on local computer\n",
        "data_train = pd.read_csv('EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
        "data_test = pd.read_csv('EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
        "data_val = pd.read_csv('EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtXW9_Z2Pbtq"
      },
      "outputs": [],
      "source": [
        "col_names = [\"sentence\",\"emotion\"]\n",
        "data_train.columns = col_names\n",
        "data_test.columns = col_names\n",
        "data_val.columns = col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC4V-55GSaVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "5c72290f-9126-4f3d-c376-3c4db5dbc8d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2c5321e6-4670-46aa-8465-5945f03e6463\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c5321e6-4670-46aa-8465-5945f03e6463')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c5321e6-4670-46aa-8465-5945f03e6463 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c5321e6-4670-46aa-8465-5945f03e6463');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  emotion\n",
              "0                            i didnt feel humiliated  sadness\n",
              "1  i can go from feeling so hopeless to so damned...  sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong    anger\n",
              "3  i am ever feeling nostalgic about the fireplac...     love\n",
              "4                               i am feeling grouchy    anger"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# See the data head to make sure data is imported correctly.\n",
        "data_train.head()\n",
        "# data_test.head()\n",
        "# data_val.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLR3Zasdel1c",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Encode the emotion labels with unique identifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJvTfXTdrN6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encode the emotion labels with unique identifiers\n",
        "data_train['emotion'].unique()\n",
        "labelencoder = LabelEncoder()\n",
        "data_train['emotion_enc'] = labelencoder.fit_transform(data_train['emotion'])\n",
        "data_test['emotion_enc'] = labelencoder.fit_transform(data_test['emotion'])\n",
        "data_val['emotion_enc'] = labelencoder.fit_transform(data_val['emotion'])\n",
        "# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bBnrKVoiBT"
      },
      "source": [
        "Display the encoded emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkm7wEN1fnMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "c9018d4f-4a6c-482d-a34f-437645a9c203"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b77d1ea1-98f8-4f25-b5b5-227d0bf5d05b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>emotion_enc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>love</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>surprise</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>fear</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>joy</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b77d1ea1-98f8-4f25-b5b5-227d0bf5d05b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b77d1ea1-98f8-4f25-b5b5-227d0bf5d05b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b77d1ea1-98f8-4f25-b5b5-227d0bf5d05b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    emotion  emotion_enc\n",
              "0   sadness            4\n",
              "2     anger            0\n",
              "3      love            3\n",
              "6  surprise            5\n",
              "7      fear            1\n",
              "8       joy            2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data_train[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
        "# data_test[['emotion','emotion_enc']].drop_duplicates(keep='first')\n",
        "# data_val[['emotion','emotion_enc']].drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhyCE_IromWn"
      },
      "source": [
        "[OLD-Don't need this] ~Add sentence length to each sentence. It should calculate number of characters, including spaces and punctuation.~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0bFupawluI7"
      },
      "outputs": [],
      "source": [
        "# data_train['length'] = [len(x) for x in data_train['sentence']]\n",
        "# data_test['length'] = [len(x) for x in data_test['sentence']]\n",
        "# data_val['length'] = [len(x) for x in data_val['sentence']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLROznaxHPNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "50e3a0f5-173a-4723-b13a-c894455b17c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d28bdcb3-3d56-44d1-9108-97432b5c5b64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>emotion</th>\n",
              "      <th>emotion_enc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i feel like i am still looking at a blank canv...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i feel like a faithful servant</td>\n",
              "      <td>love</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am just feeling cranky and blue</td>\n",
              "      <td>anger</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i can have for a treat or if i am feeling festive</td>\n",
              "      <td>joy</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d28bdcb3-3d56-44d1-9108-97432b5c5b64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d28bdcb3-3d56-44d1-9108-97432b5c5b64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d28bdcb3-3d56-44d1-9108-97432b5c5b64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  emotion  emotion_enc\n",
              "0  im feeling quite sad and sorry for myself but ...  sadness            4\n",
              "1  i feel like i am still looking at a blank canv...  sadness            4\n",
              "2                     i feel like a faithful servant     love            3\n",
              "3                  i am just feeling cranky and blue    anger            0\n",
              "4  i can have for a treat or if i am feeling festive      joy            2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_train.head()\n",
        "data_test.head()\n",
        "data_val.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IU3wyOOocsQ"
      },
      "source": [
        "[OLD-Unncessary] ~Finding the maximum sentence length. It seems to be 300. From the testing and validation set, they are 296 and 295, respectively.~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhBSqPnQoL-l"
      },
      "outputs": [],
      "source": [
        "# max_len = data_train['length'].max()\n",
        "# print(max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DW_8szfEo1T",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKULabmoEqMt"
      },
      "source": [
        "We need to do some data cleaning first~, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words...~"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning Process:** Keep only words, convert all words to lowercase, split all words, remove stopwords, lemmization for word root.<br>\n",
        "The result of all of this work is a cleaned data vocab list."
      ],
      "metadata": {
        "id": "6uE7b3QZrB6O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVXM1f4iYf6"
      },
      "source": [
        "Replace stemming with lemmization, which keeps the actual form of the word better. This is necessary for using pre-existing word embedding models.\n",
        "Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJWWXLcFdzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3383eb9-d492-4240-e364-c0c97bc56fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Attempting data cleaning here\n",
        "def preprocess(raw_text):\n",
        "    # keep only words\n",
        "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
        "\n",
        "    # convert to lower case and split \n",
        "    words = letters_only_text.lower().split()\n",
        "\n",
        "    # remove stopwords\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "    meaningful_words = [w for w in words if w not in stopword_set]\n",
        "    \n",
        "    #stemmed words (looks like this is causing some words to be weird)\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
        "\n",
        "    #lemmed words (trying this because this gets the root word?)\n",
        "    lem = WordNetLemmatizer()\n",
        "    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]\n",
        "    \n",
        "    #join the cleaned words in a list\n",
        "    # cleaned_word_list = \" \".join(stemmed_words)\n",
        "    cleaned_word_list = \" \".join(lemmed_words)\n",
        "    # cleaned_word_list = \" \".join(meaningful_words)\n",
        "\n",
        "    return cleaned_word_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BmRPEeBpYZN"
      },
      "source": [
        "Apply data cleaning to all data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "schsncvleDrh"
      },
      "outputs": [],
      "source": [
        "data_train['sentence_cleaned'] = data_train['sentence'].apply(lambda line : preprocess(line))\n",
        "data_test['sentence_cleaned'] = data_test['sentence'].apply(lambda line : preprocess(line))\n",
        "data_val['sentence_cleaned'] = data_val['sentence'].apply(lambda line : preprocess(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb9vzfUAqXoZ",
        "tags": []
      },
      "source": [
        "## Pre-Processing and Training\n",
        "\n",
        "Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>\n",
        "There are several methods available: Bag-of-words with TF-IDF, Word Embedding using ~Word2Vec~ [I used GloVe, not Word2Vec] (unknown NN), and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBznUlBupqqF",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### [IGNORE] ~METHOD 1: TF-IDF~\n",
        "\n",
        "I think I misunderstood what this actually is doing... In terms of `texts_to_sequences` function in `keras.preprocessing.text`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlYMLkDjiqVA"
      },
      "source": [
        "Tokenize text and vectorize. (This is literally TF-IDF, as per Tensorflow's documentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buLUD-rfdvS2"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# from keras.preprocessing import text\n",
        "# token = text.Tokenizer() # uses keras.preprocessing I believe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwFfYZW1KmA"
      },
      "outputs": [],
      "source": [
        "# token.fit_on_texts(data_train['sentence'])\n",
        "# word_index = token.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaakCiM3c0pw"
      },
      "outputs": [],
      "source": [
        "# # Text to sequence\n",
        "# x_train_token = token.texts_to_sequences(data_train['sentence'])\n",
        "# x_test_token = token.texts_to_sequences(data_test['sentence'])\n",
        "# x_val_token = token.texts_to_sequences(data_val['sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwG1MSVLi5Pr"
      },
      "source": [
        "Pad the data sets to be of the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDNYmSwRi49k"
      },
      "outputs": [],
      "source": [
        "# def checkLength(listArr):\n",
        "#   max = 0\n",
        "#   for i in range(0,len(listArr)):\n",
        "#     if(max < len(listArr[i])):\n",
        "#       max = len(listArr[i])\n",
        "#   return max\n",
        "# print(checkLength(x_train_token))\n",
        "# print(checkLength(x_test_token))\n",
        "# print(checkLength(x_val_token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aylls2V6kAhL"
      },
      "source": [
        "Max length is 35. Pad all arrays to be of size 35."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEs7s4vSkENi"
      },
      "outputs": [],
      "source": [
        "# Need to add padding code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDtYhj0SaH7y",
        "tags": []
      },
      "source": [
        "### METHOD 2: Word Embedding\n",
        "\n",
        "I did pre-processing, word stemming, and stuff like that in Data Cleaning. The simplest way avoid words not being found in a database is if word stemming is not performed on the dataset (or as I just found out, use lemmization instead. More computationally complex but better for actually working with word embedding techniques (I think))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyQ0gXpbrWuW"
      },
      "source": [
        "Partial reference: Find words in the Word2VecKeyedVector (using 2.3 in source https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb) by using `Word2VecKeyedVector.index2word`. This returns a list of the word2vec array."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions used for pre-processing (this part): https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (as posted on Feb.17)"
      ],
      "metadata": {
        "id": "7otfrHqvs_4S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rrmiLXAPbty"
      },
      "source": [
        "For CNN (not attempted): https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4X05qMsQmcl",
        "outputId": "67377af7-139b-4c61-aa2f-44627770e9ab",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "/root/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
          ]
        }
      ],
      "source": [
        "# DO NOT RUN THIS BLOCK MORE THAN ONCE IN ONE SESSION\n",
        "# Import gensim data\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "# Load a pre-trained word embedding model\n",
        "# Gensim data obtained from https://github.com/RaRe-Technologies/gensim-data (official source)\n",
        "word_embed = api.load('glove-twitter-25')\n",
        "# word_embed = api.load('word2vec-google-news-300') # This is 1.6GB... good luck doing this on Google Colab...\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "print(api.load(\"glove-twitter-25\", return_path=True))\n",
        "# print(api.load('word2vec-google-news-300', return_path=True))\n",
        "\n",
        "# Check dimension of word vectors\n",
        "# model.vector_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkZ0LrphPbtz"
      },
      "source": [
        "#### Pre-Processing\n",
        "Using Keras for Preprocessing. Steps taken:\n",
        "1. Called the Tokenizer object\n",
        "2. Added Training Set Vocabulary to the Tokenizer object (`fit_on_texts`)\n",
        "    * Viewed the added vocabulary using `tokenizer.word_index` command.\n",
        "3. Convert all text to numeric values using `text_to_sequences` method function\n",
        "4. Padded the length of every sample so that the input matrix would be equal in size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUzjRawYPbtz"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import seaborn as sns\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_train[\"sentence_cleaned\"])\n",
        "dic_vocabulary = tokenizer.word_index\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train[\"sentence_cleaned\"])\n",
        "X_test = tokenizer.texts_to_sequences(data_test[\"sentence_cleaned\"])\n",
        "X_val = tokenizer.texts_to_sequences(data_val[\"sentence_cleaned\"])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding out which data set has the longest \"sentence\" a.k.a. useful words that we did not eliminate via lemmization."
      ],
      "metadata": {
        "id": "ffDRXRISyhsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# string_name = ['X_train', 'X_test', 'X_val']\n",
        "dict_data = {'X_train': X_train,\n",
        "             'X_test': X_test,\n",
        "             'X_val': X_val}\n",
        "for key, value in dict_data.items():\n",
        "    feedback = 0;\n",
        "    feedback_sum = 0;\n",
        "    for i in value:\n",
        "        feedback_sum += len(i)\n",
        "        if len(i) > feedback:\n",
        "            feedback = len(i)\n",
        "    print(f\"{key}, Longest ID: {feedback}, Average ID length: {feedback_sum/len(value)}\")\n",
        "\n",
        "# Longest sentence has 35 elements. Average is around 10.\n",
        "# TODO: This value, which influences padding, should be adjusted I think...\n",
        "maxlen = 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfsSVjnqwRRM",
        "outputId": "cadd378d-27bd-4c8c-fc80-83283d66f09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train, Longest ID: 35, Average ID length: 9.3530625\n",
            "X_test, Longest ID: 30, Average ID length: 8.85\n",
            "X_val, Longest ID: 28, Average ID length: 8.764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad each sample to the same length."
      ],
      "metadata": {
        "id": "Y3pBemRHwEN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Honestly don't know what this is doing, I just followed the website's instructions\n",
        "# Looks like this shows the padding heat map or something similar to that\n",
        "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "WhQBTFT6v54X",
        "outputId": "6d5b4abc-3337-471c-9244-353ea6016b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain the Embedding Matrix, which is necessary for the machine learning algorithm."
      ],
      "metadata": {
        "id": "sh1SpY8hvmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.zeros((len(dic_vocabulary)+1, 25))\n",
        "counter=0\n",
        "for word, idx in dic_vocabulary.items():\n",
        "    # embeddings[idx] = word_embed[word]\n",
        "    try:\n",
        "        # Reminder: word_embed is the pre-trained word embedding model...\n",
        "        embeddings[idx] = word_embed[word]\n",
        "    except:\n",
        "        counter += 1\n",
        "        pass\n",
        "\n",
        "print(f\"Number of words in dictionary that have been assigned a matrix of 0's: {counter}\")\n",
        "\n",
        "# word = \"data\"\n",
        "# print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
        "# print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
        "#       \"|vector\")"
      ],
      "metadata": {
        "id": "SjN7DmxxUp3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c67049-94ed-4edf-ec9c-c07b7338f92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in dictionary that have been assigned a matrix of 0's: 872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neural Network\n",
        "\n",
        "Referencing source: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
      ],
      "metadata": {
        "id": "m_0cPtAOdcek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers, models, optimizers\n",
        "import keras\n",
        "\n",
        "def attention_layer(inputs, neurons):\n",
        "    x = layers.Permute((2,1))(inputs)\n",
        "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
        "    x = layers.Permute((2,1), name='attention')(x)\n",
        "    x = layers.multiply([inputs, x])\n",
        "    return x\n",
        "\n",
        "# input\n",
        "x_in = layers.Input(shape=(maxlen,))\n",
        "\n",
        "# embedding\n",
        "# trainable=False means that these embedding weights will not change. What if they did though?\n",
        "x = layers.Embedding(input_dim=embeddings.shape[0],\n",
        "                     output_dim=embeddings.shape[1],\n",
        "                     weights=[embeddings],\n",
        "                     input_length=maxlen, trainable=False)(x_in)\n",
        "\n",
        "# apply attention\n",
        "x = attention_layer(x, neurons=maxlen)\n",
        "\n",
        "# 2 layers of bidirectional lstm\n",
        "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2))(x)\n",
        "\n",
        "# final dense layers\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "y_out = layers.Dense(6, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(x_in, y_out)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be-tKnSqdjLK",
        "outputId": "58ec087c-c9a5-416c-e61b-3ae5029c92b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 15)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 15, 25)       336975      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " permute_1 (Permute)            (None, 25, 15)       0           ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 25, 15)       240         ['permute_1[0][0]']              \n",
            "                                                                                                  \n",
            " attention (Permute)            (None, 15, 25)       0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 15, 25)       0           ['embedding_1[0][0]',            \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 15, 30)      4920        ['multiply_1[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 30)          5520        ['bidirectional_2[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           1984        ['bidirectional_3[0][0]']        \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 6)            390         ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 350,029\n",
            "Trainable params: 13,054\n",
            "Non-trainable params: 336,975\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The fitting method should be placed in a variable so that results can be easily extracted later...\n",
        "# For now, I would like to see training happening in real time, so making it verbose I guess.\n",
        "# Still need to adjust hyper-parameters for better results... if I can get better results.\n",
        "# batch_size=256 (default given on the website)\n",
        "model.fit(x=X_train, y=data_train['emotion_enc'], batch_size=32, epochs=100,\n",
        "                     shuffle=True, verbose=1, validation_data=[X_val, data_val['emotion_enc']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGE2_s1_gB7B",
        "outputId": "1b71057a-840b-4440-aac0-b88353357f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "500/500 [==============================] - 17s 22ms/step - loss: 1.3113 - accuracy: 0.4904 - val_loss: 1.2560 - val_accuracy: 0.5145\n",
            "Epoch 2/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2876 - accuracy: 0.5002 - val_loss: 1.2340 - val_accuracy: 0.5170\n",
            "Epoch 3/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2695 - accuracy: 0.5077 - val_loss: 1.2225 - val_accuracy: 0.5305\n",
            "Epoch 4/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2576 - accuracy: 0.5079 - val_loss: 1.2134 - val_accuracy: 0.5335\n",
            "Epoch 5/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2505 - accuracy: 0.5097 - val_loss: 1.1854 - val_accuracy: 0.5340\n",
            "Epoch 6/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2264 - accuracy: 0.5161 - val_loss: 1.1614 - val_accuracy: 0.5370\n",
            "Epoch 7/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2227 - accuracy: 0.5226 - val_loss: 1.1483 - val_accuracy: 0.5485\n",
            "Epoch 8/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2147 - accuracy: 0.5247 - val_loss: 1.1349 - val_accuracy: 0.5655\n",
            "Epoch 9/100\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 1.2032 - accuracy: 0.5270 - val_loss: 1.1134 - val_accuracy: 0.5580\n",
            "Epoch 10/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1961 - accuracy: 0.5324 - val_loss: 1.1093 - val_accuracy: 0.5725\n",
            "Epoch 11/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1857 - accuracy: 0.5345 - val_loss: 1.0879 - val_accuracy: 0.5805\n",
            "Epoch 12/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1723 - accuracy: 0.5401 - val_loss: 1.0794 - val_accuracy: 0.5885\n",
            "Epoch 13/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1564 - accuracy: 0.5456 - val_loss: 1.0862 - val_accuracy: 0.5860\n",
            "Epoch 14/100\n",
            "500/500 [==============================] - 12s 25ms/step - loss: 1.1551 - accuracy: 0.5504 - val_loss: 1.0531 - val_accuracy: 0.5925\n",
            "Epoch 15/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1458 - accuracy: 0.5524 - val_loss: 1.0441 - val_accuracy: 0.6065\n",
            "Epoch 16/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1412 - accuracy: 0.5526 - val_loss: 1.0460 - val_accuracy: 0.5975\n",
            "Epoch 17/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1301 - accuracy: 0.5564 - val_loss: 1.0337 - val_accuracy: 0.6115\n",
            "Epoch 18/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1221 - accuracy: 0.5593 - val_loss: 1.0188 - val_accuracy: 0.5995\n",
            "Epoch 19/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1121 - accuracy: 0.5651 - val_loss: 1.0272 - val_accuracy: 0.6110\n",
            "Epoch 20/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1082 - accuracy: 0.5627 - val_loss: 1.0117 - val_accuracy: 0.6135\n",
            "Epoch 21/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.1026 - accuracy: 0.5688 - val_loss: 1.0091 - val_accuracy: 0.6130\n",
            "Epoch 22/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0988 - accuracy: 0.5698 - val_loss: 0.9865 - val_accuracy: 0.6245\n",
            "Epoch 23/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0914 - accuracy: 0.5743 - val_loss: 0.9808 - val_accuracy: 0.6280\n",
            "Epoch 24/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0876 - accuracy: 0.5734 - val_loss: 0.9883 - val_accuracy: 0.6145\n",
            "Epoch 25/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0798 - accuracy: 0.5803 - val_loss: 0.9682 - val_accuracy: 0.6360\n",
            "Epoch 26/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0658 - accuracy: 0.5851 - val_loss: 0.9607 - val_accuracy: 0.6280\n",
            "Epoch 27/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0645 - accuracy: 0.5795 - val_loss: 0.9435 - val_accuracy: 0.6380\n",
            "Epoch 28/100\n",
            "500/500 [==============================] - 11s 21ms/step - loss: 1.0605 - accuracy: 0.5887 - val_loss: 0.9478 - val_accuracy: 0.6385\n",
            "Epoch 29/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0511 - accuracy: 0.5932 - val_loss: 0.9346 - val_accuracy: 0.6460\n",
            "Epoch 30/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0477 - accuracy: 0.5916 - val_loss: 0.9384 - val_accuracy: 0.6370\n",
            "Epoch 31/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0476 - accuracy: 0.5931 - val_loss: 0.9216 - val_accuracy: 0.6460\n",
            "Epoch 32/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0429 - accuracy: 0.5950 - val_loss: 0.9357 - val_accuracy: 0.6390\n",
            "Epoch 33/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0376 - accuracy: 0.5944 - val_loss: 0.9142 - val_accuracy: 0.6485\n",
            "Epoch 34/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0287 - accuracy: 0.5976 - val_loss: 0.9350 - val_accuracy: 0.6395\n",
            "Epoch 35/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0253 - accuracy: 0.5983 - val_loss: 0.9075 - val_accuracy: 0.6595\n",
            "Epoch 36/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0220 - accuracy: 0.6012 - val_loss: 0.8990 - val_accuracy: 0.6540\n",
            "Epoch 37/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0157 - accuracy: 0.6034 - val_loss: 0.8854 - val_accuracy: 0.6620\n",
            "Epoch 38/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0088 - accuracy: 0.6091 - val_loss: 0.8937 - val_accuracy: 0.6555\n",
            "Epoch 39/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.0022 - accuracy: 0.6099 - val_loss: 0.8975 - val_accuracy: 0.6570\n",
            "Epoch 40/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9986 - accuracy: 0.6135 - val_loss: 0.8795 - val_accuracy: 0.6595\n",
            "Epoch 41/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9944 - accuracy: 0.6116 - val_loss: 0.8901 - val_accuracy: 0.6615\n",
            "Epoch 42/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9912 - accuracy: 0.6141 - val_loss: 0.8582 - val_accuracy: 0.6760\n",
            "Epoch 43/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9791 - accuracy: 0.6209 - val_loss: 0.8630 - val_accuracy: 0.6705\n",
            "Epoch 44/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9730 - accuracy: 0.6209 - val_loss: 0.8553 - val_accuracy: 0.6855\n",
            "Epoch 45/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9709 - accuracy: 0.6201 - val_loss: 0.8722 - val_accuracy: 0.6640\n",
            "Epoch 46/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9673 - accuracy: 0.6229 - val_loss: 0.8386 - val_accuracy: 0.6745\n",
            "Epoch 47/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9673 - accuracy: 0.6234 - val_loss: 0.8381 - val_accuracy: 0.6780\n",
            "Epoch 48/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9628 - accuracy: 0.6227 - val_loss: 0.8222 - val_accuracy: 0.6855\n",
            "Epoch 49/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9542 - accuracy: 0.6288 - val_loss: 0.8374 - val_accuracy: 0.6820\n",
            "Epoch 50/100\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.9456 - accuracy: 0.6299 - val_loss: 0.8229 - val_accuracy: 0.6790\n",
            "Epoch 51/100\n",
            "500/500 [==============================] - 20s 40ms/step - loss: 0.9407 - accuracy: 0.6341 - val_loss: 0.8298 - val_accuracy: 0.6880\n",
            "Epoch 52/100\n",
            "500/500 [==============================] - 13s 25ms/step - loss: 0.9389 - accuracy: 0.6327 - val_loss: 0.8225 - val_accuracy: 0.6805\n",
            "Epoch 53/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9371 - accuracy: 0.6402 - val_loss: 0.8202 - val_accuracy: 0.6830\n",
            "Epoch 54/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9320 - accuracy: 0.6394 - val_loss: 0.8156 - val_accuracy: 0.6855\n",
            "Epoch 55/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9349 - accuracy: 0.6363 - val_loss: 0.7927 - val_accuracy: 0.6980\n",
            "Epoch 56/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9272 - accuracy: 0.6389 - val_loss: 0.7953 - val_accuracy: 0.7015\n",
            "Epoch 57/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9203 - accuracy: 0.6434 - val_loss: 0.8077 - val_accuracy: 0.7000\n",
            "Epoch 58/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9184 - accuracy: 0.6441 - val_loss: 0.7807 - val_accuracy: 0.7075\n",
            "Epoch 59/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9145 - accuracy: 0.6481 - val_loss: 0.7927 - val_accuracy: 0.6975\n",
            "Epoch 60/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9146 - accuracy: 0.6471 - val_loss: 0.7792 - val_accuracy: 0.7020\n",
            "Epoch 61/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9089 - accuracy: 0.6453 - val_loss: 0.7846 - val_accuracy: 0.6995\n",
            "Epoch 62/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9098 - accuracy: 0.6471 - val_loss: 0.7825 - val_accuracy: 0.6965\n",
            "Epoch 63/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8927 - accuracy: 0.6549 - val_loss: 0.7798 - val_accuracy: 0.7125\n",
            "Epoch 64/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.9033 - accuracy: 0.6518 - val_loss: 0.7521 - val_accuracy: 0.7230\n",
            "Epoch 65/100\n",
            "500/500 [==============================] - 12s 24ms/step - loss: 0.8979 - accuracy: 0.6555 - val_loss: 0.7586 - val_accuracy: 0.7195\n",
            "Epoch 66/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8904 - accuracy: 0.6564 - val_loss: 0.7582 - val_accuracy: 0.7140\n",
            "Epoch 67/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8863 - accuracy: 0.6572 - val_loss: 0.7500 - val_accuracy: 0.7245\n",
            "Epoch 68/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8836 - accuracy: 0.6616 - val_loss: 0.7475 - val_accuracy: 0.7235\n",
            "Epoch 69/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8818 - accuracy: 0.6589 - val_loss: 0.7399 - val_accuracy: 0.7155\n",
            "Epoch 70/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8783 - accuracy: 0.6580 - val_loss: 0.7351 - val_accuracy: 0.7275\n",
            "Epoch 71/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8731 - accuracy: 0.6653 - val_loss: 0.7365 - val_accuracy: 0.7240\n",
            "Epoch 72/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8767 - accuracy: 0.6603 - val_loss: 0.7296 - val_accuracy: 0.7185\n",
            "Epoch 73/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8661 - accuracy: 0.6653 - val_loss: 0.7479 - val_accuracy: 0.7190\n",
            "Epoch 74/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8662 - accuracy: 0.6702 - val_loss: 0.7223 - val_accuracy: 0.7295\n",
            "Epoch 75/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8575 - accuracy: 0.6711 - val_loss: 0.7182 - val_accuracy: 0.7360\n",
            "Epoch 76/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8621 - accuracy: 0.6642 - val_loss: 0.7048 - val_accuracy: 0.7425\n",
            "Epoch 77/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8618 - accuracy: 0.6706 - val_loss: 0.7193 - val_accuracy: 0.7320\n",
            "Epoch 78/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8556 - accuracy: 0.6715 - val_loss: 0.7177 - val_accuracy: 0.7315\n",
            "Epoch 79/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8554 - accuracy: 0.6710 - val_loss: 0.7057 - val_accuracy: 0.7335\n",
            "Epoch 80/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8493 - accuracy: 0.6741 - val_loss: 0.7166 - val_accuracy: 0.7445\n",
            "Epoch 81/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8519 - accuracy: 0.6722 - val_loss: 0.7100 - val_accuracy: 0.7250\n",
            "Epoch 82/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8398 - accuracy: 0.6788 - val_loss: 0.6927 - val_accuracy: 0.7415\n",
            "Epoch 83/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8406 - accuracy: 0.6766 - val_loss: 0.6901 - val_accuracy: 0.7420\n",
            "Epoch 84/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8390 - accuracy: 0.6839 - val_loss: 0.6869 - val_accuracy: 0.7455\n",
            "Epoch 85/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8360 - accuracy: 0.6799 - val_loss: 0.6859 - val_accuracy: 0.7385\n",
            "Epoch 86/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8304 - accuracy: 0.6835 - val_loss: 0.6835 - val_accuracy: 0.7440\n",
            "Epoch 87/100\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.8296 - accuracy: 0.6841 - val_loss: 0.6899 - val_accuracy: 0.7450\n",
            "Epoch 88/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8298 - accuracy: 0.6813 - val_loss: 0.6813 - val_accuracy: 0.7410\n",
            "Epoch 89/100\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.8292 - accuracy: 0.6824 - val_loss: 0.6751 - val_accuracy: 0.7510\n",
            "Epoch 90/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8250 - accuracy: 0.6867 - val_loss: 0.6763 - val_accuracy: 0.7490\n",
            "Epoch 91/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8234 - accuracy: 0.6848 - val_loss: 0.6732 - val_accuracy: 0.7475\n",
            "Epoch 92/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8165 - accuracy: 0.6836 - val_loss: 0.6615 - val_accuracy: 0.7520\n",
            "Epoch 93/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8126 - accuracy: 0.6887 - val_loss: 0.6758 - val_accuracy: 0.7450\n",
            "Epoch 94/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8075 - accuracy: 0.6903 - val_loss: 0.6569 - val_accuracy: 0.7500\n",
            "Epoch 95/100\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.8132 - accuracy: 0.6859 - val_loss: 0.6584 - val_accuracy: 0.7525\n",
            "Epoch 96/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8079 - accuracy: 0.6921 - val_loss: 0.6769 - val_accuracy: 0.7425\n",
            "Epoch 97/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8046 - accuracy: 0.6889 - val_loss: 0.6557 - val_accuracy: 0.7520\n",
            "Epoch 98/100\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.8058 - accuracy: 0.6959 - val_loss: 0.6398 - val_accuracy: 0.7580\n",
            "Epoch 99/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.8014 - accuracy: 0.6933 - val_loss: 0.6559 - val_accuracy: 0.7560\n",
            "Epoch 100/100\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.7904 - accuracy: 0.7020 - val_loss: 0.6450 - val_accuracy: 0.7565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03edd1fcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshai5NXPbt0"
      },
      "source": [
        "#### [IGNORE] ~Using GloVe (Pre-trained Word Embeddings)~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9-xZ59BPbt0"
      },
      "outputs": [],
      "source": [
        "# IGNORE this code block for now.\n",
        "# import numpy as np\n",
        "\n",
        "# def create_embedding_matrix(model, word_index, embedding_dim):\n",
        "#     counter=0\n",
        "#     vocab_size = len(word_index) + 1\n",
        "#     embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
        "    \n",
        "#     for word in word_index:\n",
        "#         idx = word_index[word]\n",
        "#         try:\n",
        "#             tmp_vec = model[word]\n",
        "#         except:\n",
        "#             tmp_vec = np.zeros(embedding_dim)\n",
        "#             counter += 1\n",
        "\n",
        "#         embedding_matrix[idx] = np.array(tmp_vec, dtype=np.float32)[:embedding_dim]\n",
        "    \n",
        "#     # with open(filepath) as f:\n",
        "#     #     for line in f:\n",
        "#     #         word, *vector = line.split()\n",
        "#     #         if word in word_index:\n",
        "#     #             idx = word_index[word]\n",
        "#     #             embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "#     print(f\"Word Index length: {len(word_index)}. Total number of 0's: {counter}\")\n",
        "#     return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDo_14cNPbt0"
      },
      "outputs": [],
      "source": [
        "# word_index['test']\n",
        "# word_embed['test']\n",
        "# t1, *test = model['test']\n",
        "# print(t1)\n",
        "# print(test)\n",
        "# print(*test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ReAo3-zuPbt1"
      },
      "outputs": [],
      "source": [
        "# IGNORE this code block for now.\n",
        "# embedding_dim = 25\n",
        "# embedding_matrix = create_embedding_matrix(word_embed, tokenizer.word_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nqF_6VpyPbt1"
      },
      "source": [
        "#### **[IGNORE]** Training Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTquW7oWPbt2"
      },
      "outputs": [],
      "source": [
        "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "# def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "#     # fit the training dataset on the classifier\n",
        "#     classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "#     # predict the labels on validation dataset\n",
        "#     predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "#     if is_neural_net:\n",
        "#         predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "#     return metrics.accuracy_score(predictions, data_val['label_enc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42r9ks6uPbt2"
      },
      "outputs": [],
      "source": [
        "# # Test CNN (This doesn't work of course lol...)\n",
        "# from keras.models import Sequential\n",
        "# from keras import layers, models, optimizers\n",
        "\n",
        "# # Add an Input Layer\n",
        "# input_layer = layers.Input((50, ))\n",
        "\n",
        "# # Add the word embedding Layer\n",
        "# embedding_layer = layers.Embedding(vocab_size, 25, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "# embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "# # Add the convolutional Layer\n",
        "# conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "# # Add the pooling Layer\n",
        "# pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "# # Add the output Layers\n",
        "# output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "# output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "# output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "# # Compile the model\n",
        "# model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# accuracy = train_model(model, X_train, data_train['label_enc'], X_val, is_neural_net=True)\n",
        "# print(\"CNN, Word Embeddings\", accuracy)\n",
        "\n",
        "# history = model.fit(X_train, data_train['label_enc'],\n",
        "#                     epochs=10,\n",
        "#                     validation_data=(X_test, data_test['label_enc']),\n",
        "#                     batch_size=10)\n",
        "\n",
        "\n",
        "\n",
        "# embedding_dim = 50\n",
        "# model = Sequential()\n",
        "# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "# model.add(layers.GlobalMaxPooling1D())\n",
        "# model.add(layers.Dense(10, activation='relu'))\n",
        "# model.add(layers.Dense(1, activation='sigmoid'))\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# history = model.fit(X_train, data_train['label_enc'],\n",
        "#                     epochs=10,\n",
        "#                     validation_data=(X_test, data_test['label_enc']),\n",
        "#                     batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "lVrMKBCmPbt2"
      },
      "source": [
        "#### **[IGNORE]** Ignore commented code (below) for the rest of this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQb6WF0yPbt2"
      },
      "outputs": [],
      "source": [
        "# Testing n-grams stuff here.\n",
        "# import nltk\n",
        "# from nltk.util import ngrams\n",
        "# from collections import Counter\n",
        "\n",
        "# corpus = data_train[\"sentence\"]\n",
        "# for string in corpus:\n",
        "#     token = nltk.word_tokenize(string)\n",
        "#     bigrams = ngrams(token,2)\n",
        "#     trigrams = ngrams(token,3)\n",
        "# print(Counter(trigrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpeHaMt8Pbt2"
      },
      "outputs": [],
      "source": [
        "# Train our own word embedding model based on our own data set.\n",
        "# corpus = data_train[\"sentence\"]\n",
        "\n",
        "# Create list of lists of unigrams\n",
        "# lst_corpus = []\n",
        "# for string in corpus:\n",
        "#     lst_words = string.split()\n",
        "#     lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
        "#     lst_corpus.append(lst_grams)\n",
        "\n",
        "\n",
        "# Detect bigrams and trigrams\n",
        "# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "\n",
        "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=25, window=8, min_count=1, sg=1, epochs=30)\n",
        "# word = \"data\"\n",
        "# nlp[word].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIpmKoilPbt2"
      },
      "outputs": [],
      "source": [
        "# Testing code from StackOverflow\n",
        "# https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim\n",
        "# documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
        "\n",
        "# sentence_stream = [doc.split(\" \") for doc in documents]\n",
        "# print(sentence_stream)\n",
        "\n",
        "# bigram = gensim.models.phrases.Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
        "\n",
        "# bigram_phraser = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "\n",
        "# print(bigram_phraser)\n",
        "\n",
        "# for sent in sentence_stream:\n",
        "#     tokens_ = bigram_phraser[sent]\n",
        "\n",
        "#     print(tokens_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZRSSdaxsMy3"
      },
      "outputs": [],
      "source": [
        "# Testing: Gets the index of where the embedded model\n",
        "# model.vocab[\"whatever\"].index\n",
        "# Now use the source above, section 2.3 and follow instructions there.\n",
        "# (And write it in the section below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjaWwtbek649",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## IGNORE THINGS IN THIS SECTION."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcywnEKFVxd9"
      },
      "source": [
        "**Ignore code blocks below this one please.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WdV1IG0JCWf"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# # Filter out stopwords\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# words = [word for word in words if not word in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC31INsoHmNE"
      },
      "outputs": [],
      "source": [
        "# from keras_preprocessing.text import Tokenizer\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(pd.concat(data_train, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZjIwxrqH487"
      },
      "outputs": [],
      "source": [
        "# vocabSize = 15000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OGxJ0j5O__d"
      },
      "source": [
        "Padding will require the text to be already in numbers... so I can't run this yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl0_r2neH690"
      },
      "outputs": [],
      "source": [
        "# from nltk.stem.porter import PorterStemmer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# import re\n",
        "\n",
        "# def text_cleaning(df, column):\n",
        "#   stemmer = PorterStemmer()\n",
        "#   corpus = []\n",
        "\n",
        "#   for text in df[column]:\n",
        "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "#     text = text.lower()\n",
        "#     text = text.split()\n",
        "#     text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
        "#     text = \" \".join(text)\n",
        "#     corpus.append(text)\n",
        "  \n",
        "#   # pad = pad_sequences(sequences=corpus, maxlen=max_len, padding='pre')\n",
        "#   # return pad\n",
        "#   return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLWdKpaKfex"
      },
      "outputs": [],
      "source": [
        "# data_train_clean = text_cleaning(data_train, 'sentence')\n",
        "# data_test_clean = text_cleaning(data_test, 'sentence')\n",
        "# data_val_clean = text_cleaning(data_val, 'sentence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1av-NH6N034E"
      },
      "source": [
        "### Pre-Processing: Method 1\n",
        "\n",
        "Source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0TCQh74V32I"
      },
      "outputs": [],
      "source": [
        "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "# from sklearn import decomposition, ensemble\n",
        "\n",
        "# import pandas, xgboost, numpy, textblob, string\n",
        "# from keras.preprocessing import text, sequence\n",
        "# from keras import layers, models, optimizers\n",
        "\n",
        "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_train['sentence']), maxlen=300)\n",
        "# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_test['sentence']), maxlen=300)\n",
        "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_val['sentence']), maxlen=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yLxfHPCx4fD"
      },
      "outputs": [],
      "source": [
        "# Create list of strings into a single long string for processing\n",
        "# title_list = [title for title in data_train['sentence']]\n",
        "\n",
        "# We definitely are not doing this.\n",
        "# Collapse the list of strings into a single long string for processing\n",
        "# big_title_string = ' '.join(title_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2mvRI_nzT4j"
      },
      "outputs": [],
      "source": [
        "# from nltk.tokenize import word_tokenize\n",
        "# Tokenize the string into words\n",
        "# tokens = word_tokenize(big_title_string)\n",
        "\n",
        "# Filter out stopwords\n",
        "# from nltk.corpus import stopwords\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# words = [word for word in words if not word in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKapia-1GDk"
      },
      "source": [
        "### Pre-Processing: Method 2\n",
        "\n",
        "Sources:\n",
        "\n",
        "* https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb\n",
        "* https://www.tensorflow.org/text/guide/word_embeddings\n",
        "* Only BOW and TF-IDF: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D4g0Y622mgc"
      },
      "outputs": [],
      "source": [
        "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "# from sklearn import decomposition, ensemble\n",
        "\n",
        "# import pandas, xgboost, numpy, textblob, string\n",
        "# from keras.preprocessing import text, sequence\n",
        "# from keras import layers, models, optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQtB1K1x3UU1"
      },
      "outputs": [],
      "source": [
        "# data_train['length'] = [len(x) for x in token]\n",
        "# data_train.head()\n",
        "# max_len = data_train['length'].max()\n",
        "# print(max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHP0--sCkC-z"
      },
      "source": [
        "### Word Vectorization\n",
        "\n",
        "We can use the `gensim` library to train our own word2vec model on a custom corpus either with CBOW or Skip Gram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9z05AQyy4P"
      },
      "source": [
        "word2vec cannot create a vector from a word that is not in its vocabulary. So we need to specify \"if word in model.vocab\" when creating the full list of word vectors (source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfwxCBpUmCBc"
      },
      "outputs": [],
      "source": [
        "# Relevant Libraries for Word Vectorization\n",
        "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "# from sklearn import decomposition, ensemble\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# import gensim\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from gensim.models import Word2Vec"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ifCzi282UO64",
        "UFTJfJkrO06u",
        "LWVt__H7R9zl",
        "PLR3Zasdel1c",
        "CBznUlBupqqF",
        "zkZ0LrphPbtz",
        "qshai5NXPbt0",
        "nqF_6VpyPbt1",
        "lVrMKBCmPbt2",
        "xjaWwtbek649",
        "1av-NH6N034E",
        "wLKapia-1GDk",
        "DHP0--sCkC-z"
      ],
      "name": "EECE_571T_Project_Testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}