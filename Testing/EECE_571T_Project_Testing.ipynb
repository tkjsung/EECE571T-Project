{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELaILu9SPaoy"
      },
      "source": [
        "# EECE 571T Project - NLP with Emotion Dataset\n",
        "\n",
        "Author: Tom Sung\n",
        "\n",
        "Last updated:\n",
        "* Date: February 25, 2022\n",
        "* Time: 12:35pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifCzi282UO64",
        "tags": []
      },
      "source": [
        "## References:\n",
        "\n",
        "* Making our own word2vec model: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
        "* https://medium.com/@adriensieg/text-similarities-da019229c894\n",
        "* Text Classification tutorial: https://github.com/adsieg/Multi_Text_Classification\n",
        "* From same author:\n",
        "  * [**Feb.17**] This is used for the Word Embedding part: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (Try following these instructions next)\n",
        "  * [**Feb.17**] https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFTJfJkrO06u",
        "tags": []
      },
      "source": [
        "## Get data from GitHub repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "724_Q82ZOihs"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/tkjsung/EECE571T_Dataset/archive/refs/heads/master.zip\n",
        "!unzip /content/master.zip\n",
        "# For local computer use:\n",
        "# !unzip master.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWVt__H7R9zl",
        "tags": []
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iKV__yZmPWsQ"
      },
      "outputs": [],
      "source": [
        "# Import libraries for data import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mMWomGTaPSES"
      },
      "outputs": [],
      "source": [
        "# Read CSV\n",
        "# data_train = pd.read_csv('/content/EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
        "# data_test = pd.read_csv('/content/EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
        "# data_val = pd.read_csv('/content/EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)\n",
        "\n",
        "# Read CSV on local computer\n",
        "data_train = pd.read_csv('EECE571T_Dataset-master/Project/train.txt',sep=';', header=None)\n",
        "data_test = pd.read_csv('EECE571T_Dataset-master/Project/test.txt',sep=';', header=None)\n",
        "data_val = pd.read_csv('EECE571T_Dataset-master/Project/val.txt',sep=';', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AtXW9_Z2Pbtq"
      },
      "outputs": [],
      "source": [
        "col_names = [\"sentence\",\"emotion\"]\n",
        "data_train.columns = col_names\n",
        "data_test.columns = col_names\n",
        "data_val.columns = col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GC4V-55GSaVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "393d8266-7588-4013-f092-6280e72e1a3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3bd184e2-e0b7-4f67-934f-a9f26dde8f62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bd184e2-e0b7-4f67-934f-a9f26dde8f62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3bd184e2-e0b7-4f67-934f-a9f26dde8f62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3bd184e2-e0b7-4f67-934f-a9f26dde8f62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  emotion\n",
              "0                            i didnt feel humiliated  sadness\n",
              "1  i can go from feeling so hopeless to so damned...  sadness\n",
              "2   im grabbing a minute to post i feel greedy wrong    anger\n",
              "3  i am ever feeling nostalgic about the fireplac...     love\n",
              "4                               i am feeling grouchy    anger"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# See the data head to make sure data is imported correctly.\n",
        "data_train.head()\n",
        "# data_test.head()\n",
        "# data_val.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLR3Zasdel1c",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Encode the emotion labels with unique identifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oFJvTfXTdrN6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encode the emotion labels with unique identifiers\n",
        "data_train['emotion'].unique()\n",
        "labelencoder = LabelEncoder()\n",
        "data_train['label_enc'] = labelencoder.fit_transform(data_train['emotion'])\n",
        "data_test['label_enc'] = labelencoder.fit_transform(data_test['emotion'])\n",
        "data_val['label_enc'] = labelencoder.fit_transform(data_val['emotion'])\n",
        "# For data_test and data_val, use the same labelencoder. Make sure it's the same by using the display code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bBnrKVoiBT"
      },
      "source": [
        "Display the encoded emotion labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kkm7wEN1fnMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "847109b3-c4fb-4c79-8a12-996f816179b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a676d7b-fdb7-431a-b94c-975d053b9b20\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>label_enc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>love</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>surprise</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>fear</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>joy</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a676d7b-fdb7-431a-b94c-975d053b9b20')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a676d7b-fdb7-431a-b94c-975d053b9b20 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a676d7b-fdb7-431a-b94c-975d053b9b20');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    emotion  label_enc\n",
              "0   sadness          4\n",
              "2     anger          0\n",
              "3      love          3\n",
              "6  surprise          5\n",
              "7      fear          1\n",
              "8       joy          2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data_train[['emotion','label_enc']].drop_duplicates(keep='first')\n",
        "# data_test[['emotion','label_enc']].drop_duplicates(keep='first')\n",
        "# data_val[['emotion','label_enc']].drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhyCE_IromWn"
      },
      "source": [
        "Add sentence length to each sentence. It should calculate number of characters, including spaces and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L0bFupawluI7"
      },
      "outputs": [],
      "source": [
        "data_train['length'] = [len(x) for x in data_train['sentence']]\n",
        "data_test['length'] = [len(x) for x in data_test['sentence']]\n",
        "data_val['length'] = [len(x) for x in data_val['sentence']]\n",
        "# data_train.head()\n",
        "# data_test.head()\n",
        "# data_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lLROznaxHPNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0a8127af-c5db-4ef9-de11-15523b902dd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84187c66-2b19-4923-8ee2-a6357d6920d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>emotion</th>\n",
              "      <th>label_enc</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i feel like i am still looking at a blank canv...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i feel like a faithful servant</td>\n",
              "      <td>love</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am just feeling cranky and blue</td>\n",
              "      <td>anger</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i can have for a treat or if i am feeling festive</td>\n",
              "      <td>joy</td>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84187c66-2b19-4923-8ee2-a6357d6920d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84187c66-2b19-4923-8ee2-a6357d6920d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84187c66-2b19-4923-8ee2-a6357d6920d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence  ... length\n",
              "0  im feeling quite sad and sorry for myself but ...  ...     69\n",
              "1  i feel like i am still looking at a blank canv...  ...     70\n",
              "2                     i feel like a faithful servant  ...     30\n",
              "3                  i am just feeling cranky and blue  ...     33\n",
              "4  i can have for a treat or if i am feeling festive  ...     49\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data_train.head()\n",
        "data_test.head()\n",
        "data_val.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IU3wyOOocsQ"
      },
      "source": [
        "Finding the maximum sentence length. It seems to be 300. From the testing and validation set, they are 296 and 295, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hhBSqPnQoL-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d245960a-d355-4186-c08b-9e455939874c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "max_len = data_train['length'].max()\n",
        "print(max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DW_8szfEo1T",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKULabmoEqMt"
      },
      "source": [
        "We need to do some data cleaning first, otherwise it would be a nightmare to do pre-processing with at least 15212 vocabulary words..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP_zCSh79y5Z"
      },
      "source": [
        "<!-- Tokenize the words. This uses `keras.preprocessing` library. We get a tokenizer that fits onto our training set's sentences. Then a dictionary of words is created from the tokenizer. -->\n",
        "\n",
        "~*Feb.16: Instructions in this code block is commented out*~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVXM1f4iYf6"
      },
      "source": [
        "**Feb.17:** For stemming, I think we should replace it with lemmization, which looks to be better and would probably work better for word2vec.\n",
        "Source: https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pJWWXLcFdzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6a1996-ee4d-4302-a9fc-19c9542d0e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Attempting data cleaning here\n",
        "def preprocess(raw_text):\n",
        "    # keep only words\n",
        "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
        "\n",
        "    # convert to lower case and split \n",
        "    words = letters_only_text.lower().split()\n",
        "\n",
        "    # remove stopwords\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "    meaningful_words = [w for w in words if w not in stopword_set]\n",
        "    \n",
        "    #stemmed words (looks like this is causing some words to be weird)\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
        "\n",
        "    #lemmed words (trying this because this gets the root word?)\n",
        "    lem = WordNetLemmatizer()\n",
        "    lemmed_words = [lem.lemmatize(word) for word in meaningful_words]\n",
        "    \n",
        "    #join the cleaned words in a list\n",
        "    # cleaned_word_list = \" \".join(stemmed_words)\n",
        "    cleaned_word_list = \" \".join(lemmed_words)\n",
        "    # cleaned_word_list = \" \".join(meaningful_words)\n",
        "\n",
        "    return cleaned_word_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BmRPEeBpYZN"
      },
      "source": [
        "Apply data cleaning to all data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "schsncvleDrh"
      },
      "outputs": [],
      "source": [
        "data_train['sentence'] = data_train['sentence'].apply(lambda line : preprocess(line))\n",
        "data_test['sentence'] = data_test['sentence'].apply(lambda line : preprocess(line))\n",
        "data_val['sentence'] = data_val['sentence'].apply(lambda line : preprocess(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb9vzfUAqXoZ",
        "tags": []
      },
      "source": [
        "## Pre-Processing and Training\n",
        "\n",
        "Pre-processing and training is bundled together as the different methods use different pre-processing steps.<br>\n",
        "There are several methods available: Bag-of-owrds with TF-IDF, Word Embedding using Word2Vec (unknown NN), and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBznUlBupqqF",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### [IGNORE] METHOD 1: TF-IDF\n",
        "\n",
        "I think I misunderstood what this actuually is doing... In terms of `texts_to_sequences` function in `keras.preprocessing.text`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlYMLkDjiqVA"
      },
      "source": [
        "Tokenize text and vectorize. (This is literally TF-IDF, as per Tensorflow's documentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buLUD-rfdvS2"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from keras.preprocessing import text\n",
        "token = text.Tokenizer() # uses keras.preprocessing I believe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwFfYZW1KmA"
      },
      "outputs": [],
      "source": [
        "token.fit_on_texts(data_train['sentence'])\n",
        "word_index = token.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaakCiM3c0pw"
      },
      "outputs": [],
      "source": [
        "# Text to sequence\n",
        "x_train_token = token.texts_to_sequences(data_train['sentence'])\n",
        "x_test_token = token.texts_to_sequences(data_test['sentence'])\n",
        "x_val_token = token.texts_to_sequences(data_val['sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwG1MSVLi5Pr"
      },
      "source": [
        "Pad the data sets to be of the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDNYmSwRi49k"
      },
      "outputs": [],
      "source": [
        "def checkLength(listArr):\n",
        "  max = 0\n",
        "  for i in range(0,len(listArr)):\n",
        "    if(max < len(listArr[i])):\n",
        "      max = len(listArr[i])\n",
        "  return max\n",
        "print(checkLength(x_train_token))\n",
        "print(checkLength(x_test_token))\n",
        "print(checkLength(x_val_token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aylls2V6kAhL"
      },
      "source": [
        "Max length is 35. Pad all arrays to be of size 35."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEs7s4vSkENi"
      },
      "outputs": [],
      "source": [
        "# Need to add padding code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDtYhj0SaH7y",
        "tags": []
      },
      "source": [
        "### METHOD 2: Word Embedding\n",
        "\n",
        "I did pre-processing, word stemming, and stuff like that in Data Cleaning. The simplest way avoid words not being found in a database is if word stemming is not performed on the dataset (or as I just found out, use lemmization instead. More computationally complex but better for actually working with word embedding techniques (I think))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyQ0gXpbrWuW"
      },
      "source": [
        "~~**February 16:**~~ Find words in the Word2VecKeyedVector (using 2.3 in source https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb) by using `Word2VecKeyedVector.index2word`. This returns a list of the word2vec array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rrmiLXAPbty"
      },
      "source": [
        "**Feb.24** Try this link: https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4X05qMsQmcl",
        "outputId": "15bd0022-c2a4-4475-889d-3fd73438dbe5",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "/root/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
          ]
        }
      ],
      "source": [
        "# DO NOT RUN THIS BLOCK MORE THAN ONCE IN ONE SESSION\n",
        "# Import gensim data\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "# Load a pre-trained word embedding model\n",
        "# Gensim data obtained from https://github.com/RaRe-Technologies/gensim-data (official source)\n",
        "word_embed = api.load('glove-twitter-25')\n",
        "# word_embed = api.load('word2vec-google-news-300')\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "print(api.load(\"glove-twitter-25\", return_path=True))\n",
        "# print(api.load('word2vec-google-news-300', return_path=True))\n",
        "\n",
        "# Check dimension of word vectors\n",
        "# model.vector_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkZ0LrphPbtz"
      },
      "source": [
        "Using Keras for Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "EUzjRawYPbtz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1089b006-4ecc-4a49-b285-8857c6367b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest ID: 35, Average ID length: 9.3530625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import seaborn as sns\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_train[\"sentence\"])\n",
        "dic_vocabulary = tokenizer.word_index\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train[\"sentence\"])\n",
        "X_test = tokenizer.texts_to_sequences(data_test[\"sentence\"])\n",
        "X_val = tokenizer.texts_to_sequences(data_val[\"sentence\"])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "feedback = 0;\n",
        "feedback_sum = 0;\n",
        "for i in X_train:\n",
        "    feedback_sum += len(i)\n",
        "    if len(i) > feedback:\n",
        "        feedback = len(i)\n",
        "print(f\"Longest ID: {feedback}, Average ID length: {feedback_sum/len(X_train)}\")\n",
        "\n",
        "# Longest sentence has 35 elements. Average is around 10.\n",
        "maxlen = 35\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "\n",
        "sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.zeros((len(dic_vocabulary)+1, 25))\n",
        "counter=0\n",
        "for word, idx in dic_vocabulary.items():\n",
        "    # embeddings[idx] = word_embed[word]\n",
        "    try:\n",
        "        embeddings[idx] = word_embed[word]\n",
        "    except:\n",
        "        counter += 1\n",
        "        pass\n",
        "\n",
        "# word = \"data\"\n",
        "# print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
        "# print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n",
        "#       \"|vector\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjN7DmxxUp3b",
        "outputId": "b2cbe8b8-de5a-46e2-e394-3a5e4bfa4ae3"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dic[word]: 3788 |idx\n",
            "embeddings[idx]: (25,) |vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neural Network\n",
        "\n",
        "Referencing source: https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
      ],
      "metadata": {
        "id": "m_0cPtAOdcek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers, models, optimizers\n",
        "import keras\n",
        "\n",
        "def attention_layer(inputs, neurons):\n",
        "    x = layers.Permute((2,1))(inputs)\n",
        "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
        "    x = layers.Permute((2,1), name='attention')(x)\n",
        "    x = layers.multiply([inputs, x])\n",
        "    return x\n",
        "\n",
        "# input\n",
        "x_in = layers.Input(shape=(maxlen,))\n",
        "\n",
        "# embedding\n",
        "x = layers.Embedding(input_dim=embeddings.shape[0],\n",
        "                     output_dim=embeddings.shape[1],\n",
        "                     weights=[embeddings],\n",
        "                     input_length=maxlen, trainable=False)(x_in)\n",
        "\n",
        "# apply attention\n",
        "x = attention_layer(x, neurons=maxlen)\n",
        "\n",
        "# 2 layers of bidirectional lstm\n",
        "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(units=maxlen, dropout=0.2))(x)\n",
        "\n",
        "# final dense layers\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "y_out = layers.Dense(6, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(x_in, y_out)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be-tKnSqdjLK",
        "outputId": "6661638f-ed53-4605-a480-393485f9f1e5"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 35)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 35, 25)       336975      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " permute_2 (Permute)            (None, 25, 35)       0           ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 25, 35)       1260        ['permute_2[0][0]']              \n",
            "                                                                                                  \n",
            " attention (Permute)            (None, 35, 25)       0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 35, 25)       0           ['embedding_2[0][0]',            \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 35, 70)      17080       ['multiply_2[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 70)          29680       ['bidirectional_4[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 64)           4544        ['bidirectional_5[0][0]']        \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 6)            390         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 389,929\n",
            "Trainable params: 52,954\n",
            "Non-trainable params: 336,975\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=X_train, y=data_train['label_enc'], batch_size=256, epochs=10,\n",
        "                     shuffle=True, verbose=1, validation_split=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGE2_s1_gB7B",
        "outputId": "14fd89ed-afda-4d74-c65f-5728792c2a04"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 44s 1s/step - loss: 1.6325 - accuracy: 0.3290 - val_loss: 1.5729 - val_accuracy: 0.3288\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.5766 - accuracy: 0.3377 - val_loss: 1.5699 - val_accuracy: 0.3288\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.5684 - accuracy: 0.3537 - val_loss: 1.5483 - val_accuracy: 0.3815\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4971 - accuracy: 0.4244 - val_loss: 1.4405 - val_accuracy: 0.4617\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4509 - accuracy: 0.4508 - val_loss: 1.4262 - val_accuracy: 0.4627\n",
            "Epoch 6/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4297 - accuracy: 0.4599 - val_loss: 1.4174 - val_accuracy: 0.4602\n",
            "Epoch 7/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4227 - accuracy: 0.4632 - val_loss: 1.4028 - val_accuracy: 0.4704\n",
            "Epoch 8/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4117 - accuracy: 0.4645 - val_loss: 1.4026 - val_accuracy: 0.4731\n",
            "Epoch 9/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.4051 - accuracy: 0.4677 - val_loss: 1.3943 - val_accuracy: 0.4727\n",
            "Epoch 10/10\n",
            "44/44 [==============================] - 45s 1s/step - loss: 1.3983 - accuracy: 0.4724 - val_loss: 1.3874 - val_accuracy: 0.4746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3bc2ddd8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshai5NXPbt0"
      },
      "source": [
        "#### [IGNORE] Using GloVe (Pre-trained Word Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Q9-xZ59BPbt0"
      },
      "outputs": [],
      "source": [
        "# IGNORE this code block for now.\n",
        "# import numpy as np\n",
        "\n",
        "# def create_embedding_matrix(model, word_index, embedding_dim):\n",
        "#     counter=0\n",
        "#     vocab_size = len(word_index) + 1\n",
        "#     embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
        "    \n",
        "#     for word in word_index:\n",
        "#         idx = word_index[word]\n",
        "#         try:\n",
        "#             tmp_vec = model[word]\n",
        "#         except:\n",
        "#             tmp_vec = np.zeros(embedding_dim)\n",
        "#             counter += 1\n",
        "\n",
        "#         embedding_matrix[idx] = np.array(tmp_vec, dtype=np.float32)[:embedding_dim]\n",
        "    \n",
        "#     # with open(filepath) as f:\n",
        "#     #     for line in f:\n",
        "#     #         word, *vector = line.split()\n",
        "#     #         if word in word_index:\n",
        "#     #             idx = word_index[word]\n",
        "#     #             embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "#     print(f\"Word Index length: {len(word_index)}. Total number of 0's: {counter}\")\n",
        "#     return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDo_14cNPbt0"
      },
      "outputs": [],
      "source": [
        "# word_index['test']\n",
        "# word_embed['test']\n",
        "# t1, *test = model['test']\n",
        "# print(t1)\n",
        "# print(test)\n",
        "# print(*test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [],
        "id": "ReAo3-zuPbt1",
        "outputId": "f9cae7f7-3158-4bbd-e34f-ede4c765cfec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index length: 13478. Total number of 0's: 872\n"
          ]
        }
      ],
      "source": [
        "# IGNORE this code block for now.\n",
        "# embedding_dim = 25\n",
        "# embedding_matrix = create_embedding_matrix(word_embed, tokenizer.word_index, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nqF_6VpyPbt1"
      },
      "source": [
        "#### **[IGNORE]** Training Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTquW7oWPbt2"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, data_val['label_enc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42r9ks6uPbt2",
        "outputId": "5405e3c9-f91e-4e88-c70c-77b59eb17691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 7s 13ms/step - loss: -27057.1328 - accuracy: 0.1212\n",
            "CNN, Word Embeddings 0.1375\n"
          ]
        }
      ],
      "source": [
        "# Test CNN (This doesn't work of course lol...)\n",
        "from keras.models import Sequential\n",
        "from keras import layers, models, optimizers\n",
        "\n",
        "# Add an Input Layer\n",
        "input_layer = layers.Input((50, ))\n",
        "\n",
        "# Add the word embedding Layer\n",
        "embedding_layer = layers.Embedding(vocab_size, 25, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "# Add the convolutional Layer\n",
        "conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "# Add the pooling Layer\n",
        "pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "# Add the output Layers\n",
        "output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "# Compile the model\n",
        "model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "accuracy = train_model(model, X_train, data_train['label_enc'], X_val, is_neural_net=True)\n",
        "print(\"CNN, Word Embeddings\", accuracy)\n",
        "\n",
        "# history = model.fit(X_train, data_train['label_enc'],\n",
        "#                     epochs=10,\n",
        "#                     validation_data=(X_test, data_test['label_enc']),\n",
        "#                     batch_size=10)\n",
        "\n",
        "\n",
        "\n",
        "# embedding_dim = 50\n",
        "# model = Sequential()\n",
        "# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "# model.add(layers.GlobalMaxPooling1D())\n",
        "# model.add(layers.Dense(10, activation='relu'))\n",
        "# model.add(layers.Dense(1, activation='sigmoid'))\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# history = model.fit(X_train, data_train['label_enc'],\n",
        "#                     epochs=10,\n",
        "#                     validation_data=(X_test, data_test['label_enc']),\n",
        "#                     batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "lVrMKBCmPbt2"
      },
      "source": [
        "#### **[IGNORE]** Ignore commented code (below) for the rest of this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQb6WF0yPbt2"
      },
      "outputs": [],
      "source": [
        "# Testing n-grams stuff here.\n",
        "# import nltk\n",
        "# from nltk.util import ngrams\n",
        "# from collections import Counter\n",
        "\n",
        "# corpus = data_train[\"sentence\"]\n",
        "# for string in corpus:\n",
        "#     token = nltk.word_tokenize(string)\n",
        "#     bigrams = ngrams(token,2)\n",
        "#     trigrams = ngrams(token,3)\n",
        "# print(Counter(trigrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpeHaMt8Pbt2"
      },
      "outputs": [],
      "source": [
        "# Train our own word embedding model based on our own data set.\n",
        "# corpus = data_train[\"sentence\"]\n",
        "\n",
        "# Create list of lists of unigrams\n",
        "# lst_corpus = []\n",
        "# for string in corpus:\n",
        "#     lst_words = string.split()\n",
        "#     lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, len(lst_words), 1)]\n",
        "#     lst_corpus.append(lst_grams)\n",
        "\n",
        "\n",
        "# Detect bigrams and trigrams\n",
        "# bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "# bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "\n",
        "# trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "# trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=25, window=8, min_count=1, sg=1, epochs=30)\n",
        "# word = \"data\"\n",
        "# nlp[word].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIpmKoilPbt2"
      },
      "outputs": [],
      "source": [
        "# Testing code from StackOverflow\n",
        "# https://stackoverflow.com/questions/46129335/get-bigrams-and-trigrams-in-word2vec-gensim\n",
        "# documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
        "\n",
        "# sentence_stream = [doc.split(\" \") for doc in documents]\n",
        "# print(sentence_stream)\n",
        "\n",
        "# bigram = gensim.models.phrases.Phrases(sentence_stream, min_count=1, threshold=2, delimiter=b' ')\n",
        "\n",
        "# bigram_phraser = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "\n",
        "# print(bigram_phraser)\n",
        "\n",
        "# for sent in sentence_stream:\n",
        "#     tokens_ = bigram_phraser[sent]\n",
        "\n",
        "#     print(tokens_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZRSSdaxsMy3"
      },
      "outputs": [],
      "source": [
        "# Testing: Gets the index of where the embedded model\n",
        "# model.vocab[\"whatever\"].index\n",
        "# Now use the source above, section 2.3 and follow instructions there.\n",
        "# (And write it in the section below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjaWwtbek649",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## IGNORE THINGS IN THIS SECTION."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcywnEKFVxd9"
      },
      "source": [
        "**Ignore code blocks below this one please.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WdV1IG0JCWf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Filter out stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# words = [word for word in words if not word in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC31INsoHmNE"
      },
      "outputs": [],
      "source": [
        "# from keras_preprocessing.text import Tokenizer\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(pd.concat(data_train, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZjIwxrqH487"
      },
      "outputs": [],
      "source": [
        "vocabSize = 15000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OGxJ0j5O__d"
      },
      "source": [
        "Padding will require the text to be already in numbers... so I can't run this yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl0_r2neH690"
      },
      "outputs": [],
      "source": [
        "# from nltk.stem.porter import PorterStemmer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# import re\n",
        "\n",
        "# def text_cleaning(df, column):\n",
        "#   stemmer = PorterStemmer()\n",
        "#   corpus = []\n",
        "\n",
        "#   for text in df[column]:\n",
        "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "#     text = text.lower()\n",
        "#     text = text.split()\n",
        "#     text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
        "#     text = \" \".join(text)\n",
        "#     corpus.append(text)\n",
        "  \n",
        "#   # pad = pad_sequences(sequences=corpus, maxlen=max_len, padding='pre')\n",
        "#   # return pad\n",
        "#   return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLWdKpaKfex"
      },
      "outputs": [],
      "source": [
        "# data_train_clean = text_cleaning(data_train, 'sentence')\n",
        "# data_test_clean = text_cleaning(data_test, 'sentence')\n",
        "# data_val_clean = text_cleaning(data_val, 'sentence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1av-NH6N034E"
      },
      "source": [
        "### Pre-Processing: Method 1\n",
        "\n",
        "Source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0TCQh74V32I"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "\n",
        "# train_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_train['sentence']), maxlen=300)\n",
        "# test_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_test['sentence']), maxlen=300)\n",
        "# valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(data_val['sentence']), maxlen=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yLxfHPCx4fD"
      },
      "outputs": [],
      "source": [
        "# Create list of strings into a single long string for processing\n",
        "# title_list = [title for title in data_train['sentence']]\n",
        "\n",
        "# We definitely are not doing this.\n",
        "# Collapse the list of strings into a single long string for processing\n",
        "# big_title_string = ' '.join(title_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2mvRI_nzT4j"
      },
      "outputs": [],
      "source": [
        "# from nltk.tokenize import word_tokenize\n",
        "# Tokenize the string into words\n",
        "# tokens = word_tokenize(big_title_string)\n",
        "\n",
        "# Filter out stopwords\n",
        "# from nltk.corpus import stopwords\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# words = [word for word in words if not word in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKapia-1GDk"
      },
      "source": [
        "### Pre-Processing: Method 2\n",
        "\n",
        "Sources:\n",
        "\n",
        "* https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BIntroduction%5D%20-%20Big%20tutorial%20-%20Text%20Classification.ipynb\n",
        "* https://www.tensorflow.org/text/guide/word_embeddings\n",
        "* Only BOW and TF-IDF: https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D4g0Y622mgc"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQtB1K1x3UU1"
      },
      "outputs": [],
      "source": [
        "# data_train['length'] = [len(x) for x in token]\n",
        "# data_train.head()\n",
        "# max_len = data_train['length'].max()\n",
        "# print(max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHP0--sCkC-z"
      },
      "source": [
        "### Word Vectorization\n",
        "\n",
        "We can use the `gensim` library to train our own word2vec model on a custom corpus either with CBOW or Skip Gram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9z05AQyy4P"
      },
      "source": [
        "word2vec cannot create a vector from a word that is not in its vocabulary. So we need to specify \"if word in model.vocab\" when creating the full list of word vectors (source: https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfwxCBpUmCBc"
      },
      "outputs": [],
      "source": [
        "# Relevant Libraries for Word Vectorization\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "import gensim\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import Word2Vec"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UFTJfJkrO06u",
        "LWVt__H7R9zl",
        "PLR3Zasdel1c",
        "0DW_8szfEo1T",
        "Rb9vzfUAqXoZ",
        "CBznUlBupqqF",
        "qshai5NXPbt0",
        "xjaWwtbek649",
        "wLKapia-1GDk",
        "DHP0--sCkC-z"
      ],
      "name": "EECE_571T_Project_Testing.ipynb",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}